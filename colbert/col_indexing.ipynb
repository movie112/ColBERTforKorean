{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e42b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import ujson\n",
    "import itertools\n",
    "from itertools import accumulate\n",
    "import math\n",
    "from math import ceil\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "import copy\n",
    "import faiss\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from transformers import BertTokenizerFast, BertPreTrainedModel, BertModel, BertConfig, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6460a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/parameters.py \n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "SEED = 12345\n",
    "\n",
    "SAVED_CHECKPOINTS = [32*1000, 100*1000, 150*1000, 200*1000, 300*1000, 400*1000]\n",
    "SAVED_CHECKPOINTS += [10*1000, 20*1000, 30*1000, 40*1000, 50*1000, 60*1000, 70*1000, 80*1000, 90*1000]\n",
    "SAVED_CHECKPOINTS += [25*1000, 50*1000, 75*1000]\n",
    "\n",
    "SAVED_CHECKPOINTS = set(SAVED_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec10149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/colbert.py\n",
    "'''\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast\n",
    "from colbert.parameters import DEVICE\n",
    "'''\n",
    "class ColBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, query_maxlen, doc_maxlen, mask_punctuation, dim=128, similarity_metric='cosine'):\n",
    "\n",
    "        super(ColBERT, self).__init__(config)\n",
    "\n",
    "        self.query_maxlen = query_maxlen\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.dim = dim\n",
    "\n",
    "        self.mask_punctuation = mask_punctuation\n",
    "        self.skiplist = {}\n",
    "\n",
    "        if self.mask_punctuation:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "            self.skiplist = {w: True\n",
    "                             for symbol in string.punctuation\n",
    "                             for w in [symbol, self.tokenizer.encode(symbol, add_special_tokens=False)[0]]}\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.linear = nn.Linear(config.hidden_size, dim, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, Q, D):\n",
    "        return self.score(self.query(*Q), self.doc(*D))\n",
    "\n",
    "    def query(self, input_ids, attention_mask):\n",
    "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "        Q = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        Q = self.linear(Q)\n",
    "\n",
    "        return torch.nn.functional.normalize(Q, p=2, dim=2)\n",
    "\n",
    "    def doc(self, input_ids, attention_mask, keep_dims=True):\n",
    "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "        D = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        D = self.linear(D)\n",
    "\n",
    "        mask = torch.tensor(self.mask(input_ids), device=DEVICE).unsqueeze(2).float()\n",
    "        D = D * mask\n",
    "\n",
    "        D = torch.nn.functional.normalize(D, p=2, dim=2)\n",
    "\n",
    "        if not keep_dims:\n",
    "            D, mask = D.cpu().to(dtype=torch.float16), mask.cpu().bool().squeeze(-1)\n",
    "            D = [d[mask[idx]] for idx, d in enumerate(D)]\n",
    "\n",
    "        return D\n",
    "\n",
    "    def score(self, Q, D):\n",
    "        if self.similarity_metric == 'cosine':\n",
    "            return (Q @ D.permute(0, 2, 1)).max(2).values.sum(1)\n",
    "\n",
    "        assert self.similarity_metric == 'l2'\n",
    "        return (-1.0 * ((Q.unsqueeze(2) - D.unsqueeze(1))**2).sum(-1)).max(-1).values.sum(-1)\n",
    "\n",
    "    def mask(self, input_ids):\n",
    "        mask = [[(x not in self.skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c8d46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/utils.py\n",
    "\n",
    "\n",
    "\n",
    "def _sort_by_length(ids, mask, bsize):\n",
    "    if ids.size(0) <= bsize:\n",
    "        return ids, mask, torch.arange(ids.size(0))\n",
    "\n",
    "    indices = mask.sum(-1).sort().indices\n",
    "    reverse_indices = indices.sort().indices\n",
    "\n",
    "    return ids[indices], mask[indices], reverse_indices\n",
    "\n",
    "\n",
    "def _split_into_batches(ids, mask, bsize):\n",
    "    batches = []\n",
    "    for offset in range(0, ids.size(0), bsize):\n",
    "        batches.append((ids[offset:offset+bsize], mask[offset:offset+bsize]))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f493487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/query_tokenization.py\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "from colbert.modeling.tokenization.utils import _split_into_batches\n",
    "'''\n",
    "\n",
    "class QueryTokenizer():\n",
    "    def __init__(self, query_maxlen):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.query_maxlen = query_maxlen\n",
    "\n",
    "        self.Q_marker_token, self.Q_marker_token_id = '[Q]', self.tok.convert_tokens_to_ids('[unused0]')\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id\n",
    "\n",
    "        assert self.Q_marker_token_id == 1 and self.mask_token_id == 103\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst)+3)) for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
    "        print(ids)\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst)+3)) for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        # add placehold for the [Q] marker\n",
    "        batch_text = ['. ' + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(batch_text, padding='max_length', truncation=True,\n",
    "                       return_tensors='pt', max_length=self.query_maxlen)\n",
    "\n",
    "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "\n",
    "        # postprocess for the [Q] marker and the [MASK] augmentation\n",
    "        ids[:, 1] = self.Q_marker_token_id\n",
    "        ids[ids == 0] = self.mask_token_id\n",
    "\n",
    "        if bsize:\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches\n",
    "\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6792ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/doc_tokenization.py \n",
    "\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "from colbert.modeling.tokenization.utils import _split_into_batches, _sort_by_length\n",
    "'''\n",
    "\n",
    "class DocTokenizer():\n",
    "    def __init__(self, doc_maxlen):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "\n",
    "        self.D_marker_token, self.D_marker_token_id = '[D]', self.tok.convert_tokens_to_ids('[unused1]')\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "\n",
    "        assert self.D_marker_token_id == 2\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        # add placehold for the [D] marker\n",
    "        batch_text = ['. ' + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(batch_text, padding='longest', truncation='longest_first',\n",
    "                       return_tensors='pt', max_length=self.doc_maxlen)\n",
    "\n",
    "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "\n",
    "        # postprocess for the [D] marker\n",
    "        ids[:, 1] = self.D_marker_token_id\n",
    "\n",
    "        if bsize:\n",
    "            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches, reverse_indices\n",
    "\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6835a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/evaluation/loaders.py\n",
    "def load_colbert(args, do_print=True):\n",
    "    colbert, checkpoint = load_model(args, do_print)\n",
    "\n",
    "    # TODO: If the parameters below were not specified on the command line, their *checkpoint* values should be used.\n",
    "    # I.e., not their purely (i.e., training) default values.\n",
    "\n",
    "    for k in ['query_maxlen', 'doc_maxlen', 'dim', 'similarity', 'amp']:\n",
    "        if 'arguments' in checkpoint and hasattr(args, k):\n",
    "            if k in checkpoint['arguments'] and checkpoint['arguments'][k] != getattr(args, k):\n",
    "                a, b = checkpoint['arguments'][k], getattr(args, k)\n",
    "                Run.warn(f\"Got checkpoint['arguments']['{k}'] != args.{k} (i.e., {a} != {b})\")\n",
    "\n",
    "    if 'arguments' in checkpoint:\n",
    "        if args.rank < 1:\n",
    "            print(ujson.dumps(checkpoint['arguments'], indent=4))\n",
    "\n",
    "    if do_print:\n",
    "        print('\\n')\n",
    "\n",
    "    return colbert, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ef47ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args, do_print=True):\n",
    "    colbert = ColBERT.from_pretrained(\n",
    "        'bert-base-uncased', \n",
    "        query_maxlen=args.query_maxlen,\n",
    "        doc_maxlen=args.doc_maxlen,\n",
    "        dim=args.dim,\n",
    "        similarity_metric=args.similarity,\n",
    "        mask_punctuation=args.mask_punctuation\n",
    "    )\n",
    "    colbert = colbert.to(DEVICE)\n",
    "\n",
    "    print_message(\"#> Loading model checkpoint.\", condition=do_print)\n",
    "\n",
    "    checkpoint = load_checkpoint(args.checkpoint, colbert, do_print=do_print)\n",
    "\n",
    "    colbert.eval()\n",
    "\n",
    "    return colbert, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab132be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/inference.py\n",
    "'''\n",
    "from colbert.modeling.colbert import ColBERT--\n",
    "from colbert.modeling.tokenization import QueryTokenizer, DocTokenizer--\n",
    "from colbert.utils.amp import MixedPrecisionManager--\n",
    "from colbert.parameters import DEVICE--\n",
    "'''\n",
    "class ModelInference():\n",
    "    def __init__(self, colbert: ColBERT, amp=False):\n",
    "        assert colbert.training is False\n",
    "\n",
    "        self.colbert = colbert\n",
    "        self.query_tokenizer = QueryTokenizer(colbert.query_maxlen)\n",
    "        self.doc_tokenizer = DocTokenizer(colbert.doc_maxlen)\n",
    "\n",
    "        self.amp_manager = MixedPrecisionManager(amp)\n",
    "\n",
    "    def query(self, *args, to_cpu=False, **kw_args):\n",
    "        with torch.no_grad():\n",
    "            with self.amp_manager.context():\n",
    "                Q = self.colbert.query(*args, **kw_args)\n",
    "                return Q.cpu() if to_cpu else Q\n",
    "\n",
    "    def doc(self, *args, to_cpu=False, **kw_args):\n",
    "        with torch.no_grad():\n",
    "            with self.amp_manager.context():\n",
    "                D = self.colbert.doc(*args, **kw_args)\n",
    "                return D.cpu() if to_cpu else D\n",
    "\n",
    "    def queryFromText(self, queries, bsize=None, to_cpu=False):\n",
    "        if bsize:\n",
    "            batches = self.query_tokenizer.tensorize(queries, bsize=bsize)\n",
    "            batches = [self.query(input_ids, attention_mask, to_cpu=to_cpu) for input_ids, attention_mask in batches]\n",
    "            return torch.cat(batches)\n",
    "\n",
    "        input_ids, attention_mask = self.query_tokenizer.tensorize(queries)\n",
    "        return self.query(input_ids, attention_mask)\n",
    "\n",
    "    def docFromText(self, docs, bsize=None, keep_dims=True, to_cpu=False):\n",
    "        if bsize:\n",
    "            batches, reverse_indices = self.doc_tokenizer.tensorize(docs, bsize=bsize)\n",
    "\n",
    "            batches = [self.doc(input_ids, attention_mask, keep_dims=keep_dims, to_cpu=to_cpu)\n",
    "                       for input_ids, attention_mask in batches]\n",
    "\n",
    "            if keep_dims:\n",
    "                D = _stack_3D_tensors(batches)\n",
    "                return D[reverse_indices]\n",
    "\n",
    "            D = [d for batch in batches for d in batch]\n",
    "            return [D[idx] for idx in reverse_indices.tolist()]\n",
    "\n",
    "        input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)\n",
    "        return self.doc(input_ids, attention_mask, keep_dims=keep_dims)\n",
    "\n",
    "    def score(self, Q, D, mask=None, lengths=None, explain=False):\n",
    "        if lengths is not None:\n",
    "            assert mask is None, \"don't supply both mask and lengths\"\n",
    "\n",
    "            mask = torch.arange(D.size(1), device=DEVICE) + 1\n",
    "            mask = mask.unsqueeze(0) <= lengths.to(DEVICE).unsqueeze(-1)\n",
    "\n",
    "        scores = (D @ Q)\n",
    "        scores = scores if mask is None else scores * mask.unsqueeze(-1)\n",
    "        scores = scores.max(1)\n",
    "\n",
    "        if explain:\n",
    "            assert False, \"TODO\"\n",
    "\n",
    "        return scores.values.sum(-1).cpu()\n",
    "\n",
    "\n",
    "def _stack_3D_tensors(groups):\n",
    "    bsize = sum([x.size(0) for x in groups])\n",
    "    maxlen = max([x.size(1) for x in groups])\n",
    "    hdim = groups[0].size(2)\n",
    "\n",
    "    output = torch.zeros(bsize, maxlen, hdim, device=groups[0].device, dtype=groups[0].dtype)\n",
    "\n",
    "    offset = 0\n",
    "    for x in groups:\n",
    "        endpos = offset + x.size(0)\n",
    "        output[offset:endpos, :x.size(1)] = x\n",
    "        offset = endpos\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "352216e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing/encoder.py\n",
    "\n",
    "'''\n",
    "from colbert.modeling.inference import ModelInference\n",
    "from colbert.evaluation.loaders import load_colbert\n",
    "from colbert.utils.utils import print_message\n",
    "\n",
    "from colbert.indexing.index_manager import IndexManager\n",
    "'''\n",
    "\n",
    "\n",
    "class CollectionEncoder():\n",
    "    def __init__(self, args, process_idx, num_processes):\n",
    "        self.args = args\n",
    "        self.collection = args.collection\n",
    "        self.process_idx = process_idx\n",
    "        self.num_processes = num_processes\n",
    "\n",
    "        assert 0.5 <= args.chunksize <= 128.0\n",
    "        max_bytes_per_file = args.chunksize * (1024*1024*1024)\n",
    "\n",
    "        max_bytes_per_doc = (self.args.doc_maxlen * self.args.dim * 2.0)\n",
    "\n",
    "        # Determine subset sizes for output\n",
    "        minimum_subset_size = 10_000\n",
    "        maximum_subset_size = max_bytes_per_file / max_bytes_per_doc\n",
    "        maximum_subset_size = max(minimum_subset_size, maximum_subset_size)\n",
    "        self.possible_subset_sizes = [int(maximum_subset_size)]\n",
    "\n",
    "        self.print_main(\"#> Local args.bsize =\", args.bsize)\n",
    "        self.print_main(\"#> args.index_root =\", args.index_root)\n",
    "        self.print_main(f\"#> self.possible_subset_sizes = {self.possible_subset_sizes}\")\n",
    "\n",
    "        self._load_model()\n",
    "        self.indexmgr = IndexManager(args.dim)\n",
    "        self.iterator = self._initialize_iterator()\n",
    "\n",
    "    def _initialize_iterator(self):\n",
    "        return open(self.collection)\n",
    "\n",
    "    def _saver_thread(self):\n",
    "        for args in iter(self.saver_queue.get, None):\n",
    "            self._save_batch(*args)\n",
    "\n",
    "    def _load_model(self):\n",
    "        self.colbert, self.checkpoint = load_colbert(self.args, do_print=(self.process_idx == 0))\n",
    "        self.colbert = self.colbert.cuda()\n",
    "        self.colbert.eval()\n",
    "\n",
    "        self.inference = ModelInference(self.colbert, amp=self.args.amp)\n",
    "\n",
    "    def encode(self):\n",
    "        self.saver_queue = queue.Queue(maxsize=3)\n",
    "        thread = threading.Thread(target=self._saver_thread)\n",
    "        thread.start()\n",
    "\n",
    "        t0 = time.time()\n",
    "        local_docs_processed = 0\n",
    "\n",
    "        for batch_idx, (offset, lines, owner) in enumerate(self._batch_passages(self.iterator)):\n",
    "            if owner != self.process_idx:\n",
    "                continue\n",
    "\n",
    "            t1 = time.time()\n",
    "            batch = self._preprocess_batch(offset, lines)\n",
    "            embs, doclens = self._encode_batch(batch_idx, batch)\n",
    "\n",
    "            t2 = time.time()\n",
    "            self.saver_queue.put((batch_idx, embs, offset, doclens))\n",
    "\n",
    "            t3 = time.time()\n",
    "            local_docs_processed += len(lines)\n",
    "            overall_throughput = compute_throughput(local_docs_processed, t0, t3)\n",
    "            this_encoding_throughput = compute_throughput(len(lines), t1, t2)\n",
    "            this_saving_throughput = compute_throughput(len(lines), t2, t3)\n",
    "\n",
    "            self.print(f'#> Completed batch #{batch_idx} (starting at passage #{offset}) \\t\\t'\n",
    "                          f'Passages/min: {overall_throughput} (overall), ',\n",
    "                          f'{this_encoding_throughput} (this encoding), ',\n",
    "                          f'{this_saving_throughput} (this saving)')\n",
    "        self.saver_queue.put(None)\n",
    "\n",
    "        self.print(\"#> Joining saver thread.\")\n",
    "        thread.join()\n",
    "\n",
    "    def _batch_passages(self, fi):\n",
    "        \"\"\"\n",
    "        Must use the same seed across processes!\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "\n",
    "        offset = 0\n",
    "        for owner in itertools.cycle(range(self.num_processes)):\n",
    "            batch_size = np.random.choice(self.possible_subset_sizes)\n",
    "\n",
    "            L = [line for _, line in zip(range(batch_size), fi)]\n",
    "\n",
    "            if len(L) == 0:\n",
    "                break  # EOF\n",
    "\n",
    "            yield (offset, L, owner)\n",
    "            offset += len(L)\n",
    "\n",
    "            if len(L) < batch_size:\n",
    "                break  # EOF\n",
    "\n",
    "        self.print(\"[NOTE] Done with local share.\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def _preprocess_batch(self, offset, lines):\n",
    "        endpos = offset + len(lines)\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        for line_idx, line in zip(range(offset, endpos), lines):\n",
    "            line_parts = line.strip().split('\\t')\n",
    "\n",
    "            pid, passage, *other = line_parts\n",
    "\n",
    "            assert len(passage) >= 1\n",
    "\n",
    "            if len(other) >= 1:\n",
    "                title, *_ = other\n",
    "                passage = title + ' | ' + passage\n",
    "\n",
    "            batch.append(passage)\n",
    "\n",
    "            assert pid == 'id' or int(pid) == line_idx\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def _encode_batch(self, batch_idx, batch):\n",
    "        with torch.no_grad():\n",
    "            embs = self.inference.docFromText(batch, bsize=self.args.bsize, keep_dims=False)\n",
    "            assert type(embs) is list\n",
    "            assert len(embs) == len(batch)\n",
    "\n",
    "            local_doclens = [d.size(0) for d in embs]\n",
    "            embs = torch.cat(embs)\n",
    "\n",
    "        return embs, local_doclens\n",
    "\n",
    "    def _save_batch(self, batch_idx, embs, offset, doclens):\n",
    "        start_time = time.time()\n",
    "\n",
    "        output_path = os.path.join(self.args.index_path, \"{}.pt\".format(batch_idx))\n",
    "        output_sample_path = os.path.join(self.args.index_path, \"{}.sample\".format(batch_idx))\n",
    "        doclens_path = os.path.join(self.args.index_path, 'doclens.{}.json'.format(batch_idx))\n",
    "\n",
    "        # Save the embeddings.\n",
    "        self.indexmgr.save(embs, output_path)\n",
    "        self.indexmgr.save(embs[torch.randint(0, high=embs.size(0), size=(embs.size(0) // 20,))], output_sample_path)\n",
    "\n",
    "        # Save the doclens.\n",
    "        with open(doclens_path, 'w') as output_doclens:\n",
    "            ujson.dump(doclens, output_doclens)\n",
    "\n",
    "        throughput = compute_throughput(len(doclens), start_time, time.time())\n",
    "        self.print_main(\"#> Saved batch #{} to {} \\t\\t\".format(batch_idx, output_path),\n",
    "                        \"Saving Throughput =\", throughput, \"passages per minute.\\n\")\n",
    "\n",
    "    def print(self, *args):\n",
    "        print_message(\"[\" + str(self.process_idx) + \"]\", \"\\t\\t\", *args)\n",
    "\n",
    "    def print_main(self, *args):\n",
    "        if self.process_idx == 0:\n",
    "            self.print(*args)\n",
    "\n",
    "\n",
    "def compute_throughput(size, t0, t1):\n",
    "    throughput = size / (t1 - t0) * 60\n",
    "\n",
    "    if throughput > 1000 * 1000:\n",
    "        throughput = throughput / (1000*1000)\n",
    "        throughput = round(throughput, 1)\n",
    "        return '{}M'.format(throughput)\n",
    "\n",
    "    throughput = throughput / (1000)\n",
    "    throughput = round(throughput, 1)\n",
    "    return '{}k'.format(throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e22beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing/index_manager.py\n",
    "'''from colbert.utils.utils import print_message'''\n",
    "class IndexManager():\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def save(self, tensor, path_prefix):\n",
    "        torch.save(tensor, path_prefix)\n",
    "\n",
    "\n",
    "def load_index_part(filename, verbose=True):\n",
    "    part = torch.load(filename)\n",
    "\n",
    "    if type(part) == list:  # for backward compatibility\n",
    "        part = torch.cat(part)\n",
    "\n",
    "    return part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b253bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/utils/amp.py\n",
    "'''\n",
    "from contextlib import contextmanager\n",
    "from colbert.utils.utils import NullContextManager\n",
    "from packaging import version\n",
    "'''\n",
    "v = version.parse\n",
    "PyTorch_over_1_6  = v(torch.__version__) >= v('1.6')\n",
    "\n",
    "class MixedPrecisionManager():\n",
    "    def __init__(self, activated):\n",
    "        assert (not activated) or PyTorch_over_1_6, \"Cannot use AMP for PyTorch version < 1.6\"\n",
    "\n",
    "        self.activated = activated\n",
    "\n",
    "        if self.activated:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def context(self):\n",
    "        return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
    "\n",
    "    def backward(self, loss):\n",
    "        if self.activated:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "    def step(self, colbert, optimizer):\n",
    "        if self.activated:\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0)\n",
    "\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec44d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/logging.py\n",
    "class Logger():\n",
    "    def __init__(self, rank, run):\n",
    "        self.rank = rank\n",
    "        self.is_main = self.rank in [-1, 0]\n",
    "        self.run = run\n",
    "        self.logs_path = os.path.join(self.run.path, \"logs/\")\n",
    "\n",
    "        if self.is_main:\n",
    "            self._init_mlflow()\n",
    "            self.initialized_tensorboard = False\n",
    "            create_directory(self.logs_path)\n",
    "\n",
    "    def _init_mlflow(self):\n",
    "        mlflow.set_tracking_uri('file://' + os.path.join(self.run.experiments_root, \"logs/mlruns/\"))\n",
    "        mlflow.set_experiment('/'.join([self.run.experiment, self.run.script]))\n",
    "        \n",
    "        mlflow.set_tag('experiment', self.run.experiment)\n",
    "        mlflow.set_tag('name', self.run.name)\n",
    "        mlflow.set_tag('path', self.run.path)\n",
    "\n",
    "    def _init_tensorboard(self):\n",
    "        root = os.path.join(self.run.experiments_root, \"logs/tensorboard/\")\n",
    "        logdir = '__'.join([self.run.experiment, self.run.script, self.run.name])\n",
    "        logdir = os.path.join(root, logdir)\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=logdir)\n",
    "        self.initialized_tensorboard = True\n",
    "\n",
    "    def _log_exception(self, etype, value, tb):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        output_path = os.path.join(self.logs_path, 'exception.txt')\n",
    "        trace = ''.join(traceback.format_exception(etype, value, tb)) + '\\n'\n",
    "        print_message(trace, '\\n\\n')\n",
    "\n",
    "        self.log_new_artifact(output_path, trace)\n",
    "\n",
    "    def _log_all_artifacts(self):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        mlflow.log_artifacts(self.logs_path)\n",
    "\n",
    "    def _log_args(self, args):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        for key in vars(args):\n",
    "            value = getattr(args, key)\n",
    "            if type(value) in [int, float, str, bool]:\n",
    "                mlflow.log_param(key, value)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.json'), 'w') as output_metadata:\n",
    "            ujson.dump(args.input_arguments.__dict__, output_metadata, indent=4)\n",
    "            output_metadata.write('\\n')\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.txt'), 'w') as output_metadata:\n",
    "            output_metadata.write(' '.join(sys.argv) + '\\n')\n",
    "\n",
    "    def log_metric(self, name, value, step, log_to_mlflow=True):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        if not self.initialized_tensorboard:\n",
    "            self._init_tensorboard()\n",
    "\n",
    "        if log_to_mlflow:\n",
    "            mlflow.log_metric(name, value, step=step)\n",
    "        self.writer.add_scalar(name, value, step)\n",
    "\n",
    "    def log_new_artifact(self, path, content):\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        mlflow.log_artifact(path)\n",
    "\n",
    "    def warn(self, *args):\n",
    "        msg = print_message('[WARNING]', '\\t', *args)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'warnings.txt'), 'a') as output_metadata:\n",
    "            output_metadata.write(msg + '\\n\\n\\n')\n",
    "\n",
    "    def info_all(self, *args):\n",
    "        print_message('[' + str(self.rank) + ']', '\\t', *args)\n",
    "\n",
    "    def info(self, *args):\n",
    "        if self.is_main:\n",
    "            print_message(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4421c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/utils.py\n",
    "def print_message(*s, condition=True):\n",
    "    s = ' '.join([str(x) for x in s])\n",
    "    msg = \"[{}] {}\".format(datetime.datetime.now().strftime(\"%b %d, %H:%M:%S\"), s)\n",
    "\n",
    "    if condition:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "    return msg\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    format_str = \"%Y-%m-%d_%H.%M.%S\"\n",
    "    result = datetime.datetime.now().strftime(format_str)\n",
    "    return result\n",
    "def create_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        print('\\n')\n",
    "        print_message(\"#> Note: Output directory\", path, 'already exists\\n\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "        print_message(\"#> Creating directory\", path, '\\n\\n')\n",
    "        os.makedirs(path)\n",
    "class NullContextManager(object):\n",
    "    def __init__(self, dummy_resource=None):\n",
    "        self.dummy_resource = dummy_resource\n",
    "    def __enter__(self):\n",
    "        return self.dummy_resource\n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "    \n",
    "def distributed_init(rank):\n",
    "    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])\n",
    "    nranks = max(1, nranks)\n",
    "    is_distributed = nranks > 1\n",
    "\n",
    "    if rank == 0:\n",
    "        print('nranks =', nranks, '\\t num_gpus =', torch.cuda.device_count())\n",
    "\n",
    "    if is_distributed:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        torch.cuda.set_device(rank % num_gpus)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    return nranks, is_distributed\n",
    "\n",
    "def distributed_barrier(rank):\n",
    "    if rank >= 0:\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "class NullContextManager(object):\n",
    "    def __init__(self, dummy_resource=None):\n",
    "        self.dummy_resource = dummy_resource\n",
    "    def __enter__(self):\n",
    "        return self.dummy_resource\n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "def load_checkpoint(path, model, optimizer=None, do_print=True):\n",
    "    if do_print:\n",
    "        print_message(\"#> Loading checkpoint\", path, \"..\")\n",
    "\n",
    "    if path.startswith(\"http:\") or path.startswith(\"https:\"):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k\n",
    "        if k[:7] == 'module.':\n",
    "            name = k[7:]\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    checkpoint['model_state_dict'] = new_state_dict\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        print_message(\"[WARNING] Loading checkpoint with strict=False\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if do_print:\n",
    "        print_message(\"#> checkpoint['epoch'] =\", checkpoint['epoch'])\n",
    "        print_message(\"#> checkpoint['batch'] =\", checkpoint['batch'])\n",
    "\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e17827e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/runs.py\n",
    "'''\n",
    "import colbert.utils.distributed as distributed\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from colbert.utils.logging import Logger\n",
    "from colbert.utils.utils import timestamp, create_directory, print_message\n",
    "'''\n",
    "class _RunManager():\n",
    "    def __init__(self):\n",
    "        self.experiments_root = None\n",
    "        self.experiment = None\n",
    "        self.path = None\n",
    "        self.script = self._get_script_name()\n",
    "        self.name = self._generate_default_run_name()\n",
    "        self.original_name = self.name\n",
    "        self.exit_status = 'FINISHED'\n",
    "\n",
    "        self._logger = None\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def init(self, rank, root, experiment, name):\n",
    "        assert '/' not in experiment, experiment\n",
    "        assert '/' not in name, name\n",
    "\n",
    "        self.experiments_root = os.path.abspath(root)\n",
    "        self.experiment = experiment\n",
    "        self.name = name\n",
    "        self.path = os.path.join(self.experiments_root, self.experiment, self.script, self.name)\n",
    "\n",
    "        if rank < 1:\n",
    "            if os.path.exists(self.path):\n",
    "                print('\\n\\n')\n",
    "                print_message(\"It seems that \", self.path, \" already exists.\")\n",
    "                print_message(\"Do you want to overwrite it? \\t yes/no \\n\")\n",
    "\n",
    "                # TODO: This should timeout and exit (i.e., fail) given no response for 60 seconds.\n",
    "\n",
    "                response = input()\n",
    "                if response.strip() != 'yes':\n",
    "                    assert not os.path.exists(self.path), self.path\n",
    "            else:\n",
    "                create_directory(self.path)\n",
    "\n",
    "        distributed_barrier(rank)\n",
    "\n",
    "        self._logger = Logger(rank, self)\n",
    "        self._log_args = self._logger._log_args\n",
    "        self.warn = self._logger.warn\n",
    "        self.info = self._logger.info\n",
    "        self.info_all = self._logger.info_all\n",
    "        self.log_metric = self._logger.log_metric\n",
    "        self.log_new_artifact = self._logger.log_new_artifact\n",
    "\n",
    "    def _generate_default_run_name(self):\n",
    "        return timestamp()\n",
    "\n",
    "    def _get_script_name(self):\n",
    "        return os.path.basename('main'.__file__) if '__file__' in dir('main') else 'none'\n",
    "#         return os.path.basename(__main__.__file__) if '__file__' in dir(__main__) else 'none'\n",
    "\n",
    "    @contextmanager\n",
    "    def context(self, consider_failed_if_interrupted=True):\n",
    "        try:\n",
    "            yield\n",
    "\n",
    "        except KeyboardInterrupt as ex:\n",
    "            print('\\n\\nInterrupted\\n\\n')\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            if consider_failed_if_interrupted:\n",
    "                self.exit_status = 'KILLED'  # mlflow.entities.RunStatus.KILLED\n",
    "\n",
    "            sys.exit(128 + 2)\n",
    "\n",
    "        except Exception as ex:\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            self.exit_status = 'FAILED'  # mlflow.entities.RunStatus.FAILED\n",
    "\n",
    "            raise ex\n",
    "\n",
    "        finally:\n",
    "            total_seconds = str(time.time() - self.start_time) + '\\n'\n",
    "            original_name = str(self.original_name)\n",
    "            name = str(self.name)\n",
    "\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'elapsed.txt'), total_seconds)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.original.txt'), original_name)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.txt'), name)\n",
    "\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            mlflow.end_run(status=self.exit_status)\n",
    "\n",
    "\n",
    "Run = _RunManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8456116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/parser.py\n",
    "'''\n",
    "import colbert.utils.distributed as distributed\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.utils.utils import print_message, timestamp, create_directory\n",
    "'''\n",
    "\n",
    "class Arguments():\n",
    "    def __init__(self, description):\n",
    "        self.parser = ArgumentParser(description=description)\n",
    "        self.checks = []\n",
    "\n",
    "        self.add_argument('--root', dest='root', default='/experiments')\n",
    "        self.add_argument('--experiment', dest='experiment', default='MSMARCO-psg')\n",
    "        self.add_argument('--run', dest='run', default=Run.name)\n",
    "\n",
    "        self.add_argument('--local_rank', dest='rank', default=-1, type=int)\n",
    "\n",
    "    def add_model_parameters(self):\n",
    "        # Core Arguments\n",
    "        self.add_argument('--similarity', dest='similarity', default='cosine', choices=['cosine', 'l2'])\n",
    "        self.add_argument('--dim', dest='dim', default=128, type=int)\n",
    "        self.add_argument('--query_maxlen', dest='query_maxlen', default=32, type=int)\n",
    "        self.add_argument('--doc_maxlen', dest='doc_maxlen', default=180, type=int)\n",
    "\n",
    "        # Filtering-related Arguments\n",
    "        self.add_argument('--mask-punctuation', dest='mask_punctuation', default=True, action='store_true')\n",
    "\n",
    "    def add_model_inference_parameters(self):\n",
    "        self.add_argument('--checkpoint', dest='checkpoint', default=True, type=str)\n",
    "        self.add_argument('--bsize', dest='bsize', default=256, type=int)\n",
    "        self.add_argument('--amp', dest='amp', default=False, action='store_true')\n",
    "\n",
    "    def add_indexing_input(self):\n",
    "        self.add_argument('--collection', dest='collection', default='collection.tsv', type=str)\n",
    "        self.add_argument('--index_root', dest='index_root', default='root')\n",
    "        self.add_argument('--index_name', dest='index_name', default='MSMARCO.L2.32x200k')\n",
    "\n",
    "    def add_argument(self, *args, **kw_args):\n",
    "        return self.parser.add_argument(*args, **kw_args)\n",
    "\n",
    "    def check_arguments(self, args):\n",
    "        for check in self.checks:\n",
    "            check(args)\n",
    "\n",
    "    def parse(self):\n",
    "        args = self.parser.parse_args()\n",
    "        self.check_arguments(args)\n",
    "\n",
    "        args.input_arguments = copy.deepcopy(args)\n",
    "\n",
    "        args.nranks, args.distributed = distributed_init(args.rank)\n",
    "\n",
    "        args.nthreads = int(max(os.cpu_count(), faiss.omp_get_max_threads()) * 0.8)\n",
    "        args.nthreads = max(1, args.nthreads // args.nranks)\n",
    "\n",
    "        if args.nranks > 1:\n",
    "            print_message(f\"#> Restricting number of threads for FAISS to {args.nthreads} per process\",\n",
    "                          condition=(args.rank == 0))\n",
    "            faiss.omp_set_num_threads(args.nthreads)\n",
    "\n",
    "        Run.init(args.rank, args.root, args.experiment, args.run)\n",
    "        Run._log_args(args)\n",
    "        Run.info(args.input_arguments.__dict__, '\\n')\n",
    "\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca941232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.utils.parser import Arguments\n",
    "import colbert.utils.distributed as distributed\n",
    "\n",
    "from colbert.utils.utils import print_message, create_directory\n",
    "from colbert.indexing.encoder import CollectionEncoder\n",
    "'''\n",
    "\n",
    "\n",
    "def main():\n",
    "    random.seed(12345)\n",
    "\n",
    "    parser = Arguments(description='Precomputing document representations with ColBERT.')\n",
    "\n",
    "    parser.add_model_parameters()\n",
    "    parser.add_model_inference_parameters()\n",
    "    parser.add_indexing_input()\n",
    "    \n",
    "    parser.add_argument('--chunksize', dest='chunksize', default=6.0, required=False, type=float)   # in GiBs\n",
    "    parser.add_argument('-f')\n",
    "    \n",
    "    args = parser.parse()\n",
    "\n",
    "    with Run.context():\n",
    "        args.index_path = os.path.join(args.index_root, args.index_name)\n",
    "        assert not os.path.exists(args.index_path), args.index_path\n",
    "\n",
    "        distributed_barrier(args.rank)\n",
    "\n",
    "        if args.rank < 1:\n",
    "            create_directory(args.index_root)\n",
    "            create_directory(args.index_path)\n",
    "\n",
    "        distributed_barrier(args.rank)\n",
    "\n",
    "        process_idx = max(0, args.rank)\n",
    "        encoder = CollectionEncoder(args, process_idx=process_idx, num_processes=args.nranks)\n",
    "        encoder.encode()\n",
    "\n",
    "        distributed_barrier(args.rank)\n",
    "\n",
    "        # Save metadata.\n",
    "        if args.rank < 1:\n",
    "            metadata_path = os.path.join(args.index_path, 'metadata.json')\n",
    "            print_message(\"Saving (the following) metadata to\", metadata_path, \"..\")\n",
    "            print(args.input_arguments)\n",
    "\n",
    "            with open(metadata_path, 'w') as output_metadata:\n",
    "                ujson.dump(args.input_arguments.__dict__, output_metadata)\n",
    "\n",
    "        distributed_barrier(args.rank)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# TODO: Add resume functionality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
