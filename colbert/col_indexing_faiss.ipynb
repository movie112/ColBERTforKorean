{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd605ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import ujson\n",
    "from itertools import accumulate\n",
    "import math\n",
    "from math import ceil\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "import copy\n",
    "import faiss\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from transformers import BertTokenizerFast, BertPreTrainedModel, BertModel, BertConfig, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c29de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM = faiss.get_num_gpus()\n",
    "NUM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27eb7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing/loaders.py\n",
    "'''\n",
    "from colbert.utils.utils import print_message\n",
    "'''\n",
    "def get_parts(directory):\n",
    "    extension = '.pt'\n",
    "\n",
    "    parts = sorted([int(filename[: -1 * len(extension)]) for filename in os.listdir(directory)\n",
    "                    if filename.endswith(extension)])\n",
    "\n",
    "    assert list(range(len(parts))) == parts, parts\n",
    "\n",
    "    # Integer-sortedness matters.\n",
    "    parts_paths = [os.path.join(directory, '{}{}'.format(filename, extension)) for filename in parts]\n",
    "    samples_paths = [os.path.join(directory, '{}.sample'.format(filename)) for filename in parts]\n",
    "\n",
    "    return parts, parts_paths, samples_paths\n",
    "\n",
    "\n",
    "def load_doclens(directory, flatten=True):\n",
    "    parts, _, _ = get_parts(directory)\n",
    "\n",
    "    doclens_filenames = [os.path.join(directory, 'doclens.{}.json'.format(filename)) for filename in parts]\n",
    "    all_doclens = [ujson.load(open(filename)) for filename in doclens_filenames]\n",
    "\n",
    "    if flatten:\n",
    "        all_doclens = [x for sub_doclens in all_doclens for x in sub_doclens]\n",
    "\n",
    "    return all_doclens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146759df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing/index_manager.py\n",
    "'''from colbert.utils.utils import print_message'''\n",
    "class IndexManager():\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def save(self, tensor, path_prefix):\n",
    "        torch.save(tensor, path_prefix)\n",
    "\n",
    "\n",
    "def load_index_part(filename, verbose=True):\n",
    "    part = torch.load(filename)\n",
    "\n",
    "    if type(part) == list:  # for backward compatibility\n",
    "        part = torch.cat(part)\n",
    "\n",
    "    return part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e82eabdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing/faiss_index.py\n",
    "'''\n",
    "from colbert.indexing.faiss_index_gpu import FaissIndexGPU\n",
    "from colbert.utils.utils import print_message\n",
    "'''\n",
    "class FaissIndex():\n",
    "    def __init__(self, dim, partitions):\n",
    "        self.dim = dim\n",
    "        self.partitions = partitions\n",
    "\n",
    "        self.gpu = FaissIndexGPU()\n",
    "        self.quantizer, self.index = self._create_index()\n",
    "        self.offset = 0\n",
    "\n",
    "    def _create_index(self):\n",
    "        quantizer = faiss.IndexFlatL2(self.dim)  # faiss.IndexHNSWFlat(dim, 32)\n",
    "        index = faiss.IndexIVFPQ(quantizer, self.dim, self.partitions, 16, 8)\n",
    "\n",
    "        return quantizer, index\n",
    "\n",
    "    def train(self, train_data):\n",
    "        print_message(f\"#> Training now (using {self.gpu.ngpu} GPUs)...\")\n",
    "\n",
    "        if self.gpu.ngpu > 0:\n",
    "            self.gpu.training_initialize(self.index, self.quantizer)\n",
    "\n",
    "        s = time.time()\n",
    "        self.index.train(train_data)\n",
    "        print(time.time() - s)\n",
    "\n",
    "        if self.gpu.ngpu > 0:\n",
    "            self.gpu.training_finalize()\n",
    "\n",
    "    def add(self, data):\n",
    "        print_message(f\"Add data with shape {data.shape} (offset = {self.offset})..\")\n",
    "\n",
    "        if self.gpu.ngpu > 0 and self.offset == 0:\n",
    "            self.gpu.adding_initialize(self.index)\n",
    "\n",
    "        if self.gpu.ngpu > 0:\n",
    "            self.gpu.add(self.index, data, self.offset)\n",
    "        else:\n",
    "            self.index.add(data)\n",
    "\n",
    "        self.offset += data.shape[0]\n",
    "\n",
    "    def save(self, output_path):\n",
    "        print_message(f\"Writing index to {output_path} ...\")\n",
    "\n",
    "        self.index.nprobe = 10  # just a default\n",
    "        faiss.write_index(self.index, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d9c17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing/faiss_index_gpu.py\n",
    "\n",
    "class FaissIndexGPU():\n",
    "    def __init__(self):\n",
    "        self.ngpu = NUM\n",
    "\n",
    "        if self.ngpu == 0:\n",
    "            return\n",
    "\n",
    "        self.tempmem = 1 << 33\n",
    "        self.max_add_per_gpu = 1 << 25\n",
    "        self.max_add = self.max_add_per_gpu * self.ngpu\n",
    "        self.add_batch_size = 65536\n",
    "\n",
    "        self.gpu_resources = self._prepare_gpu_resources()\n",
    "\n",
    "    def _prepare_gpu_resources(self):\n",
    "        print_message(f\"Preparing resources for {self.ngpu} GPUs.\")\n",
    "\n",
    "        gpu_resources = []\n",
    "\n",
    "        for _ in range(self.ngpu):\n",
    "            res = faiss.StandardGpuResources()\n",
    "            if self.tempmem >= 0:\n",
    "                res.setTempMemory(self.tempmem)\n",
    "            gpu_resources.append(res)\n",
    "\n",
    "        return gpu_resources\n",
    "\n",
    "    def _make_vres_vdev(self):\n",
    "        \"\"\"\n",
    "        return vectors of device ids and resources useful for gpu_multiple\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        vres = faiss.GpuResourcesVector()\n",
    "        vdev = faiss.IntVector()\n",
    "\n",
    "        for i in range(self.ngpu):\n",
    "            vdev.push_back(i)\n",
    "            vres.push_back(self.gpu_resources[i])\n",
    "\n",
    "        return vres, vdev\n",
    "\n",
    "    def training_initialize(self, index, quantizer):\n",
    "        \"\"\"\n",
    "        The index and quantizer should be owned by caller.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        s = time.time()\n",
    "        self.index_ivf = faiss.extract_index_ivf(index)\n",
    "        self.clustering_index = faiss.index_cpu_to_all_gpus(quantizer)\n",
    "        self.index_ivf.clustering_index = self.clustering_index\n",
    "        print(time.time() - s)\n",
    "\n",
    "    def training_finalize(self):\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        s = time.time()\n",
    "        self.index_ivf.clustering_index = faiss.index_gpu_to_cpu(self.index_ivf.clustering_index)\n",
    "        print(time.time() - s)\n",
    "\n",
    "    def adding_initialize(self, index):\n",
    "        \"\"\"\n",
    "        The index should be owned by caller.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        self.co = faiss.GpuMultipleClonerOptions()\n",
    "        self.co.useFloat16 = True\n",
    "        self.co.useFloat16CoarseQuantizer = False\n",
    "        self.co.usePrecomputed = False\n",
    "        self.co.indicesOptions = faiss.INDICES_CPU\n",
    "        self.co.verbose = True\n",
    "        self.co.reserveVecs = self.max_add\n",
    "        self.co.shard = True\n",
    "        assert self.co.shard_type in (0, 1, 2)\n",
    "\n",
    "        self.vres, self.vdev = self._make_vres_vdev()\n",
    "        self.gpu_index = faiss.index_cpu_to_gpu_multiple(self.vres, self.vdev, index, self.co)\n",
    "\n",
    "    def add(self, index, data, offset):\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        t0 = time.time()\n",
    "        nb = data.shape[0]\n",
    "\n",
    "        for i0 in range(0, nb, self.add_batch_size):\n",
    "            i1 = min(i0 + self.add_batch_size, nb)\n",
    "            xs = data[i0:i1]\n",
    "\n",
    "            self.gpu_index.add_with_ids(xs, np.arange(offset+i0, offset+i1))\n",
    "\n",
    "            if self.max_add > 0 and self.gpu_index.ntotal > self.max_add:\n",
    "                self._flush_to_cpu(index, nb, offset)\n",
    "\n",
    "            print('\\r%d/%d (%.3f s)  ' % (i0, nb, time.time() - t0), end=' ')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        if self.gpu_index.ntotal > 0:\n",
    "            self._flush_to_cpu(index, nb, offset)\n",
    "\n",
    "        assert index.ntotal == offset+nb, (index.ntotal, offset+nb, offset, nb)\n",
    "        print(f\"add(.) time: %.3f s \\t\\t--\\t\\t index.ntotal = {index.ntotal}\" % (time.time() - t0))\n",
    "\n",
    "    def _flush_to_cpu(self, index, nb, offset):\n",
    "        print(\"Flush indexes to CPU\")\n",
    "\n",
    "        for i in range(self.ngpu):\n",
    "            index_src_gpu = faiss.downcast_index(self.gpu_index if self.ngpu == 1 else self.gpu_index.at(i))\n",
    "            index_src = faiss.index_gpu_to_cpu(index_src_gpu)\n",
    "\n",
    "            index_src.copy_subset_to(index, 0, offset, offset+nb)\n",
    "            index_src_gpu.reset()\n",
    "            index_src_gpu.reserveMemory(self.max_add)\n",
    "\n",
    "        if self.ngpu > 1:\n",
    "            try:\n",
    "                self.gpu_index.sync_with_shard_indexes()\n",
    "            except:\n",
    "                self.gpu_index.syncWithSubIndexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8689e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing/faiss.py\n",
    "'''\n",
    "from colbert.utils.utils import print_message, grouper\n",
    "from colbert.indexing.loaders import get_parts\n",
    "from colbert.indexing.index_manager import load_index_part\n",
    "from colbert.indexing.faiss_index import FaissIndex\n",
    "'''\n",
    "\n",
    "def get_faiss_index_name(args, offset=None, endpos=None):\n",
    "    partitions_info = '' if args.partitions is None else f'.{args.partitions}'\n",
    "    range_info = '' if offset is None else f'.{offset}-{endpos}'\n",
    "\n",
    "    return f'ivfpq{partitions_info}{range_info}.faiss'\n",
    "\n",
    "\n",
    "def load_sample(samples_paths, sample_fraction=None):\n",
    "    sample = []\n",
    "\n",
    "    for filename in samples_paths:\n",
    "        print_message(f\"#> Loading {filename} ...\")\n",
    "        part = load_index_part(filename)\n",
    "        if sample_fraction:\n",
    "            part = part[torch.randint(0, high=part.size(0), size=(int(part.size(0) * sample_fraction),))]\n",
    "        sample.append(part)\n",
    "\n",
    "    sample = torch.cat(sample).float().numpy()\n",
    "\n",
    "    print(\"#> Sample has shape\", sample.shape)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def prepare_faiss_index(slice_samples_paths, partitions, sample_fraction=None):\n",
    "    training_sample = load_sample(slice_samples_paths, sample_fraction=sample_fraction)\n",
    "\n",
    "    dim = training_sample.shape[-1]\n",
    "    index = FaissIndex(dim, partitions)\n",
    "\n",
    "    print_message(\"#> Training with the vectors...\")\n",
    "\n",
    "    index.train(training_sample)\n",
    "\n",
    "    print_message(\"Done training!\\n\")\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "SPAN = 3\n",
    "\n",
    "\n",
    "def index_faiss(args):\n",
    "    print_message(\"#> Starting..\")\n",
    "\n",
    "    parts, parts_paths, samples_paths = get_parts(args.index_path)\n",
    "\n",
    "    if args.sample is not None:\n",
    "        assert args.sample, args.sample\n",
    "        print_message(f\"#> Training with {round(args.sample * 100.0, 1)}% of *all* embeddings (provided --sample).\")\n",
    "        samples_paths = parts_paths\n",
    "\n",
    "    num_parts_per_slice = math.ceil(len(parts) / args.slices)\n",
    "\n",
    "    for slice_idx, part_offset in enumerate(range(0, len(parts), num_parts_per_slice)):\n",
    "        part_endpos = min(part_offset + num_parts_per_slice, len(parts))\n",
    "\n",
    "        slice_parts_paths = parts_paths[part_offset:part_endpos]\n",
    "        slice_samples_paths = samples_paths[part_offset:part_endpos]\n",
    "\n",
    "        if args.slices == 1:\n",
    "            faiss_index_name = get_faiss_index_name(args)\n",
    "        else:\n",
    "            faiss_index_name = get_faiss_index_name(args, offset=part_offset, endpos=part_endpos)\n",
    "\n",
    "        output_path = os.path.join(args.index_path, faiss_index_name)\n",
    "        print_message(f\"#> Processing slice #{slice_idx+1} of {args.slices} (range {part_offset}..{part_endpos}).\")\n",
    "        print_message(f\"#> Will write to {output_path}.\")\n",
    "\n",
    "        assert not os.path.exists(output_path), output_path\n",
    "\n",
    "        index = prepare_faiss_index(slice_samples_paths, args.partitions, args.sample)\n",
    "\n",
    "        loaded_parts = queue.Queue(maxsize=1)\n",
    "\n",
    "        def _loader_thread(thread_parts_paths):\n",
    "            for filenames in grouper(thread_parts_paths, SPAN, fillvalue=None):\n",
    "                sub_collection = [load_index_part(filename) for filename in filenames if filename is not None]\n",
    "                sub_collection = torch.cat(sub_collection)\n",
    "                sub_collection = sub_collection.float().numpy()\n",
    "                loaded_parts.put(sub_collection)\n",
    "\n",
    "        thread = threading.Thread(target=_loader_thread, args=(slice_parts_paths,))\n",
    "        thread.start()\n",
    "\n",
    "        print_message(\"#> Indexing the vectors...\")\n",
    "\n",
    "        for filenames in grouper(slice_parts_paths, SPAN, fillvalue=None):\n",
    "            print_message(\"#> Loading\", filenames, \"(from queue)...\")\n",
    "            sub_collection = loaded_parts.get()\n",
    "\n",
    "            print_message(\"#> Processing a sub_collection with shape\", sub_collection.shape)\n",
    "            index.add(sub_collection)\n",
    "\n",
    "        print_message(\"Done indexing!\")\n",
    "\n",
    "        index.save(output_path)\n",
    "\n",
    "        print_message(f\"\\n\\nDone! All complete (for slice #{slice_idx+1} of {args.slices})!\")\n",
    "\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5abe9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/utils.py\n",
    "def print_message(*s, condition=True):\n",
    "    s = ' '.join([str(x) for x in s])\n",
    "    msg = \"[{}] {}\".format(datetime.datetime.now().strftime(\"%b %d, %H:%M:%S\"), s)\n",
    "\n",
    "    if condition:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "    return msg\n",
    "def timestamp():\n",
    "    format_str = \"%Y-%m-%d_%H.%M.%S\"\n",
    "    result = datetime.datetime.now().strftime(format_str)\n",
    "    return result\n",
    "def create_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        print('\\n')\n",
    "        print_message(\"#> Note: Output directory\", path, 'already exists\\n\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "        print_message(\"#> Creating directory\", path, '\\n\\n')\n",
    "        os.makedirs(path)\n",
    "def distributed_init(rank):\n",
    "    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])\n",
    "    nranks = max(1, nranks)\n",
    "    is_distributed = nranks > 1\n",
    "\n",
    "    if rank == 0:\n",
    "        print('nranks =', nranks, '\\t num_gpus =', 1)#torch.cuda.device_count())\n",
    "\n",
    "    if is_distributed:\n",
    "        num_gpus = 1#torch.cuda.device_count()\n",
    "        torch.cuda.set_device(rank % num_gpus)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    return nranks, is_distributed\n",
    "def distributed_barrier(rank):\n",
    "    if rank >= 0:\n",
    "        torch.distributed_barrier()\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    import itertools\n",
    "    \"\"\"\n",
    "    Collect data into fixed-length chunks or blocks\n",
    "        Example: grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "        Source: https://docs.python.org/3/library/itertools.html#itertools-recipes\n",
    "    \"\"\"\n",
    "\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e7c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils/plgging.py\n",
    "class Logger():\n",
    "    def __init__(self, rank, run):\n",
    "        self.rank = rank\n",
    "        self.is_main = self.rank in [-1, 0]\n",
    "        self.run = run\n",
    "        self.logs_path = os.path.join(self.run.path, \"logs/\")\n",
    "\n",
    "        if self.is_main:\n",
    "            self._init_mlflow()\n",
    "            self.initialized_tensorboard = False\n",
    "            create_directory(self.logs_path)\n",
    "\n",
    "    def _init_mlflow(self):\n",
    "        mlflow.set_tracking_uri('file://' + os.path.join(self.run.experiments_root, \"logs/mlruns/\"))\n",
    "        mlflow.set_experiment('/'.join([self.run.experiment, self.run.script]))\n",
    "        \n",
    "        mlflow.set_tag('experiment', self.run.experiment)\n",
    "        mlflow.set_tag('name', self.run.name)\n",
    "        mlflow.set_tag('path', self.run.path)\n",
    "\n",
    "    def _init_tensorboard(self):\n",
    "        root = os.path.join(self.run.experiments_root, \"logs/tensorboard/\")\n",
    "        logdir = '__'.join([self.run.experiment, self.run.script, self.run.name])\n",
    "        logdir = os.path.join(root, logdir)\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=logdir)\n",
    "        self.initialized_tensorboard = True\n",
    "\n",
    "    def _log_exception(self, etype, value, tb):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        output_path = os.path.join(self.logs_path, 'exception.txt')\n",
    "        trace = ''.join(traceback.format_exception(etype, value, tb)) + '\\n'\n",
    "        print_message(trace, '\\n\\n')\n",
    "\n",
    "        self.log_new_artifact(output_path, trace)\n",
    "\n",
    "    def _log_all_artifacts(self):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        mlflow.log_artifacts(self.logs_path)\n",
    "\n",
    "    def _log_args(self, args):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        for key in vars(args):\n",
    "            value = getattr(args, key)\n",
    "            if type(value) in [int, float, str, bool]:\n",
    "                mlflow.log_param(key, value)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.json'), 'w') as output_metadata:\n",
    "            ujson.dump(args.input_arguments.__dict__, output_metadata, indent=4)\n",
    "            output_metadata.write('\\n')\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.txt'), 'w') as output_metadata:\n",
    "            output_metadata.write(' '.join(sys.argv) + '\\n')\n",
    "\n",
    "    def log_metric(self, name, value, step, log_to_mlflow=True):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        if not self.initialized_tensorboard:\n",
    "            self._init_tensorboard()\n",
    "\n",
    "        if log_to_mlflow:\n",
    "            mlflow.log_metric(name, value, step=step)\n",
    "        self.writer.add_scalar(name, value, step)\n",
    "\n",
    "    def log_new_artifact(self, path, content):\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        mlflow.log_artifact(path)\n",
    "\n",
    "    def warn(self, *args):\n",
    "        msg = print_message('[WARNING]', '\\t', *args)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'warnings.txt'), 'a') as output_metadata:\n",
    "            output_metadata.write(msg + '\\n\\n\\n')\n",
    "\n",
    "    def info_all(self, *args):\n",
    "        print_message('[' + str(self.rank) + ']', '\\t', *args)\n",
    "\n",
    "    def info(self, *args):\n",
    "        if self.is_main:\n",
    "            print_message(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3027e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/runs.py\n",
    "class _RunManager():\n",
    "    def __init__(self):\n",
    "        self.experiments_root = None\n",
    "        self.experiment = None\n",
    "        self.path = None\n",
    "        self.script = self._get_script_name()\n",
    "        self.name = self._generate_default_run_name()\n",
    "        self.original_name = self.name\n",
    "        self.exit_status = 'FINISHED'\n",
    "\n",
    "        self._logger = None\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def init(self, rank, root, experiment, name):\n",
    "        assert '/' not in experiment, experiment\n",
    "        assert '/' not in name, name\n",
    "\n",
    "        self.experiments_root = os.path.abspath(root)\n",
    "        self.experiment = experiment\n",
    "        self.name = name\n",
    "        self.path = os.path.join(self.experiments_root, self.experiment, self.script, self.name)\n",
    "\n",
    "        if rank < 1:\n",
    "            if os.path.exists(self.path):\n",
    "                print('\\n\\n')\n",
    "                print_message(\"It seems that \", self.path, \" already exists.\")\n",
    "                print_message(\"Do you want to overwrite it? \\t yes/no \\n\")\n",
    "\n",
    "                # TODO: This should timeout and exit (i.e., fail) given no response for 60 seconds.\n",
    "\n",
    "                response = input()\n",
    "                if response.strip() != 'yes':\n",
    "                    assert not os.path.exists(self.path), self.path\n",
    "            else:\n",
    "                create_directory(self.path)\n",
    "\n",
    "        distributed_barrier(rank)\n",
    "\n",
    "        self._logger = Logger(rank, self)\n",
    "        self._log_args = self._logger._log_args\n",
    "        self.warn = self._logger.warn\n",
    "        self.info = self._logger.info\n",
    "        self.info_all = self._logger.info_all\n",
    "        self.log_metric = self._logger.log_metric\n",
    "        self.log_new_artifact = self._logger.log_new_artifact\n",
    "\n",
    "    def _generate_default_run_name(self):\n",
    "        return timestamp()\n",
    "\n",
    "    def _get_script_name(self):\n",
    "        return os.path.basename('main'.__file__) if '__file__' in dir('main') else 'none'\n",
    "#         return os.path.basename(__main__.__file__) if '__file__' in dir(__main__) else 'none'\n",
    "\n",
    "\n",
    "    @contextmanager\n",
    "    def context(self, consider_failed_if_interrupted=True):\n",
    "        try:\n",
    "            yield\n",
    "\n",
    "        except KeyboardInterrupt as ex:\n",
    "            print('\\n\\nInterrupted\\n\\n')\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            if consider_failed_if_interrupted:\n",
    "                self.exit_status = 'KILLED'  # mlflow.entities.RunStatus.KILLED\n",
    "\n",
    "            sys.exit(128 + 2)\n",
    "\n",
    "        except Exception as ex:\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            self.exit_status = 'FAILED'  # mlflow.entities.RunStatus.FAILED\n",
    "\n",
    "            raise ex\n",
    "\n",
    "        finally:\n",
    "            total_seconds = str(time.time() - self.start_time) + '\\n'\n",
    "            original_name = str(self.original_name)\n",
    "            name = str(self.name)\n",
    "\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'elapsed.txt'), total_seconds)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.original.txt'), original_name)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.txt'), name)\n",
    "\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            mlflow.end_run(status=self.exit_status)\n",
    "\n",
    "\n",
    "Run = _RunManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d772f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils/parser.py\n",
    "class Arguments():\n",
    "    def __init__(self, description):\n",
    "        self.parser = ArgumentParser(description=description)\n",
    "        self.checks = []\n",
    "\n",
    "        self.add_argument('--root', dest='root', default='/')\n",
    "        self.add_argument('--experiment', dest='experiment', default='MSMARCO-psg')\n",
    "        self.add_argument('--run', dest='run', default=Run.name)\n",
    "\n",
    "        self.add_argument('--local_rank', dest='rank', default=-1, type=int)\n",
    "\n",
    "    def add_model_parameters(self):\n",
    "        # Core Arguments\n",
    "        self.add_argument('--similarity', dest='similarity', default='cosine', choices=['cosine', 'l2'])\n",
    "        self.add_argument('--dim', dest='dim', default=128, type=int)\n",
    "        self.add_argument('--query_maxlen', dest='query_maxlen', default=32, type=int)\n",
    "        self.add_argument('--doc_maxlen', dest='doc_maxlen', default=180, type=int)\n",
    "\n",
    "        # Filtering-related Arguments\n",
    "        self.add_argument('--mask-punctuation', dest='mask_punctuation', default=False, action='store_true')\n",
    "\n",
    "    def add_index_use_input(self):\n",
    "        self.add_argument('--index_root', dest='index_root', default='/')\n",
    "        self.add_argument('--index_name', dest='index_name', default='MSMARCO.L2.32x200k')\n",
    "        self.add_argument('--partitions', dest='partitions', default=32768, type=int)\n",
    "\n",
    "    def add_argument(self, *args, **kw_args):\n",
    "        return self.parser.add_argument(*args, **kw_args)\n",
    "\n",
    "    def check_arguments(self, args):\n",
    "        for check in self.checks:\n",
    "            check(args)\n",
    "\n",
    "    def parse(self):\n",
    "        args = self.parser.parse_args()\n",
    "        self.check_arguments(args)\n",
    "\n",
    "        args.input_arguments = copy.deepcopy(args)\n",
    "\n",
    "        args.nranks, args.distributed = distributed_init(args.rank)\n",
    "\n",
    "        args.nthreads = int(max(os.cpu_count(), faiss.omp_get_max_threads()) * 0.8)\n",
    "        args.nthreads = max(1, args.nthreads // args.nranks)\n",
    "\n",
    "        if args.nranks > 1:\n",
    "            print_message(f\"#> Restricting number of threads for FAISS to {args.nthreads} per process\",\n",
    "                          condition=(args.rank == 0))\n",
    "            faiss.omp_set_num_threads(args.nthreads)\n",
    "\n",
    "        Run.init(args.rank, args.root, args.experiment, args.run)\n",
    "        Run._log_args(args)\n",
    "        Run.info(args.input_arguments.__dict__, '\\n')\n",
    "\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## index_falss.py\n",
    "'''\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.utils.parser import Arguments\n",
    "from colbert.indexing.faiss import index_faiss\n",
    "from colbert.indexing.loaders import load_doclens\n",
    "'''\n",
    "def main():\n",
    "    random.seed(12345)\n",
    "\n",
    "    parser = Arguments(description='Faiss indexing for end-to-end retrieval with ColBERT.')\n",
    "    parser.add_index_use_input()\n",
    "\n",
    "    parser.add_argument('--sample', dest='sample', default=0.3, type=float)\n",
    "    parser.add_argument('--slices', dest='slices', default=1, type=int)\n",
    "    parser.add_argument('-f')\n",
    "\n",
    "    args = parser.parse()\n",
    "    assert args.slices >= 1\n",
    "    assert args.sample is None or (0.0 < args.sample < 1.0), args.sample\n",
    "\n",
    "    with Run.context():\n",
    "        args.index_path = os.path.join(args.index_root, args.index_name)\n",
    "        assert os.path.exists(args.index_path), args.index_path\n",
    "\n",
    "        num_embeddings = sum(load_doclens(args.index_path))\n",
    "        print(\"#> num_embeddings =\", num_embeddings)\n",
    "\n",
    "        if args.partitions is None:\n",
    "            args.partitions = 1 << math.ceil(math.log2(8 * math.sqrt(num_embeddings)))\n",
    "            print('\\n\\n')\n",
    "            Run.warn(\"You did not specify --partitions!\")\n",
    "            Run.warn(\"Default computation chooses\", args.partitions,\n",
    "                     \"partitions (for {} embeddings)\".format(num_embeddings))\n",
    "            print('\\n\\n')\n",
    "\n",
    "        index_faiss(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
