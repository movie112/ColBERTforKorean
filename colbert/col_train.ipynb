{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4519,
     "status": "ok",
     "timestamp": 1655573652532,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "28bab90a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# scala에서 json 파싱\n",
    "import ujson\n",
    "import copy\n",
    "import faiss\n",
    "import mlflow\n",
    "from functools import partial\n",
    "from packaging import version\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import sys\n",
    "import mlflow\n",
    "import traceback\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import  BertModel, BertConfig, AutoConfig, AutoTokenizer,BertPreTrainedModel, BertTokenizerFast, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ae13044"
   },
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2c3e79f"
   },
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1655573667210,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "90a99b24"
   },
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/utils.py\n",
    "\n",
    "def tensorize_triples(query_tokenizer, doc_tokenizer, queries, positives, negatives, bsize):\n",
    "    assert len(queries) == len(positives) == len(negatives)\n",
    "    assert bsize is None or len(queries) % bsize == 0\n",
    "\n",
    "    N = len(queries)\n",
    "    Q_ids, Q_mask = query_tokenizer.tensorize(queries)\n",
    "    D_ids, D_mask = doc_tokenizer.tensorize(positives + negatives)\n",
    "    D_ids, D_mask = D_ids.view(2, N, -1), D_mask.view(2, N, -1)\n",
    "\n",
    "    # Compute max among {length of i^th positive, length of i^th negative} for i \\in N\n",
    "    maxlens = D_mask.sum(-1).max(0).values\n",
    "\n",
    "    # Sort by maxlens\n",
    "    indices = maxlens.sort().indices\n",
    "    Q_ids, Q_mask = Q_ids[indices], Q_mask[indices]\n",
    "    D_ids, D_mask = D_ids[:, indices], D_mask[:, indices]\n",
    "\n",
    "    (positive_ids, negative_ids), (positive_mask, negative_mask) = D_ids, D_mask\n",
    "\n",
    "    query_batches = _split_into_batches(Q_ids, Q_mask, bsize)\n",
    "    positive_batches = _split_into_batches(positive_ids, positive_mask, bsize)\n",
    "    negative_batches = _split_into_batches(negative_ids, negative_mask, bsize)\n",
    "\n",
    "    batches = []\n",
    "    for (q_ids, q_mask), (p_ids, p_mask), (n_ids, n_mask) in zip(query_batches, positive_batches, negative_batches):\n",
    "        Q = (torch.cat((q_ids, q_ids)), torch.cat((q_mask, q_mask)))\n",
    "        D = (torch.cat((p_ids, n_ids)), torch.cat((p_mask, n_mask)))\n",
    "        batches.append((Q, D))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def _sort_by_length(ids, mask, bsize):\n",
    "    if ids.size(0) <= bsize:\n",
    "        return ids, mask, torch.arange(ids.size(0))\n",
    "\n",
    "    indices = mask.sum(-1).sort().indices\n",
    "    reverse_indices = indices.sort().indices\n",
    "\n",
    "    return ids[indices], mask[indices], reverse_indices\n",
    "\n",
    "\n",
    "def _split_into_batches(ids, mask, bsize):\n",
    "    batches = []\n",
    "    for offset in range(0, ids.size(0), bsize):\n",
    "        batches.append((ids[offset:offset+bsize], mask[offset:offset+bsize]))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1655573667210,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "d69d5140"
   },
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/query_tokenization.py\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "from colbert.modeling.tokenization.utils import _split_into_batches\n",
    "'''\n",
    "\n",
    "class QueryTokenizer():\n",
    "    def __init__(self, query_maxlen):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.query_maxlen = query_maxlen\n",
    "\n",
    "        self.Q_marker_token, self.Q_marker_token_id = '[Q]', self.tok.convert_tokens_to_ids('[unused0]')\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id\n",
    "\n",
    "        assert self.Q_marker_token_id == 1 and self.mask_token_id == 103\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst)+3)) for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
    "        print(ids)\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst)+3)) for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        # add placehold for the [Q] marker\n",
    "        batch_text = ['. ' + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(batch_text, padding='max_length', truncation=True,\n",
    "                       return_tensors='pt', max_length=self.query_maxlen)\n",
    "\n",
    "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "\n",
    "        # postprocess for the [Q] marker and the [MASK] augmentation\n",
    "        ids[:, 1] = self.Q_marker_token_id\n",
    "        ids[ids == 0] = self.mask_token_id\n",
    "\n",
    "        if bsize:\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches\n",
    "\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1655573668350,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "51c8eafd"
   },
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/doc_tokenization.py \n",
    "\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "from colbert.modeling.tokenization.utils import _split_into_batches, _sort_by_length\n",
    "'''\n",
    "\n",
    "class DocTokenizer():\n",
    "    def __init__(self, doc_maxlen):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "\n",
    "        self.D_marker_token, self.D_marker_token_id = '[D]', self.tok.convert_tokens_to_ids('[unused1]')\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "\n",
    "        assert self.D_marker_token_id == 2\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        # add placehold for the [D] marker\n",
    "        batch_text = ['. ' + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(batch_text, padding='longest', truncation='longest_first',\n",
    "                       return_tensors='pt', max_length=self.doc_maxlen)\n",
    "\n",
    "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "\n",
    "        # postprocess for the [D] marker\n",
    "        ids[:, 1] = self.D_marker_token_id\n",
    "\n",
    "        if bsize:\n",
    "            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches, reverse_indices\n",
    "\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85f07341"
   },
   "source": [
    "### ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1655576566825,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "7e54b693"
   },
   "outputs": [],
   "source": [
    "# colbert/modeling/colbert.py\n",
    "'''\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast\n",
    "from colbert.parameters import DEVICE\n",
    "'''\n",
    "class ColBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, query_maxlen, doc_maxlen, mask_punctuation, dim=128, similarity_metric='cosine'):\n",
    "\n",
    "        super(ColBERT, self).__init__(config)\n",
    "\n",
    "        self.query_maxlen = query_maxlen\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.dim = dim\n",
    "\n",
    "        self.mask_punctuation = mask_punctuation\n",
    "        self.skiplist = {}\n",
    "\n",
    "        if self.mask_punctuation:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "            self.skiplist = {w: True\n",
    "                             for symbol in string.punctuation\n",
    "                             for w in [symbol, self.tokenizer.encode(symbol, add_special_tokens=False)[0]]}\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.linear = nn.Linear(config.hidden_size, dim, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, Q, D):\n",
    "        return self.score(self.query(*Q), self.doc(*D))\n",
    "\n",
    "    def query(self, input_ids, attention_mask):\n",
    "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "        Q = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        Q = self.linear(Q)\n",
    "\n",
    "        return torch.nn.functional.normalize(Q, p=2, dim=2)\n",
    "\n",
    "    def doc(self, input_ids, attention_mask, keep_dims=True):\n",
    "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "        D = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        D = self.linear(D)\n",
    "\n",
    "        mask = torch.tensor(self.mask(input_ids), device=DEVICE).unsqueeze(2).float()\n",
    "        D = D * mask\n",
    "\n",
    "        D = torch.nn.functional.normalize(D, p=2, dim=2)\n",
    "\n",
    "        if not keep_dims:\n",
    "            D, mask = D.cpu().to(dtype=torch.float16), mask.cpu().bool().squeeze(-1)\n",
    "            D = [d[mask[idx]] for idx, d in enumerate(D)]\n",
    "\n",
    "        return D\n",
    "\n",
    "    def score(self, Q, D):\n",
    "        if self.similarity_metric == 'cosine':\n",
    "            return (Q @ D.permute(0, 2, 1)).max(2).values.sum(1)\n",
    "\n",
    "        assert self.similarity_metric == 'l2'\n",
    "        return (-1.0 * ((Q.unsqueeze(2) - D.unsqueeze(1))**2).sum(-1)).max(-1).values.sum(-1)\n",
    "\n",
    "    def mask(self, input_ids):\n",
    "        mask = [[(x not in self.skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d398c359"
   },
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1655576567425,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "429fe11e"
   },
   "outputs": [],
   "source": [
    "# colbert/parameters.py \n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "\n",
    "SAVED_CHECKPOINTS = [32*1000, 100*1000, 150*1000, 200*1000, 300*1000, 400*1000]\n",
    "SAVED_CHECKPOINTS += [10*1000, 20*1000, 30*1000, 40*1000, 50*1000, 60*1000, 70*1000, 80*1000, 90*1000]\n",
    "SAVED_CHECKPOINTS += [25*1000, 50*1000, 75*1000]\n",
    "\n",
    "SAVED_CHECKPOINTS = set(SAVED_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82e59c16"
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34b2668d"
   },
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1655576567425,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "32f149c1"
   },
   "outputs": [],
   "source": [
    "# colbert/utils/utils.py\n",
    "\n",
    "def print_message(*s, condition=True):\n",
    "    s = ' '.join([str(x) for x in s])\n",
    "    msg = \"[{}] {}\".format(datetime.datetime.now().strftime(\"%b %d, %H:%M:%S\"), s)\n",
    "\n",
    "    if condition:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "    return msg\n",
    "def timestamp():\n",
    "    format_str = \"%Y-%m-%d_%H.%M.%S\"\n",
    "    result = datetime.datetime.now().strftime(format_str)\n",
    "    return result\n",
    "\n",
    "def save_checkpoint(path, epoch_idx, mb_idx, model, optimizer, arguments=None):\n",
    "    print(f\"#> Saving a checkpoint to {path} ..\")\n",
    "\n",
    "    if hasattr(model, 'module'):\n",
    "        model = model.module  # extract model from a distributed/data-parallel wrapper\n",
    "\n",
    "    checkpoint = {}\n",
    "    checkpoint['epoch'] = epoch_idx\n",
    "    checkpoint['batch'] = mb_idx\n",
    "    checkpoint['model_state_dict'] = model.state_dict()\n",
    "    checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
    "    checkpoint['arguments'] = arguments\n",
    "\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "def create_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        print('\\n')\n",
    "        print_message(\"#> Note: Output directory\", path, 'already exists\\n\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "        print_message(\"#> Creating directory\", path, '\\n\\n')\n",
    "        os.makedirs(path)\n",
    "\n",
    "def distributed_init(rank):\n",
    "    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])\n",
    "    nranks = max(1, nranks)\n",
    "    is_distributed = nranks > 1\n",
    "\n",
    "    if rank == 0:\n",
    "        print('nranks =', nranks, '\\t num_gpus =', torch.cuda.device_count())\n",
    "\n",
    "    if is_distributed:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        torch.cuda.set_device(rank % num_gpus)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    return nranks, is_distributed\n",
    "def distributed_barrier(rank):\n",
    "    if rank >= 0:\n",
    "        torch.distributed_barrier()\n",
    "# see https://stackoverflow.com/a/45187287\n",
    "class NullContextManager(object):\n",
    "    def __init__(self, dummy_resource=None):\n",
    "        self.dummy_resource = dummy_resource\n",
    "    def __enter__(self):\n",
    "        return self.dummy_resource\n",
    "    def __exit__(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 642,
     "status": "ok",
     "timestamp": 1655576569161,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "96432caa"
   },
   "outputs": [],
   "source": [
    "# colbert/utils/runs.py\n",
    "'''\n",
    "import colbert.utils.distributed as distributed\n",
    "'''\n",
    "\n",
    "'''\n",
    "from colbert.utils.logging import Logger\n",
    "from colbert.utils.utils import timestamp, create_directory, print_message\n",
    "'''\n",
    "\n",
    "class _RunManager():\n",
    "    def __init__(self):\n",
    "        self.experiments_root = None\n",
    "        self.experiment = None\n",
    "        self.path = None\n",
    "        self.script = self._get_script_name()\n",
    "        self.name = self._generate_default_run_name()\n",
    "        self.original_name = self.name\n",
    "        self.exit_status = 'FINISHED'\n",
    "\n",
    "        self._logger = None\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def init(self, rank, root, experiment, name):\n",
    "        assert '/' not in experiment, experiment\n",
    "        assert '/' not in name, name\n",
    "\n",
    "        self.experiments_root = os.path.abspath(root)\n",
    "        self.experiment = experiment\n",
    "        self.name = name\n",
    "        self.path = os.path.join(self.experiments_root, self.experiment, self.script, self.name)\n",
    "\n",
    "        if rank < 1:\n",
    "            if os.path.exists(self.path):\n",
    "                print('\\n\\n')\n",
    "                print_message(\"It seems that \", self.path, \" already exists.\")\n",
    "                print_message(\"Do you want to overwrite it? \\t yes/no \\n\")\n",
    "\n",
    "                # TODO: This should timeout and exit (i.e., fail) given no response for 60 seconds.\n",
    "\n",
    "                response = input()\n",
    "                if response.strip() != 'yes':\n",
    "                    assert not os.path.exists(self.path), self.path\n",
    "            else:\n",
    "                create_directory(self.path)\n",
    "\n",
    "\n",
    "        if rank >= 0:\n",
    "          torch.distributed.barrier()\n",
    "\n",
    "        self._logger = Logger(rank, self)\n",
    "        self._log_args = self._logger._log_args\n",
    "        self.warn = self._logger.warn\n",
    "        self.info = self._logger.info\n",
    "        self.info_all = self._logger.info_all\n",
    "        self.log_metric = self._logger.log_metric\n",
    "        self.log_new_artifact = self._logger.log_new_artifact\n",
    "\n",
    "    def _generate_default_run_name(self):\n",
    "        return timestamp()\n",
    "\n",
    "    def _get_script_name(self):\n",
    "        #return os.path.basename(__main__.__file__) if '__file__' in dir(__main__) else 'none'\n",
    "        return os.path.basename('main'.__file__) if '__file__' in dir('main') else 'none'\n",
    "\n",
    "    @contextmanager\n",
    "    def context(self, consider_failed_if_interrupted=True):\n",
    "        try:\n",
    "            yield\n",
    "\n",
    "        except KeyboardInterrupt as ex:\n",
    "            print('\\n\\nInterrupted\\n\\n')\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            if consider_failed_if_interrupted:\n",
    "                self.exit_status = 'KILLED'  # mlflow.entities.RunStatus.KILLED\n",
    "\n",
    "            sys.exit(128 + 2)\n",
    "\n",
    "        except Exception as ex:\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            self.exit_status = 'FAILED'  # mlflow.entities.RunStatus.FAILED\n",
    "\n",
    "            raise ex\n",
    "\n",
    "        finally:\n",
    "            total_seconds = str(time.time() - self.start_time) + '\\n'\n",
    "            original_name = str(self.original_name)\n",
    "            name = str(self.name)\n",
    "\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'elapsed.txt'), total_seconds)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.original.txt'), original_name)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.txt'), name)\n",
    "\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            mlflow.end_run(status=self.exit_status)\n",
    "\n",
    "\n",
    "Run = _RunManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1655576569161,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "5b0ce443"
   },
   "outputs": [],
   "source": [
    "# colbert/utils/logging.py\n",
    "\n",
    "'''\n",
    "from colbert.utils.utils import print_message, create_directory\n",
    "'''\n",
    "\n",
    "class Logger():\n",
    "    def __init__(self, rank, run):\n",
    "        self.rank = rank\n",
    "        self.is_main = self.rank in [-1, 0]\n",
    "        self.run = run\n",
    "        self.logs_path = os.path.join(self.run.path, \"logs/\")\n",
    "\n",
    "        if self.is_main:\n",
    "            self._init_mlflow()\n",
    "            self.initialized_tensorboard = False\n",
    "            create_directory(self.logs_path)\n",
    "\n",
    "    def _init_mlflow(self):\n",
    "        mlflow.set_tracking_uri('file://' + os.path.join(self.run.experiments_root, \"logs/mlruns/\"))\n",
    "        mlflow.set_experiment('/'.join([self.run.experiment, self.run.script]))\n",
    "        \n",
    "        mlflow.set_tag('experiment', self.run.experiment)\n",
    "        mlflow.set_tag('name', self.run.name)\n",
    "        mlflow.set_tag('path', self.run.path)\n",
    "\n",
    "    def _init_tensorboard(self):\n",
    "        root = os.path.join(self.run.experiments_root, \"logs/tensorboard/\")\n",
    "        logdir = '__'.join([self.run.experiment, self.run.script, self.run.name])\n",
    "        logdir = os.path.join(root, logdir)\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=logdir)\n",
    "        self.initialized_tensorboard = True\n",
    "\n",
    "    def _log_exception(self, etype, value, tb):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        output_path = os.path.join(self.logs_path, 'exception.txt')\n",
    "        trace = ''.join(traceback.format_exception(etype, value, tb)) + '\\n'\n",
    "        print_message(trace, '\\n\\n')\n",
    "\n",
    "        self.log_new_artifact(output_path, trace)\n",
    "\n",
    "    def _log_all_artifacts(self):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        mlflow.log_artifacts(self.logs_path)\n",
    "\n",
    "    def _log_args(self, args):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        for key in vars(args):\n",
    "            value = getattr(args, key)\n",
    "            if type(value) in [int, float, str, bool]:\n",
    "                mlflow.log_param(key, value)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.json'), 'w') as output_metadata:\n",
    "            ujson.dump(args.input_arguments.__dict__, output_metadata, indent=4)\n",
    "            output_metadata.write('\\n')\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.txt'), 'w') as output_metadata:\n",
    "            output_metadata.write(' '.join(sys.argv) + '\\n')\n",
    "\n",
    "    def log_metric(self, name, value, step, log_to_mlflow=True):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        if not self.initialized_tensorboard:\n",
    "            self._init_tensorboard()\n",
    "\n",
    "        if log_to_mlflow:\n",
    "            mlflow.log_metric(name, value, step=step)\n",
    "        self.writer.add_scalar(name, value, step)\n",
    "\n",
    "    def log_new_artifact(self, path, content):\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        mlflow.log_artifact(path)\n",
    "\n",
    "    def warn(self, *args):\n",
    "        msg = print_message('[WARNING]', '\\t', *args)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'warnings.txt'), 'a') as output_metadata:\n",
    "            output_metadata.write(msg + '\\n\\n\\n')\n",
    "\n",
    "    def info_all(self, *args):\n",
    "        print_message('[' + str(self.rank) + ']', '\\t', *args)\n",
    "\n",
    "    def info(self, *args):\n",
    "        if self.is_main:\n",
    "            print_message(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1655576569162,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "c188d1a3"
   },
   "outputs": [],
   "source": [
    "# colbert/utils/amp.py\n",
    "'''\n",
    "from contextlib import contextmanager\n",
    "from colbert.utils.utils import NullContextManager\n",
    "from packaging import version\n",
    "'''\n",
    "v = version.parse\n",
    "PyTorch_over_1_6  = v(torch.__version__) >= v('1.6')\n",
    "\n",
    "class MixedPrecisionManager():\n",
    "    def __init__(self, activated):\n",
    "        assert (not activated) or PyTorch_over_1_6, \"Cannot use AMP for PyTorch version < 1.6\"\n",
    "\n",
    "        self.activated = activated\n",
    "\n",
    "        if self.activated:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def context(self):\n",
    "        return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
    "\n",
    "    def backward(self, loss):\n",
    "        if self.activated:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "    def step(self, colbert, optimizer):\n",
    "        if self.activated:\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0)\n",
    "\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1655576569162,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "16ef834f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c44e5b0e"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1655576569163,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "2aaebcd5"
   },
   "outputs": [],
   "source": [
    "# colbert/training/utils.py \n",
    "'''\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.utils.utils import print_message, save_checkpoint\n",
    "from colbert.parameters import SAVED_CHECKPOINTS\n",
    "'''\n",
    "\n",
    "def print_progress(scores):\n",
    "    positive_avg, negative_avg = round(scores[:, 0].mean().item(), 2), round(scores[:, 1].mean().item(), 2)\n",
    "    print(\"#>>>   \", positive_avg, negative_avg, '\\t\\t|\\t\\t', positive_avg - negative_avg)\n",
    "\n",
    "\n",
    "def manage_checkpoints(args, colbert, optimizer, batch_idx):\n",
    "    arguments = args.input_arguments.__dict__\n",
    "\n",
    "    path = os.path.join(Run.path, 'checkpoints')\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    if batch_idx % 2000 == 0:\n",
    "        name = os.path.join(path, \"colbert.dnn\")\n",
    "        save_checkpoint(name, 0, batch_idx, colbert, optimizer, arguments)\n",
    "\n",
    "    if batch_idx in SAVED_CHECKPOINTS:\n",
    "        name = os.path.join(path, \"colbert-{}.dnn\".format(batch_idx))\n",
    "        save_checkpoint(name, 0, batch_idx, colbert, optimizer, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1655576569163,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "d0d865e1"
   },
   "outputs": [],
   "source": [
    "# /colbert/training/eager_batcher.py /\n",
    "'''\n",
    "from functools import partial\n",
    "from colbert.utils.utils import print_message\n",
    "from colbert.modeling.tokenization import QueryTokenizer, DocTokenizer, tensorize_triples\n",
    "\n",
    "from colbert.utils.runs import Run\n",
    "'''\n",
    "\n",
    "class EagerBatcher():\n",
    "    def __init__(self, args, rank=0, nranks=1):\n",
    "        self.rank, self.nranks = rank, nranks\n",
    "        self.bsize, self.accumsteps = args.bsize, args.accumsteps\n",
    "\n",
    "        self.query_tokenizer = QueryTokenizer(args.query_maxlen)\n",
    "        self.doc_tokenizer = DocTokenizer(args.doc_maxlen)\n",
    "        self.tensorize_triples = partial(tensorize_triples, self.query_tokenizer, self.doc_tokenizer)\n",
    "\n",
    "        self.triples_path = args.triples\n",
    "        self._reset_triples()\n",
    "\n",
    "    def _reset_triples(self):\n",
    "        self.reader = open(self.triples_path, mode='r', encoding=\"utf-8\")\n",
    "        self.position = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        queries, positives, negatives = [], [], []\n",
    "\n",
    "        for line_idx, line in zip(range(self.bsize * self.nranks), self.reader):\n",
    "            if (self.position + line_idx) % self.nranks != self.rank:\n",
    "                continue\n",
    "\n",
    "            query, pos, neg = line.strip().split('\\t')\n",
    "\n",
    "            queries.append(query)\n",
    "            positives.append(pos)\n",
    "            negatives.append(neg)\n",
    "\n",
    "        self.position += line_idx + 1\n",
    "\n",
    "        if len(queries) < self.bsize:\n",
    "            raise StopIteration\n",
    "\n",
    "        return self.collate(queries, positives, negatives)\n",
    "\n",
    "    def collate(self, queries, positives, negatives):\n",
    "        assert len(queries) == len(positives) == len(negatives) == self.bsize\n",
    "\n",
    "        return self.tensorize_triples(queries, positives, negatives, self.bsize // self.accumsteps)\n",
    "\n",
    "    def skip_to_batch(self, batch_idx, intended_batch_size):\n",
    "        self._reset_triples()\n",
    "\n",
    "        Run.warn(f'Skipping to batch #{batch_idx} (with intended_batch_size = {intended_batch_size}) for training.')\n",
    "\n",
    "        _ = [self.reader.readline() for _ in range(batch_idx * intended_batch_size)]\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1655576569163,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "b8bcb346"
   },
   "outputs": [],
   "source": [
    "# colbert/training/training.py\n",
    "\n",
    "'''\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.utils.amp import MixedPrecisionManager\n",
    "\n",
    "from colbert.training.lazy_batcher import LazyBatcher\n",
    "from colbert.training.eager_batcher import EagerBatcher\n",
    "from colbert.parameters import DEVICE\n",
    "\n",
    "from colbert.modeling.colbert import ColBERT\n",
    "from colbert.utils.utils import print_message\n",
    "from colbert.training.utils import print_progress, manage_checkpoints\n",
    "'''\n",
    "\n",
    "def train(args):\n",
    "    random.seed(12345)\n",
    "    np.random.seed(12345)\n",
    "    torch.manual_seed(12345)\n",
    "    if args.distributed:\n",
    "        torch.cuda.manual_seed_all(12345)\n",
    "\n",
    "    if args.distributed:\n",
    "        assert args.bsize % args.nranks == 0, (args.bsize, args.nranks)\n",
    "        assert args.accumsteps == 1\n",
    "        args.bsize = args.bsize // args.nranks\n",
    "\n",
    "        print(\"Using args.bsize =\", args.bsize, \"(per process) and args.accumsteps =\", args.accumsteps)\n",
    "\n",
    "    if args.lazy:\n",
    "        reader = LazyBatcher(args, (0 if args.rank == -1 else args.rank), args.nranks)\n",
    "    else:\n",
    "        reader = EagerBatcher(args, (0 if args.rank == -1 else args.rank), args.nranks)\n",
    "\n",
    "    if args.rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    colbert = ColBERT.from_pretrained('bert-base-uncased',\n",
    "                                      query_maxlen=args.query_maxlen,\n",
    "                                      doc_maxlen=args.doc_maxlen,\n",
    "                                      dim=args.dim,\n",
    "                                      similarity_metric=args.similarity,\n",
    "                                      mask_punctuation=args.mask_punctuation)\n",
    "\n",
    "    if args.checkpoint is not None:\n",
    "        assert args.resume_optimizer is False, \"TODO: This would mean reload optimizer too.\"\n",
    "        print_message(f\"#> Starting from checkpoint {args.checkpoint} -- but NOT the optimizer!\")\n",
    "\n",
    "        checkpoint = torch.load(args.checkpoint, map_location='cpu')\n",
    "\n",
    "        try:\n",
    "            colbert.load_state_dict(checkpoint['model_state_dict'])\n",
    "        except:\n",
    "            print_message(\"[WARNING] Loading checkpoint with strict=False\")\n",
    "            colbert.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "    if args.rank == 0:\n",
    "        torch.distributed.barrier()\n",
    "\n",
    "    colbert = colbert.to(DEVICE)\n",
    "    colbert.train()\n",
    "\n",
    "    if args.distributed:\n",
    "        colbert = torch.nn.parallel.DistributedDataParallel(colbert, device_ids=[args.rank],\n",
    "                                                            output_device=args.rank,\n",
    "                                                            find_unused_parameters=True)\n",
    "\n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad, colbert.parameters()), lr=args.lr, eps=1e-8)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    amp = MixedPrecisionManager(args.amp)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    labels = torch.zeros(args.bsize, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    start_batch_idx = 0\n",
    "\n",
    "    if args.resume:\n",
    "        assert args.checkpoint is not None\n",
    "        start_batch_idx = checkpoint['batch']\n",
    "\n",
    "        reader.skip_to_batch(start_batch_idx, checkpoint['arguments']['bsize'])\n",
    "    pos_lst = []\n",
    "    neg_lst = []\n",
    "    for batch_idx, BatchSteps in zip(range(start_batch_idx, args.maxsteps), reader):\n",
    "        this_batch_loss = 0.0\n",
    "\n",
    "        for queries, passages in BatchSteps:\n",
    "            with amp.context():\n",
    "                scores = colbert(queries, passages).view(2, -1).permute(1, 0)\n",
    "                loss = criterion(scores, labels[:scores.size(0)])\n",
    "                loss = loss / args.accumsteps\n",
    "\n",
    "            if args.rank < 1:\n",
    "                # round(scores[:, 0].mean().item(), 2), round(scores[:, 1].mean().item(), 2)\n",
    "                if batch_idx % 50 == 0:\n",
    "                    print_progress(scores)\n",
    "                    positive_avg, negative_avg = round(scores[:, 0].mean().item(), 2), round(scores[:, 1].mean().item(), 2)##\n",
    "                    pos_lst.append(positive_avg)\n",
    "                    neg_lst.append(negative_avg)\n",
    "\n",
    "\n",
    "            amp.backward(loss)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            this_batch_loss += loss.item()\n",
    "\n",
    "        amp.step(colbert, optimizer)\n",
    "\n",
    "        if args.rank < 1:\n",
    "            avg_loss = train_loss / (batch_idx+1)\n",
    "\n",
    "            num_examples_seen = (batch_idx - start_batch_idx) * args.bsize * args.nranks\n",
    "            elapsed = float(time.time() - start_time)\n",
    "\n",
    "            log_to_mlflow = (batch_idx % 20 == 0)\n",
    "            Run.log_metric('train/avg_loss', avg_loss, step=batch_idx, log_to_mlflow=log_to_mlflow)\n",
    "            Run.log_metric('train/batch_loss', this_batch_loss, step=batch_idx, log_to_mlflow=log_to_mlflow)\n",
    "            Run.log_metric('train/examples', num_examples_seen, step=batch_idx, log_to_mlflow=log_to_mlflow)\n",
    "            Run.log_metric('train/throughput', num_examples_seen / elapsed, step=batch_idx, log_to_mlflow=log_to_mlflow)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print_message(batch_idx, avg_loss)\n",
    "            manage_checkpoints(args, colbert, optimizer, batch_idx+1)\n",
    "    return pos_lst, neg_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1655576569163,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "895e4fa0"
   },
   "outputs": [],
   "source": [
    "# auguments\n",
    "'''\n",
    "import colbert.utils.distributed as distributed\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.utils.utils import print_message, timestamp, create_directory\n",
    "'''\n",
    "\n",
    "class Arguments():\n",
    "    def __init__(self, description):\n",
    "        self.parser = ArgumentParser(description=description)\n",
    "        self.checks = []\n",
    "\n",
    "        self.add_argument('--root', dest='root', default='/experiments')\n",
    "        self.add_argument('--experiment', dest='experiment', default='MSMARCO-psg')\n",
    "        self.add_argument('--run', dest='run', default=Run.name)\n",
    "                        \n",
    "\n",
    "        self.add_argument('--local_rank', dest='rank', default=-1, type=int)\n",
    "\n",
    "    def add_model_parameters(self):\n",
    "        # Core Arguments\n",
    "        self.add_argument('--similarity', dest='similarity', default='cosine', choices=['cosine', 'l2'])\n",
    "        self.add_argument('--dim', dest='dim', default=128, type=int)\n",
    "        self.add_argument('--query_maxlen', dest='query_maxlen', default=32, type=int)\n",
    "        self.add_argument('--doc_maxlen', dest='doc_maxlen', default=180, type=int)\n",
    "\n",
    "        # Filtering-related Arguments\n",
    "        self.add_argument('--mask-punctuation', dest='mask_punctuation', default=True, action='store_true')\n",
    "\n",
    "    def add_model_training_parameters(self):\n",
    "        # NOTE: Providing a checkpoint is one thing, --resume is another, --resume_optimizer is yet another.\n",
    "        self.add_argument('--resume', dest='resume', default=False, action='store_true')\n",
    "        self.add_argument('--resume_optimizer', dest='resume_optimizer', default=False, action='store_true')\n",
    "        self.add_argument('--checkpoint', dest='checkpoint', default=None, required=False)\n",
    "\n",
    "        self.add_argument('--lr', dest='lr', default=3e-06, type=float)\n",
    "        self.add_argument('--maxsteps', dest='maxsteps', default=200000, type=int)\n",
    "        self.add_argument('--bsize', dest='bsize', default=32, type=int)\n",
    "        self.add_argument('--accum', dest='accumsteps', default=1, type=int)\n",
    "        self.add_argument('--amp', dest='amp', default=True, action='store_true')\n",
    "\n",
    "    def add_training_input(self):\n",
    "        self.add_argument('--triples', dest='triples', default='triples.train.small.tsv', type=str)\n",
    "        self.add_argument('--queries', dest='queries', default=None)\n",
    "        self.add_argument('--collection', dest='collection', default=None)\n",
    "\n",
    "        def check_training_input(args):\n",
    "            assert (args.collection is None) == (args.queries is None), \\\n",
    "                \"For training, both (or neither) --collection and --queries must be supplied.\" \\\n",
    "                \"If neither is supplied, the --triples file must contain texts (not PIDs).\"\n",
    "\n",
    "        self.checks.append(check_training_input)\n",
    "\n",
    "    def add_argument(self, *args, **kw_args):\n",
    "        return self.parser.add_argument(*args, **kw_args)\n",
    "\n",
    "    def check_arguments(self, args):\n",
    "        for check in self.checks:\n",
    "            check(args)\n",
    "\n",
    "    def parse(self):\n",
    "        args = self.parser.parse_args()\n",
    "        self.check_arguments(args)\n",
    "\n",
    "        args.input_arguments = copy.deepcopy(args)\n",
    "\n",
    "        args.nranks, args.distributed = distributed_init(args.rank)\n",
    "\n",
    "        args.nthreads = int(max(os.cpu_count(), faiss.omp_get_max_threads()) * 0.8)\n",
    "        args.nthreads = max(1, args.nthreads // args.nranks)\n",
    "\n",
    "        if args.nranks > 1:\n",
    "            print_message(f\"#> Restricting number of threads for FAISS to {args.nthreads} per process\",\n",
    "                          condition=(args.rank == 0))\n",
    "            faiss.omp_set_num_threads(args.nthreads)\n",
    "\n",
    "        Run.init(args.rank, args.root, args.experiment, args.run)\n",
    "        Run._log_args(args)\n",
    "        Run.info(args.input_arguments.__dict__, '\\n')\n",
    "\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1655576569164,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "51505531",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# colbert/train.py \n",
    "'''\n",
    "import colbert.utils.distributed as distributed\n",
    "\n",
    "from colbert.utils.parser import Arguments\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.training.training import train\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    parser = Arguments(description='Training ColBERT with <query, positive passage, negative passage> triples.')\n",
    "\n",
    "    parser.add_model_parameters()\n",
    "    parser.add_model_training_parameters()\n",
    "    parser.add_training_input()\n",
    "    parser.add_argument('-f')\n",
    "    \n",
    "    args = parser.parse()\n",
    "    \n",
    "    labels = torch.zeros(args.bsize, dtype=torch.long, device=DEVICE) #32, torch.int64\n",
    "\n",
    "    assert args.bsize % args.accumsteps == 0, ((args.bsize, args.accumsteps),\n",
    "                                               \"The batch size must be divisible by the number of gradient accumulation steps.\")\n",
    "    assert args.query_maxlen <= 512\n",
    "    assert args.doc_maxlen <= 512\n",
    "    \n",
    "    args.lazy = args.collection is not None\n",
    "\n",
    "    with Run.context(consider_failed_if_interrupted=False):\n",
    "        return train(args)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/train.py \n",
    "'''\n",
    "import colbert.utils.distributed as distributed\n",
    "\n",
    "from colbert.utils.parser import Arguments\n",
    "from colbert.utils.runs import Run\n",
    "from colbert.training.training import train\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    parser = Arguments(description='Training ColBERT with <query, positive passage, negative passage> triples.')\n",
    "\n",
    "    parser.add_model_parameters()\n",
    "    parser.add_model_training_parameters()\n",
    "    parser.add_training_input()\n",
    "    parser.add_argument('-f')\n",
    "    \n",
    "    args = parser.parse()\n",
    "    \n",
    "    labels = torch.zeros(args.bsize, dtype=torch.long, device=DEVICE) #32, torch.int64\n",
    "\n",
    "    assert args.bsize % args.accumsteps == 0, ((args.bsize, args.accumsteps),\n",
    "                                               \"The batch size must be divisible by the number of gradient accumulation steps.\")\n",
    "    assert args.query_maxlen <= 512\n",
    "    assert args.doc_maxlen <= 512\n",
    "    \n",
    "    args.lazy = args.collection is not None\n",
    "\n",
    "    with Run.context(consider_failed_if_interrupted=False):\n",
    "        return train(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pos_lst, neg_lst = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1655575578490,
     "user": {
      "displayName": "Yeong hwa",
      "userId": "14335833007320819050"
     },
     "user_tz": -540
    },
    "id": "1787df07",
    "outputId": "d4776709-062e-4f1a-e4bb-117173ec12d7"
   },
   "outputs": [],
   "source": [
    "print('positive_score_avg :', sum(pos_lst)/len(pos_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "157e4c37"
   },
   "outputs": [],
   "source": [
    "print('negative_score_avg :', sum(neg_lst)/len(neg_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83508574"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "437d6022"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "colbert_train_clear_clear.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
