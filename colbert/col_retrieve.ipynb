{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e581b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import ujson\n",
    "from itertools import accumulate\n",
    "from math import ceil\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "import copy\n",
    "import faiss\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from transformers import BertTokenizerFast, BertPreTrainedModel, BertModel, BertConfig, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c8b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "print(\"Torch version:{}\".format(torch.__version__))\n",
    "print(\"cuda version: {}\".format(torch.version.cuda))\n",
    "torch.cuda.is_available()\n",
    "\n",
    "SEED = 12345\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d0245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/colbert.py\n",
    "'''\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast\n",
    "from colbert.parameters import DEVICE\n",
    "'''\n",
    "class ColBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, query_maxlen, doc_maxlen, mask_punctuation, dim=128, similarity_metric='cosine'):\n",
    "\n",
    "        super(ColBERT, self).__init__(config)\n",
    "\n",
    "        self.query_maxlen = query_maxlen\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.dim = dim\n",
    "\n",
    "        self.mask_punctuation = mask_punctuation\n",
    "        self.skiplist = {}\n",
    "\n",
    "        if self.mask_punctuation:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "            self.skiplist = {w: True\n",
    "                             for symbol in string.punctuation\n",
    "                             for w in [symbol, self.tokenizer.encode(symbol, add_special_tokens=False)[0]]}\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.linear = nn.Linear(config.hidden_size, dim, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, Q, D):\n",
    "        return self.score(self.query(*Q), self.doc(*D))\n",
    "\n",
    "    def query(self, input_ids, attention_mask):\n",
    "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "        Q = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        Q = self.linear(Q)\n",
    "\n",
    "        return torch.nn.functional.normalize(Q, p=2, dim=2)\n",
    "\n",
    "    def doc(self, input_ids, attention_mask, keep_dims=True):\n",
    "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "        D = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        D = self.linear(D)\n",
    "\n",
    "        mask = torch.tensor(self.mask(input_ids), device=DEVICE).unsqueeze(2).float()\n",
    "        D = D * mask\n",
    "\n",
    "        D = torch.nn.functional.normalize(D, p=2, dim=2)\n",
    "\n",
    "        if not keep_dims:\n",
    "            D, mask = D.cpu().to(dtype=torch.float16), mask.cpu().bool().squeeze(-1)\n",
    "            D = [d[mask[idx]] for idx, d in enumerate(D)]\n",
    "\n",
    "        return D\n",
    "\n",
    "    def score(self, Q, D):\n",
    "        if self.similarity_metric == 'cosine':\n",
    "            return (Q @ D.permute(0, 2, 1)).max(2).values.sum(1)\n",
    "\n",
    "        assert self.similarity_metric == 'l2'\n",
    "        return (-1.0 * ((Q.unsqueeze(2) - D.unsqueeze(1))**2).sum(-1)).max(-1).values.sum(-1)\n",
    "\n",
    "    def mask(self, input_ids):\n",
    "        mask = [[(x not in self.skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a204a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/utils.py\n",
    "def _sort_by_length(ids, mask, bsize):\n",
    "    if ids.size(0) <= bsize:\n",
    "        return ids, mask, torch.arange(ids.size(0))\n",
    "\n",
    "    indices = mask.sum(-1).sort().indices\n",
    "    reverse_indices = indices.sort().indices\n",
    "\n",
    "    return ids[indices], mask[indices], reverse_indices\n",
    "\n",
    "\n",
    "def _split_into_batches(ids, mask, bsize):\n",
    "    batches = []\n",
    "    for offset in range(0, ids.size(0), bsize):\n",
    "        batches.append((ids[offset:offset+bsize], mask[offset:offset+bsize]))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30f8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/query_tokenization.py\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "from colbert.modeling.tokenization.utils import _split_into_batches\n",
    "'''\n",
    "\n",
    "class QueryTokenizer():\n",
    "    def __init__(self, query_maxlen):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.query_maxlen = query_maxlen\n",
    "\n",
    "        self.Q_marker_token, self.Q_marker_token_id = '[Q]', self.tok.convert_tokens_to_ids('[unused0]')\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id\n",
    "\n",
    "        assert self.Q_marker_token_id == 1 and self.mask_token_id == 103\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst)+3)) for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
    "        print(ids)\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst)+3)) for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        # add placehold for the [Q] marker\n",
    "        batch_text = ['. ' + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(batch_text, padding='max_length', truncation=True,\n",
    "                       return_tensors='pt', max_length=self.query_maxlen)\n",
    "\n",
    "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "\n",
    "        # postprocess for the [Q] marker and the [MASK] augmentation\n",
    "        ids[:, 1] = self.Q_marker_token_id\n",
    "        ids[ids == 0] = self.mask_token_id\n",
    "\n",
    "        if bsize:\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches\n",
    "\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a1c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/doc_tokenization.py \n",
    "\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "from colbert.modeling.tokenization.utils import _split_into_batches, _sort_by_length\n",
    "'''\n",
    "\n",
    "class DocTokenizer():\n",
    "    def __init__(self, doc_maxlen):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "\n",
    "        self.D_marker_token, self.D_marker_token_id = '[D]', self.tok.convert_tokens_to_ids('[unused1]')\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "\n",
    "        assert self.D_marker_token_id == 2\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        # add placehold for the [D] marker\n",
    "        batch_text = ['. ' + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(batch_text, padding='longest', truncation='longest_first',\n",
    "                       return_tensors='pt', max_length=self.doc_maxlen)\n",
    "\n",
    "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "\n",
    "        # postprocess for the [D] marker\n",
    "        ids[:, 1] = self.D_marker_token_id\n",
    "\n",
    "        if bsize:\n",
    "            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches, reverse_indices\n",
    "\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b25d1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/inference.py\n",
    "'''\n",
    "from colbert.modeling.colbert import ColBERT--\n",
    "from colbert.modeling.tokenization import QueryTokenizer, DocTokenizer--\n",
    "from colbert.utils.amp import MixedPrecisionManager--\n",
    "from colbert.parameters import DEVICE--\n",
    "'''\n",
    "class ModelInference():\n",
    "    def __init__(self, colbert: ColBERT, amp=False):\n",
    "        assert colbert.training is False\n",
    "\n",
    "        self.colbert = colbert\n",
    "        self.query_tokenizer = QueryTokenizer(colbert.query_maxlen)\n",
    "        self.doc_tokenizer = DocTokenizer(colbert.doc_maxlen)\n",
    "\n",
    "        self.amp_manager = MixedPrecisionManager(amp)\n",
    "\n",
    "    def query(self, *args, to_cpu=False, **kw_args):\n",
    "        with torch.no_grad():\n",
    "            with self.amp_manager.context():\n",
    "                Q = self.colbert.query(*args, **kw_args)\n",
    "                return Q.cpu() if to_cpu else Q\n",
    "\n",
    "    def doc(self, *args, to_cpu=False, **kw_args):\n",
    "        with torch.no_grad():\n",
    "            with self.amp_manager.context():\n",
    "                D = self.colbert.doc(*args, **kw_args)\n",
    "                return D.cpu() if to_cpu else D\n",
    "\n",
    "    def queryFromText(self, queries, bsize=None, to_cpu=False):\n",
    "        if bsize:\n",
    "            batches = self.query_tokenizer.tensorize(queries, bsize=bsize)\n",
    "            batches = [self.query(input_ids, attention_mask, to_cpu=to_cpu) for input_ids, attention_mask in batches]\n",
    "            return torch.cat(batches)\n",
    "\n",
    "        input_ids, attention_mask = self.query_tokenizer.tensorize(queries)\n",
    "        return self.query(input_ids, attention_mask)\n",
    "\n",
    "    def docFromText(self, docs, bsize=None, keep_dims=True, to_cpu=False):\n",
    "        if bsize:\n",
    "            batches, reverse_indices = self.doc_tokenizer.tensorize(docs, bsize=bsize)\n",
    "\n",
    "            batches = [self.doc(input_ids, attention_mask, keep_dims=keep_dims, to_cpu=to_cpu)\n",
    "                       for input_ids, attention_mask in batches]\n",
    "\n",
    "            if keep_dims:\n",
    "                D = _stack_3D_tensors(batches)\n",
    "                return D[reverse_indices]\n",
    "\n",
    "            D = [d for batch in batches for d in batch]\n",
    "            return [D[idx] for idx in reverse_indices.tolist()]\n",
    "\n",
    "        input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)\n",
    "        return self.doc(input_ids, attention_mask, keep_dims=keep_dims)\n",
    "\n",
    "    def score(self, Q, D, mask=None, lengths=None, explain=False):\n",
    "        if lengths is not None:\n",
    "            assert mask is None, \"don't supply both mask and lengths\"\n",
    "\n",
    "            mask = torch.arange(D.size(1), device=DEVICE) + 1\n",
    "            mask = mask.unsqueeze(0) <= lengths.to(DEVICE).unsqueeze(-1)\n",
    "\n",
    "        scores = (D @ Q)\n",
    "        scores = scores if mask is None else scores * mask.unsqueeze(-1)\n",
    "        scores = scores.max(1)\n",
    "\n",
    "        if explain:\n",
    "            assert False, \"TODO\"\n",
    "\n",
    "        return scores.values.sum(-1).cpu()\n",
    "\n",
    "\n",
    "def _stack_3D_tensors(groups):\n",
    "    bsize = sum([x.size(0) for x in groups])\n",
    "    maxlen = max([x.size(1) for x in groups])\n",
    "    hdim = groups[0].size(2)\n",
    "\n",
    "    output = torch.zeros(bsize, maxlen, hdim, device=groups[0].device, dtype=groups[0].dtype)\n",
    "\n",
    "    offset = 0\n",
    "    for x in groups:\n",
    "        endpos = offset + x.size(0)\n",
    "        output[offset:endpos, :x.size(1)] = x\n",
    "        offset = endpos\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef4d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/evaluation/loaders.py\n",
    "def load_queries(queries_path):\n",
    "    queries = OrderedDict()\n",
    "\n",
    "    print_message(\"#> Loading the queries from\", queries_path, \"...\")\n",
    "    \n",
    "    file = pd.read_csv(queries_path, sep=\"\\t\", header=None)\n",
    "    for i in range(len(file)):\n",
    "        qid = file.loc[i][0]\n",
    "        query = file.loc[i][1]\n",
    "        qid = int(qid)\n",
    "        assert (qid not in queries), (\"Query QID\", qid, \"is repeated!\")\n",
    "        queries[qid] = query\n",
    "        \n",
    "    print_message(\"#> Got\", len(queries), \"queries. All QIDs are unique.\\n\")\n",
    "\n",
    "    return queries\n",
    "def load_qrels(qrels_path):\n",
    "    if qrels_path is None:\n",
    "        return None\n",
    "\n",
    "    print_message(\"#> Loading qrels from\", qrels_path, \"...\")\n",
    "\n",
    "    qrels = OrderedDict()\n",
    "    \n",
    "    file = pd.read_csv(qrels_path, sep=\"\\t\", header=None)\n",
    "    for i in range(len(file)):\n",
    "        qid = int(file.loc[i][0])\n",
    "        x = int(file.loc[i][1])\n",
    "        pid = int(file.loc[i][2])\n",
    "        y = int(file.loc[i][3])\n",
    "        assert x == 0 and y == 1\n",
    "        qrels[qid] = qrels.get(qid, [])\n",
    "        qrels[qid].append(pid)\n",
    "\n",
    "    assert all(len(qrels[qid]) == len(set(qrels[qid])) for qid in qrels)\n",
    "\n",
    "    avg_positive = round(sum(len(qrels[qid]) for qid in qrels) / len(qrels), 2)\n",
    "\n",
    "    print_message(\"#> Loaded qrels for\", len(qrels), \"unique queries with\",\n",
    "                  avg_positive, \"positives per query on average.\\n\")\n",
    "\n",
    "    return qrels\n",
    "# colbert/evaluation/loaders.py\n",
    "def load_colbert(args, do_print=True):\n",
    "    colbert, checkpoint = load_model(args, do_print)\n",
    "\n",
    "    # TODO: If the parameters below were not specified on the command line, their *checkpoint* values should be used.\n",
    "    # I.e., not their purely (i.e., training) default values.\n",
    "\n",
    "    for k in ['query_maxlen', 'doc_maxlen', 'dim', 'similarity', 'amp']:\n",
    "        if 'arguments' in checkpoint and hasattr(args, k):\n",
    "            if k in checkpoint['arguments'] and checkpoint['arguments'][k] != getattr(args, k):\n",
    "                a, b = checkpoint['arguments'][k], getattr(args, k)\n",
    "                Run.warn(f\"Got checkpoint['arguments']['{k}'] != args.{k} (i.e., {a} != {b})\")\n",
    "\n",
    "    if 'arguments' in checkpoint:\n",
    "        if args.rank < 1:\n",
    "            print(ujson.dumps(checkpoint['arguments'], indent=4))\n",
    "\n",
    "    if do_print:\n",
    "        print('\\n')\n",
    "\n",
    "    return colbert, checkpoint\n",
    "def load_model(args, do_print=True):\n",
    "    kolbert = KolBERT(\n",
    "        config=MCONFIG,\n",
    "        query_maxlen=args.query_maxlen,\n",
    "        doc_maxlen=args.doc_maxlen,\n",
    "        dim=args.dim,\n",
    "        similarity_metric=args.similarity,\n",
    "        mask_punctuation=args.mask_punctuation\n",
    "    )\n",
    "    kolbert = kolbert.to(DEVICE)\n",
    "\n",
    "    print_message(\"#> Loading model checkpoint.\", condition=do_print)\n",
    "\n",
    "    checkpoint = load_checkpoint(args.checkpoint, kolbert, do_print=do_print)\n",
    "\n",
    "    kolbert.eval()\n",
    "\n",
    "    return kolbert, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb4f943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation/ranking_logger.py\n",
    "from contextlib import contextmanager\n",
    "'''from colbert.utils.utils import print_message, NullContextManager\n",
    "from colbert.utils.runs import Run'''\n",
    "\n",
    "\n",
    "class RankingLogger():\n",
    "    def __init__(self, directory, qrels=None, log_scores=False):\n",
    "        self.directory = directory\n",
    "        self.qrels = qrels\n",
    "        self.filename, self.also_save_annotations = None, None\n",
    "        self.log_scores = log_scores\n",
    "\n",
    "    @contextmanager\n",
    "    def context(self, filename, also_save_annotations=False):\n",
    "        assert self.filename is None\n",
    "        assert self.also_save_annotations is None\n",
    "\n",
    "        filename = os.path.join(self.directory, filename)\n",
    "        self.filename, self.also_save_annotations = filename, also_save_annotations\n",
    "\n",
    "        print_message(\"#> Logging ranked lists to {}\".format(self.filename))\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            self.f = f\n",
    "            with (open(filename + '.annotated', 'w') if also_save_annotations else NullContextManager()) as g:\n",
    "                self.g = g\n",
    "                try:\n",
    "                    yield self\n",
    "                finally:\n",
    "                    pass\n",
    "\n",
    "    def log(self, qid, ranking, is_ranked=True, print_positions=[]):\n",
    "        print_positions = set(print_positions)\n",
    "\n",
    "        f_buffer = []\n",
    "        g_buffer = []\n",
    "\n",
    "        for rank, (score, pid, passage) in enumerate(ranking):\n",
    "            is_relevant = self.qrels and int(pid in self.qrels[qid])\n",
    "            rank = rank+1 if is_ranked else -1\n",
    "\n",
    "            possibly_score = [score] if self.log_scores else []\n",
    "\n",
    "            f_buffer.append('\\t'.join([str(x) for x in [qid, pid, rank] + possibly_score]) + \"\\n\")\n",
    "            if self.g:\n",
    "                g_buffer.append('\\t'.join([str(x) for x in [qid, pid, rank, is_relevant]]) + \"\\n\")\n",
    "\n",
    "            if rank in print_positions:\n",
    "                prefix = \"** \" if is_relevant else \"\"\n",
    "                prefix += str(rank)\n",
    "                print(\"#> ( QID {} ) \".format(qid) + prefix + \") \", pid, \":\", score, '    ', passage)\n",
    "\n",
    "        self.f.write(''.join(f_buffer))\n",
    "        if self.g:\n",
    "            self.g.write(''.join(g_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa52255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing/loaders.py\n",
    "def get_parts(directory):\n",
    "    extension = '.pt'\n",
    "\n",
    "    parts = sorted([int(filename[: -1 * len(extension)]) for filename in os.listdir(directory)\n",
    "                    if filename.endswith(extension)])\n",
    "\n",
    "    assert list(range(len(parts))) == parts, parts\n",
    "\n",
    "    # Integer-sortedness matters.\n",
    "    parts_paths = [os.path.join(directory, '{}{}'.format(filename, extension)) for filename in parts]\n",
    "    samples_paths = [os.path.join(directory, '{}.sample'.format(filename)) for filename in parts]\n",
    "\n",
    "    return parts, parts_paths, samples_paths\n",
    "def load_doclens(directory, flatten=True):\n",
    "    parts, _, _ = get_parts(directory)\n",
    "\n",
    "    doclens_filenames = [os.path.join(directory, 'doclens.{}.json'.format(filename)) for filename in parts]\n",
    "    all_doclens = [ujson.load(open(filename)) for filename in doclens_filenames]\n",
    "\n",
    "    if flatten:\n",
    "        all_doclens = [x for sub_doclens in all_doclens for x in sub_doclens]\n",
    "\n",
    "    return all_doclens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b3a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing/faiss.py\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "'''from colbert.utils.utils import print_message, grouper\n",
    "from colbert.indexing.loaders import get_parts\n",
    "from colbert.indexing.index_manager import load_index_part\n",
    "from colbert.indexing.faiss_index import FaissIndex'''\n",
    "\n",
    "\n",
    "def get_faiss_index_name(args, offset=None, endpos=None):\n",
    "    partitions_info = '' if args.partitions is None else f'.{args.partitions}'\n",
    "    range_info = '' if offset is None else f'.{offset}-{endpos}'\n",
    "\n",
    "    return f'ivfpq{partitions_info}{range_info}.faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc1d622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indexing/index_manager.py\n",
    "class IndexManager():\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "\n",
    "    def save(self, tensor, path_prefix):\n",
    "        torch.save(tensor, path_prefix)\n",
    "\n",
    "\n",
    "def load_index_part(filename, verbose=True):\n",
    "    part = torch.load(filename)\n",
    "\n",
    "    if type(part) == list:  # for backward compatibility\n",
    "        part = torch.cat(part)\n",
    "\n",
    "    return part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f40ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranking/faiss_index.py\n",
    "\n",
    "'''from multiprocessing import Pool\n",
    "from colbert.modeling.inference import ModelInference\n",
    "\n",
    "from colbert.utils.utils import print_message, flatten, batch\n",
    "from colbert.indexing.loaders import load_doclens'''\n",
    "\n",
    "\n",
    "class FaissIndex():\n",
    "    def __init__(self, index_path, faiss_index_path, nprobe, part_range=None):\n",
    "        print_message(\"#> Loading the FAISS index from\", faiss_index_path, \"..\")\n",
    "\n",
    "        faiss_part_range = os.path.basename(faiss_index_path).split('.')[-2].split('-')\n",
    "\n",
    "        if len(faiss_part_range) == 2:\n",
    "            faiss_part_range = range(*map(int, faiss_part_range))\n",
    "            assert part_range[0] in faiss_part_range, (part_range, faiss_part_range)\n",
    "            assert part_range[-1] in faiss_part_range, (part_range, faiss_part_range)\n",
    "        else:\n",
    "            faiss_part_range = None\n",
    "\n",
    "        self.part_range = part_range\n",
    "        self.faiss_part_range = faiss_part_range\n",
    "\n",
    "        self.faiss_index = faiss.read_index(faiss_index_path)\n",
    "        self.faiss_index.nprobe = nprobe\n",
    "\n",
    "        print_message(\"#> Building the emb2pid mapping..\")\n",
    "        all_doclens = load_doclens(index_path, flatten=False)\n",
    "\n",
    "        pid_offset = 0\n",
    "        if faiss_part_range is not None:\n",
    "            print(f\"#> Restricting all_doclens to the range {faiss_part_range}.\")\n",
    "            pid_offset = len(flatten(all_doclens[:faiss_part_range.start]))\n",
    "            all_doclens = all_doclens[faiss_part_range.start:faiss_part_range.stop]\n",
    "\n",
    "        self.relative_range = None\n",
    "        if self.part_range is not None:\n",
    "            start = self.faiss_part_range.start if self.faiss_part_range is not None else 0\n",
    "            a = len(flatten(all_doclens[:self.part_range.start - start]))\n",
    "            b = len(flatten(all_doclens[:self.part_range.stop - start]))\n",
    "            self.relative_range = range(a, b)\n",
    "            print(f\"self.relative_range = {self.relative_range}\")\n",
    "\n",
    "        all_doclens = flatten(all_doclens)\n",
    "\n",
    "        total_num_embeddings = sum(all_doclens)\n",
    "        self.emb2pid = torch.zeros(total_num_embeddings, dtype=torch.int)\n",
    "\n",
    "        offset_doclens = 0\n",
    "        for pid, dlength in enumerate(all_doclens):\n",
    "            self.emb2pid[offset_doclens: offset_doclens + dlength] = pid_offset + pid\n",
    "            offset_doclens += dlength\n",
    "\n",
    "        print_message(\"len(self.emb2pid) =\", len(self.emb2pid))\n",
    "\n",
    "        self.parallel_pool = Pool(16)\n",
    "\n",
    "    def retrieve(self, faiss_depth, Q, verbose=False):\n",
    "        embedding_ids = self.queries_to_embedding_ids(faiss_depth, Q, verbose=verbose)\n",
    "        pids = self.embedding_ids_to_pids(embedding_ids, verbose=verbose)\n",
    "\n",
    "        if self.relative_range is not None:\n",
    "            pids = [[pid for pid in pids_ if pid in self.relative_range] for pids_ in pids]\n",
    "\n",
    "        return pids\n",
    "\n",
    "    def queries_to_embedding_ids(self, faiss_depth, Q, verbose=True):\n",
    "        # Flatten into a matrix for the faiss search.\n",
    "        num_queries, embeddings_per_query, dim = Q.size()\n",
    "        Q_faiss = Q.view(num_queries * embeddings_per_query, dim).cpu().contiguous()\n",
    "\n",
    "        # Search in large batches with faiss.\n",
    "        print_message(\"#> Search in batches with faiss. \\t\\t\",\n",
    "                      f\"Q.size() = {Q.size()}, Q_faiss.size() = {Q_faiss.size()}\",\n",
    "                      condition=verbose)\n",
    "\n",
    "        embeddings_ids = []\n",
    "        faiss_bsize = embeddings_per_query * 5000\n",
    "        for offset in range(0, Q_faiss.size(0), faiss_bsize):\n",
    "            endpos = min(offset + faiss_bsize, Q_faiss.size(0))\n",
    "\n",
    "            print_message(\"#> Searching from {} to {}...\".format(offset, endpos), condition=verbose)\n",
    "\n",
    "            some_Q_faiss = Q_faiss[offset:endpos].float().numpy()\n",
    "            _, some_embedding_ids = self.faiss_index.search(some_Q_faiss, faiss_depth)\n",
    "            embeddings_ids.append(torch.from_numpy(some_embedding_ids))\n",
    "\n",
    "        embedding_ids = torch.cat(embeddings_ids)\n",
    "\n",
    "        # Reshape to (number of queries, non-unique embedding IDs per query)\n",
    "        embedding_ids = embedding_ids.view(num_queries, embeddings_per_query * embedding_ids.size(1))\n",
    "\n",
    "        return embedding_ids\n",
    "\n",
    "    def embedding_ids_to_pids(self, embedding_ids, verbose=True):\n",
    "        # Find unique PIDs per query.\n",
    "        print_message(\"#> Lookup the PIDs..\", condition=verbose)\n",
    "        all_pids = self.emb2pid[embedding_ids]\n",
    "\n",
    "        print_message(f\"#> Converting to a list [shape = {all_pids.size()}]..\", condition=verbose)\n",
    "        all_pids = all_pids.tolist()\n",
    "\n",
    "        print_message(\"#> Removing duplicates (in parallel if large enough)..\", condition=verbose)\n",
    "\n",
    "        if len(all_pids) > 5000:\n",
    "            all_pids = list(self.parallel_pool.map(uniq, all_pids))\n",
    "        else:\n",
    "            all_pids = list(map(uniq, all_pids))\n",
    "\n",
    "        print_message(\"#> Done with embedding_ids_to_pids().\", condition=verbose)\n",
    "\n",
    "        return all_pids\n",
    "\n",
    "\n",
    "def uniq(l):\n",
    "    return list(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb713b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idnexing/faiss_index_gpu.py\n",
    "class FaissIndexGPU():\n",
    "    def __init__(self):\n",
    "        self.ngpu = faiss.get_num_gpus()\n",
    "\n",
    "        if self.ngpu == 0:\n",
    "            return\n",
    "\n",
    "        self.tempmem = 1 << 33\n",
    "        self.max_add_per_gpu = 1 << 25\n",
    "        self.max_add = self.max_add_per_gpu * self.ngpu\n",
    "        self.add_batch_size = 65536\n",
    "\n",
    "        self.gpu_resources = self._prepare_gpu_resources()\n",
    "\n",
    "    def _prepare_gpu_resources(self):\n",
    "        print_message(f\"Preparing resources for {self.ngpu} GPUs.\")\n",
    "\n",
    "        gpu_resources = []\n",
    "\n",
    "        for _ in range(self.ngpu):\n",
    "            res = faiss.StandardGpuResources()\n",
    "            if self.tempmem >= 0:\n",
    "                res.setTempMemory(self.tempmem)\n",
    "            gpu_resources.append(res)\n",
    "\n",
    "        return gpu_resources\n",
    "\n",
    "    def _make_vres_vdev(self):\n",
    "        \"\"\"\n",
    "        return vectors of device ids and resources useful for gpu_multiple\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        vres = faiss.GpuResourcesVector()\n",
    "        vdev = faiss.IntVector()\n",
    "\n",
    "        for i in range(self.ngpu):\n",
    "            vdev.push_back(i)\n",
    "            vres.push_back(self.gpu_resources[i])\n",
    "\n",
    "        return vres, vdev\n",
    "\n",
    "    def training_initialize(self, index, quantizer):\n",
    "        \"\"\"\n",
    "        The index and quantizer should be owned by caller.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        s = time.time()\n",
    "        self.index_ivf = faiss.extract_index_ivf(index)\n",
    "        self.clustering_index = faiss.index_cpu_to_all_gpus(quantizer)\n",
    "        self.index_ivf.clustering_index = self.clustering_index\n",
    "        print(time.time() - s)\n",
    "\n",
    "    def training_finalize(self):\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        s = time.time()\n",
    "        self.index_ivf.clustering_index = faiss.index_gpu_to_cpu(self.index_ivf.clustering_index)\n",
    "        print(time.time() - s)\n",
    "\n",
    "    def adding_initialize(self, index):\n",
    "        \"\"\"\n",
    "        The index should be owned by caller.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        self.co = faiss.GpuMultipleClonerOptions()\n",
    "        self.co.useFloat16 = True\n",
    "        self.co.useFloat16CoarseQuantizer = False\n",
    "        self.co.usePrecomputed = False\n",
    "        self.co.indicesOptions = faiss.INDICES_CPU\n",
    "        self.co.verbose = True\n",
    "        self.co.reserveVecs = self.max_add\n",
    "        self.co.shard = True\n",
    "        assert self.co.shard_type in (0, 1, 2)\n",
    "\n",
    "        self.vres, self.vdev = self._make_vres_vdev()\n",
    "        self.gpu_index = faiss.index_cpu_to_gpu_multiple(self.vres, self.vdev, index, self.co)\n",
    "\n",
    "    def add(self, index, data, offset):\n",
    "        assert self.ngpu > 0\n",
    "\n",
    "        t0 = time.time()\n",
    "        nb = data.shape[0]\n",
    "\n",
    "        for i0 in range(0, nb, self.add_batch_size):\n",
    "            i1 = min(i0 + self.add_batch_size, nb)\n",
    "            xs = data[i0:i1]\n",
    "\n",
    "            self.gpu_index.add_with_ids(xs, np.arange(offset+i0, offset+i1))\n",
    "\n",
    "            if self.max_add > 0 and self.gpu_index.ntotal > self.max_add:\n",
    "                self._flush_to_cpu(index, nb, offset)\n",
    "\n",
    "            print('\\r%d/%d (%.3f s)  ' % (i0, nb, time.time() - t0), end=' ')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        if self.gpu_index.ntotal > 0:\n",
    "            self._flush_to_cpu(index, nb, offset)\n",
    "\n",
    "        assert index.ntotal == offset+nb, (index.ntotal, offset+nb, offset, nb)\n",
    "        print(f\"add(.) time: %.3f s \\t\\t--\\t\\t index.ntotal = {index.ntotal}\" % (time.time() - t0))\n",
    "\n",
    "    def _flush_to_cpu(self, index, nb, offset):\n",
    "        print(\"Flush indexes to CPU\")\n",
    "\n",
    "        for i in range(self.ngpu):\n",
    "            index_src_gpu = faiss.downcast_index(self.gpu_index if self.ngpu == 1 else self.gpu_index.at(i))\n",
    "            index_src = faiss.index_gpu_to_cpu(index_src_gpu)\n",
    "\n",
    "            index_src.copy_subset_to(index, 0, offset, offset+nb)\n",
    "            index_src_gpu.reset()\n",
    "            index_src_gpu.reserveMemory(self.max_add)\n",
    "\n",
    "        if self.ngpu > 1:\n",
    "            try:\n",
    "                self.gpu_index.sync_with_shard_indexes()\n",
    "            except:\n",
    "                self.gpu_index.syncWithSubIndexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a150dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e7edb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking/retrieval.py\n",
    "from multiprocessing import Pool\n",
    "'''\n",
    "from colbert.modeling.inference import ModelInference\n",
    "from colbert.evaluation.ranking_logger import RankingLogger\n",
    "\n",
    "from colbert.utils.utils import print_message, batch\n",
    "from colbert.ranking.rankers import Ranker'''\n",
    "\n",
    "\n",
    "def retrieve(args):\n",
    "    import itertools\n",
    "    inference = ModelInference(args.colbert, amp=args.amp)\n",
    "    ranker = Ranker(args, inference, faiss_depth=args.faiss_depth)\n",
    "\n",
    "    ranking_logger = RankingLogger(Run.path, qrels=None)\n",
    "    milliseconds = 0\n",
    "\n",
    "    with ranking_logger.context('ranking.tsv', also_save_annotations=False) as rlogger:\n",
    "        queries = args.queries\n",
    "        qids_in_order = list(queries.keys())\n",
    "\n",
    "        for qoffset, qbatch in batch(qids_in_order, 100, provide_offset=True):\n",
    "            qbatch_text = [queries[qid] for qid in qbatch]\n",
    "\n",
    "            rankings = []\n",
    "\n",
    "            for query_idx, q in enumerate(qbatch_text):\n",
    "                torch.cuda.synchronize('cuda:0')\n",
    "                s = time.time()\n",
    "\n",
    "                Q = ranker.encode([q])\n",
    "                pids, scores = ranker.rank(Q)\n",
    "\n",
    "                torch.cuda.synchronize()\n",
    "                milliseconds += (time.time() - s) * 1000.0\n",
    "\n",
    "                if len(pids):\n",
    "                    print(qoffset+query_idx, q, len(scores), len(pids), scores[0], pids[0],\n",
    "                          milliseconds / (qoffset+query_idx+1), 'ms')\n",
    "\n",
    "                rankings.append(zip(pids, scores))\n",
    "\n",
    "            for query_idx, (qid, ranking) in enumerate(zip(qbatch, rankings)):\n",
    "                query_idx = qoffset + query_idx\n",
    "\n",
    "                if query_idx % 100 == 0:\n",
    "                    print_message(f\"#> Logging query #{query_idx} (qid {qid}) now...\")\n",
    "\n",
    "                ranking = [(score, pid, None) for pid, score in itertools.islice(ranking, args.depth)]\n",
    "                rlogger.log(qid, ranking, is_ranked=True)\n",
    "\n",
    "    print('\\n\\n')\n",
    "    print(ranking_logger.filename)\n",
    "    print(\"#> Done.\")\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ffe31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "'''from colbert.ranking.index_part import IndexPart\n",
    "from colbert.ranking.faiss_index import FaissIndex\n",
    "from colbert.utils.utils import flatten, zipstar'''\n",
    "\n",
    "\n",
    "class Ranker():\n",
    "    def __init__(self, args, inference, faiss_depth=1024):\n",
    "        self.inference = inference\n",
    "        self.faiss_depth = faiss_depth\n",
    "\n",
    "        if faiss_depth is not None:\n",
    "            self.faiss_index = FaissIndex(args.index_path, args.faiss_index_path, args.nprobe, part_range=args.part_range)\n",
    "            self.retrieve = partial(self.faiss_index.retrieve, self.faiss_depth)\n",
    "\n",
    "        self.index = IndexPart(args.index_path, dim=inference.colbert.dim, part_range=args.part_range, verbose=True)\n",
    "\n",
    "    def encode(self, queries):\n",
    "        assert type(queries) in [list, tuple], type(queries)\n",
    "\n",
    "        Q = self.inference.queryFromText(queries, bsize=512 if len(queries) > 512 else None)\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def rank(self, Q, pids=None):\n",
    "        pids = self.retrieve(Q, verbose=False)[0] if pids is None else pids\n",
    "\n",
    "        assert type(pids) in [list, tuple], type(pids)\n",
    "        assert Q.size(0) == 1, (len(pids), Q.size())\n",
    "        assert all(type(pid) is int for pid in pids)\n",
    "\n",
    "        scores = []\n",
    "        if len(pids) > 0:\n",
    "            Q = Q.permute(0, 2, 1)\n",
    "            scores = self.index.rank(Q, pids)\n",
    "\n",
    "            scores_sorter = torch.tensor(scores).sort(descending=True)\n",
    "            pids, scores = torch.tensor(pids)[scores_sorter.indices].tolist(), scores_sorter.values.tolist()\n",
    "\n",
    "        return pids, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e0bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking/index_ranker.py\n",
    "BSIZE = 1 << 14\n",
    "\n",
    "\n",
    "class IndexRanker():\n",
    "    def __init__(self, tensor, doclens):\n",
    "        self.tensor = tensor\n",
    "        self.doclens = doclens\n",
    "\n",
    "        self.maxsim_dtype = torch.float32\n",
    "        self.doclens_pfxsum = [0] + list(accumulate(self.doclens))\n",
    "\n",
    "        self.doclens = torch.tensor(self.doclens)\n",
    "        self.doclens_pfxsum = torch.tensor(self.doclens_pfxsum)\n",
    "\n",
    "        self.dim = self.tensor.size(-1)\n",
    "\n",
    "        self.strides = [torch_percentile(self.doclens, p) for p in [90]]\n",
    "        self.strides.append(self.doclens.max().item())\n",
    "        self.strides = sorted(list(set(self.strides)))\n",
    "\n",
    "        print_message(f\"#> Using strides {self.strides}..\")\n",
    "\n",
    "        self.views = self._create_views(self.tensor)\n",
    "        self.buffers = self._create_buffers(BSIZE, self.tensor.dtype, {'cpu', 'cuda:0'})\n",
    "\n",
    "    def _create_views(self, tensor):\n",
    "        views = []\n",
    "\n",
    "        for stride in self.strides:\n",
    "            outdim = tensor.size(0) - stride + 1\n",
    "            view = torch.as_strided(tensor, (outdim, stride, self.dim), (self.dim, self.dim, 1))\n",
    "            views.append(view)\n",
    "\n",
    "        return views\n",
    "\n",
    "    def _create_buffers(self, max_bsize, dtype, devices):\n",
    "        buffers = {}\n",
    "\n",
    "        for device in devices:\n",
    "            buffers[device] = [torch.zeros(max_bsize, stride, self.dim, dtype=dtype,\n",
    "                                           device=device, pin_memory=(device == 'cpu'))\n",
    "                               for stride in self.strides]\n",
    "\n",
    "        return buffers\n",
    "\n",
    "    def rank(self, Q, pids, views=None, shift=0):\n",
    "        assert len(pids) > 0\n",
    "        assert Q.size(0) in [1, len(pids)]\n",
    "\n",
    "        Q = Q.contiguous().to(DEVICE).to(dtype=self.maxsim_dtype)\n",
    "\n",
    "        views = self.views if views is None else views\n",
    "        VIEWS_DEVICE = views[0].device\n",
    "\n",
    "        D_buffers = self.buffers[str(VIEWS_DEVICE)]\n",
    "\n",
    "        raw_pids = pids if type(pids) is list else pids.tolist()\n",
    "        pids = torch.tensor(pids) if type(pids) is list else pids\n",
    "\n",
    "        doclens, offsets = self.doclens[pids], self.doclens_pfxsum[pids]\n",
    "\n",
    "        assignments = (doclens.unsqueeze(1) > torch.tensor(self.strides).unsqueeze(0) + 1e-6).sum(-1)\n",
    "\n",
    "        one_to_n = torch.arange(len(raw_pids))\n",
    "        output_pids, output_scores, output_permutation = [], [], []\n",
    "\n",
    "        for group_idx, stride in enumerate(self.strides):\n",
    "            locator = (assignments == group_idx)\n",
    "\n",
    "            if locator.sum() < 1e-5:\n",
    "                continue\n",
    "\n",
    "            group_pids, group_doclens, group_offsets = pids[locator], doclens[locator], offsets[locator]\n",
    "            group_Q = Q if Q.size(0) == 1 else Q[locator]\n",
    "\n",
    "            group_offsets = group_offsets.to(VIEWS_DEVICE) - shift\n",
    "            group_offsets_uniq, group_offsets_expand = torch.unique_consecutive(group_offsets, return_inverse=True)\n",
    "\n",
    "            D_size = group_offsets_uniq.size(0)\n",
    "            D = torch.index_select(views[group_idx], 0, group_offsets_uniq, out=D_buffers[group_idx][:D_size])\n",
    "            D = D.to(DEVICE)\n",
    "            D = D[group_offsets_expand.to(DEVICE)].to(dtype=self.maxsim_dtype)\n",
    "\n",
    "            mask = torch.arange(stride, device=DEVICE) + 1\n",
    "            mask = mask.unsqueeze(0) <= group_doclens.to(DEVICE).unsqueeze(-1)\n",
    "\n",
    "            scores = (D @ group_Q) * mask.unsqueeze(-1)\n",
    "            scores = scores.max(1).values.sum(-1).cpu()\n",
    "\n",
    "            output_pids.append(group_pids)\n",
    "            output_scores.append(scores)\n",
    "            output_permutation.append(one_to_n[locator])\n",
    "\n",
    "        output_permutation = torch.cat(output_permutation).sort().indices\n",
    "        output_pids = torch.cat(output_pids)[output_permutation].tolist()\n",
    "        output_scores = torch.cat(output_scores)[output_permutation].tolist()\n",
    "\n",
    "        assert len(raw_pids) == len(output_pids)\n",
    "        assert len(raw_pids) == len(output_scores)\n",
    "        assert raw_pids == output_pids\n",
    "\n",
    "        return output_scores\n",
    "\n",
    "    def batch_rank(self, all_query_embeddings, all_query_indexes, all_pids, sorted_pids):\n",
    "        assert sorted_pids is True\n",
    "\n",
    "        ######\n",
    "\n",
    "        scores = []\n",
    "        range_start, range_end = 0, 0\n",
    "\n",
    "        for pid_offset in range(0, len(self.doclens), 50_000):\n",
    "            pid_endpos = min(pid_offset + 50_000, len(self.doclens))\n",
    "\n",
    "            range_start = range_start + (all_pids[range_start:] < pid_offset).sum()\n",
    "            range_end = range_end + (all_pids[range_end:] < pid_endpos).sum()\n",
    "\n",
    "            pids = all_pids[range_start:range_end]\n",
    "            query_indexes = all_query_indexes[range_start:range_end]\n",
    "\n",
    "            print_message(f\"###--> Got {len(pids)} query--passage pairs in this sub-range {(pid_offset, pid_endpos)}.\")\n",
    "\n",
    "            if len(pids) == 0:\n",
    "                continue\n",
    "\n",
    "            print_message(f\"###--> Ranking in batches the pairs #{range_start} through #{range_end} in this sub-range.\")\n",
    "\n",
    "            tensor_offset = self.doclens_pfxsum[pid_offset].item()\n",
    "            tensor_endpos = self.doclens_pfxsum[pid_endpos].item() + 512\n",
    "\n",
    "            collection = self.tensor[tensor_offset:tensor_endpos].to(DEVICE)\n",
    "            views = self._create_views(collection)\n",
    "\n",
    "            print_message(f\"#> Ranking in batches of {BSIZE} query--passage pairs...\")\n",
    "\n",
    "            for batch_idx, offset in enumerate(range(0, len(pids), BSIZE)):\n",
    "                if batch_idx % 100 == 0:\n",
    "                    print_message(\"#> Processing batch #{}..\".format(batch_idx))\n",
    "\n",
    "                endpos = offset + BSIZE\n",
    "                batch_query_index, batch_pids = query_indexes[offset:endpos], pids[offset:endpos]\n",
    "\n",
    "                Q = all_query_embeddings[batch_query_index]\n",
    "\n",
    "                scores.extend(self.rank(Q, batch_pids, views, shift=tensor_offset))\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "def torch_percentile(tensor, p):\n",
    "    assert p in range(1, 100+1)\n",
    "    assert tensor.dim() == 1\n",
    "\n",
    "    return tensor.kthvalue(int(p * tensor.size(0) / 100.0)).values.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa88fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranking/index_part\n",
    "'''from math import ceil\n",
    "from itertools import accumulate\n",
    "from colbert.utils.utils import print_message, dotdict, flatten\n",
    "\n",
    "from colbert.indexing.loaders import get_parts, load_doclens\n",
    "from colbert.indexing.index_manager import load_index_part\n",
    "from colbert.ranking.index_ranker import IndexRanker'''\n",
    "\n",
    "\n",
    "class IndexPart():\n",
    "    def __init__(self, directory, dim=128, part_range=None, verbose=True):\n",
    "        first_part, last_part = (0, None) if part_range is None else (part_range.start, part_range.stop)\n",
    "\n",
    "        # Load parts metadata\n",
    "        all_parts, all_parts_paths, _ = get_parts(directory)\n",
    "        self.parts = all_parts[first_part:last_part]\n",
    "        self.parts_paths = all_parts_paths[first_part:last_part]\n",
    "\n",
    "        # Load doclens metadata\n",
    "        all_doclens = load_doclens(directory, flatten=False)\n",
    "\n",
    "        self.doc_offset = sum([len(part_doclens) for part_doclens in all_doclens[:first_part]])\n",
    "        self.doc_endpos = sum([len(part_doclens) for part_doclens in all_doclens[:last_part]])\n",
    "        self.pids_range = range(self.doc_offset, self.doc_endpos)\n",
    "\n",
    "        self.parts_doclens = all_doclens[first_part:last_part]\n",
    "        self.doclens = flatten(self.parts_doclens)\n",
    "        self.num_embeddings = sum(self.doclens)\n",
    "\n",
    "        self.tensor = self._load_parts(dim, verbose)\n",
    "        self.ranker = IndexRanker(self.tensor, self.doclens)\n",
    "\n",
    "    def _load_parts(self, dim, verbose):\n",
    "        tensor = torch.zeros(self.num_embeddings + 512, dim, dtype=torch.float16)\n",
    "\n",
    "        if verbose:\n",
    "            print_message(\"tensor.size() = \", tensor.size())\n",
    "\n",
    "        offset = 0\n",
    "        for idx, filename in enumerate(self.parts_paths):\n",
    "            print_message(\"|> Loading\", filename, \"...\", condition=verbose)\n",
    "\n",
    "            endpos = offset + sum(self.parts_doclens[idx])\n",
    "            part = load_index_part(filename, verbose=verbose)\n",
    "\n",
    "            tensor[offset:endpos] = part\n",
    "            offset = endpos\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def pid_in_range(self, pid):\n",
    "        return pid in self.pids_range\n",
    "\n",
    "    def rank(self, Q, pids):\n",
    "        \"\"\"\n",
    "        Rank a single batch of Q x pids (e.g., 1k--10k pairs).\n",
    "        \"\"\"\n",
    "\n",
    "        assert Q.size(0) in [1, len(pids)], (Q.size(0), len(pids))\n",
    "        assert all(pid in self.pids_range for pid in pids), self.pids_range\n",
    "\n",
    "        pids_ = [pid - self.doc_offset for pid in pids]\n",
    "        scores = self.ranker.rank(Q, pids_)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def batch_rank(self, all_query_embeddings, query_indexes, pids, sorted_pids):\n",
    "        \"\"\"\n",
    "        Rank a large, fairly dense set of query--passage pairs (e.g., 1M+ pairs).\n",
    "        Higher overhead, much faster for large batches.\n",
    "        \"\"\"\n",
    "\n",
    "        assert ((pids >= self.pids_range.start) & (pids < self.pids_range.stop)).sum() == pids.size(0)\n",
    "\n",
    "        pids_ = pids - self.doc_offset\n",
    "        scores = self.ranker.batch_rank(all_query_embeddings, query_indexes, pids_, sorted_pids)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5fb4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/utils.py\n",
    "def print_message(*s, condition=True):\n",
    "    s = ' '.join([str(x) for x in s])\n",
    "    msg = \"[{}] {}\".format(datetime.datetime.now().strftime(\"%b %d, %H:%M:%S\"), s)\n",
    "\n",
    "    if condition:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "    return msg\n",
    "def timestamp():\n",
    "    format_str = \"%Y-%m-%d_%H.%M.%S\"\n",
    "    result = datetime.datetime.now().strftime(format_str)\n",
    "    return result\n",
    "\n",
    "def create_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        print('\\n')\n",
    "        print_message(\"#> Note: Output directory\", path, 'already exists\\n\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "        print_message(\"#> Creating directory\", path, '\\n\\n')\n",
    "        os.makedirs(path)\n",
    "        \n",
    "def distributed_init(rank):\n",
    "    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])\n",
    "    nranks = max(1, nranks)\n",
    "    is_distributed = nranks > 1\n",
    "\n",
    "    if rank == 0:\n",
    "        print('nranks =', nranks, '\\t num_gpus =', torch.cuda.device_count())\n",
    "\n",
    "    if is_distributed:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        torch.cuda.set_device(rank % num_gpus)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    return nranks, is_distributed\n",
    "\n",
    "def distributed_barrier(rank):\n",
    "    if rank >= 0:\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "def load_checkpoint(path, model, optimizer=None, do_print=True):\n",
    "    if do_print:\n",
    "        print_message(\"#> Loading checkpoint\", path, \"..\")\n",
    "\n",
    "    if path.startswith(\"http:\") or path.startswith(\"https:\"):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k\n",
    "        if k[:7] == 'module.':\n",
    "            name = k[7:]\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    checkpoint['model_state_dict'] = new_state_dict\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        print_message(\"[WARNING] Loading checkpoint with strict=False\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if do_print:\n",
    "        print_message(\"#> checkpoint['epoch'] =\", checkpoint['epoch'])\n",
    "        print_message(\"#> checkpoint['batch'] =\", checkpoint['batch'])\n",
    "\n",
    "    return checkpoint\n",
    "class NullContextManager(object):\n",
    "    def __init__(self, dummy_resource=None):\n",
    "        self.dummy_resource = dummy_resource\n",
    "    def __enter__(self):\n",
    "        return self.dummy_resource\n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "    \n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"\"\"\n",
    "    Collect data into fixed-length chunks or blocks\n",
    "        Example: grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "        Source: https://docs.python.org/3/library/itertools.html#itertools-recipes\n",
    "    \"\"\"\n",
    "\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "def batch(group, bsize, provide_offset=False):\n",
    "    offset = 0\n",
    "    while offset < len(group):\n",
    "        L = group[offset: offset + bsize]\n",
    "        yield ((offset, L) if provide_offset else L)\n",
    "        offset += len(L)\n",
    "    return\n",
    "class dotdict(dict):\n",
    "    \"\"\"\n",
    "    dot.notation access to dictionary attributes\n",
    "    Credit: derek73 @ https://stackoverflow.com/questions/2352181\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "def flatten(L):\n",
    "    return [x for y in L for x in y]\n",
    "def zipstar(L, lazy=False):\n",
    "    \"\"\"\n",
    "    A much faster A, B, C = zip(*[(a, b, c), (a, b, c), ...])\n",
    "    May return lists or tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(L) == 0:\n",
    "        return L\n",
    "\n",
    "    width = len(L[0])\n",
    "\n",
    "    if width < 100:\n",
    "        return [[elem[idx] for elem in L] for idx in range(width)]\n",
    "\n",
    "    L = zip(*L)\n",
    "\n",
    "    return L if lazy else list(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80cf14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/utils/amp.py\n",
    "'''\n",
    "from contextlib import contextmanager\n",
    "from colbert.utils.utils import NullContextManager'''\n",
    "from packaging import version\n",
    "\n",
    "v = version.parse\n",
    "PyTorch_over_1_6  = v(torch.__version__) >= v('1.6')\n",
    "\n",
    "class MixedPrecisionManager():\n",
    "    def __init__(self, activated):\n",
    "        assert (not activated) or PyTorch_over_1_6, \"Cannot use AMP for PyTorch version < 1.6\"\n",
    "\n",
    "        self.activated = activated\n",
    "\n",
    "        if self.activated:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def context(self):\n",
    "        return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
    "\n",
    "    def backward(self, loss):\n",
    "        if self.activated:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "    def step(self, kolbert, optimizer):\n",
    "        if self.activated:\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(kolbert.parameters(), 2.0)\n",
    "\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(kolbert.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7077527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/logging.py\n",
    "class Logger():\n",
    "    def __init__(self, rank, run):\n",
    "        self.rank = rank\n",
    "        self.is_main = self.rank in [-1, 0]\n",
    "        self.run = run\n",
    "        self.logs_path = os.path.join(self.run.path, \"logs/\")\n",
    "\n",
    "        if self.is_main:\n",
    "            self._init_mlflow()\n",
    "            self.initialized_tensorboard = False\n",
    "            create_directory(self.logs_path)\n",
    "\n",
    "    def _init_mlflow(self):\n",
    "        mlflow.set_tracking_uri('file://' + os.path.join(self.run.experiments_root, \"logs/mlruns/\"))\n",
    "        mlflow.set_experiment('/'.join([self.run.experiment, self.run.script]))\n",
    "        \n",
    "        mlflow.set_tag('experiment', self.run.experiment)\n",
    "        mlflow.set_tag('name', self.run.name)\n",
    "        mlflow.set_tag('path', self.run.path)\n",
    "\n",
    "    def _init_tensorboard(self):\n",
    "        root = os.path.join(self.run.experiments_root, \"logs/tensorboard/\")\n",
    "        logdir = '__'.join([self.run.experiment, self.run.script, self.run.name])\n",
    "        logdir = os.path.join(root, logdir)\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=logdir)\n",
    "        self.initialized_tensorboard = True\n",
    "\n",
    "    def _log_exception(self, etype, value, tb):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        output_path = os.path.join(self.logs_path, 'exception.txt')\n",
    "        trace = ''.join(traceback.format_exception(etype, value, tb)) + '\\n'\n",
    "        print_message(trace, '\\n\\n')\n",
    "\n",
    "        self.log_new_artifact(output_path, trace)\n",
    "\n",
    "    def _log_all_artifacts(self):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        mlflow.log_artifacts(self.logs_path)\n",
    "\n",
    "    def _log_args(self, args):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        for key in vars(args):\n",
    "            value = getattr(args, key)\n",
    "            if type(value) in [int, float, str, bool]:\n",
    "                mlflow.log_param(key, value)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.json'), 'w') as output_metadata:\n",
    "            ujson.dump(args.input_arguments.__dict__, output_metadata, indent=4)\n",
    "            output_metadata.write('\\n')\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.txt'), 'w') as output_metadata:\n",
    "            output_metadata.write(' '.join(sys.argv) + '\\n')\n",
    "\n",
    "    def log_metric(self, name, value, step, log_to_mlflow=True):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        if not self.initialized_tensorboard:\n",
    "            self._init_tensorboard()\n",
    "\n",
    "        if log_to_mlflow:\n",
    "            mlflow.log_metric(name, value, step=step)\n",
    "        self.writer.add_scalar(name, value, step)\n",
    "\n",
    "    def log_new_artifact(self, path, content):\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        mlflow.log_artifact(path)\n",
    "\n",
    "    def warn(self, *args):\n",
    "        msg = print_message('[WARNING]', '\\t', *args)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'warnings.txt'), 'a') as output_metadata:\n",
    "            output_metadata.write(msg + '\\n\\n\\n')\n",
    "\n",
    "    def info_all(self, *args):\n",
    "        print_message('[' + str(self.rank) + ']', '\\t', *args)\n",
    "\n",
    "    def info(self, *args):\n",
    "        if self.is_main:\n",
    "            print_message(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1003b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/runs.py\n",
    "'''\n",
    "import colbert.utils.distributed as distributed\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from colbert.utils.logging import Logger\n",
    "from colbert.utils.utils import timestamp, create_directory, print_message\n",
    "'''\n",
    "class _RunManager():\n",
    "    def __init__(self):\n",
    "        self.experiments_root = None\n",
    "        self.experiment = None\n",
    "        self.path = None\n",
    "        self.script = self._get_script_name()\n",
    "        self.name = self._generate_default_run_name()\n",
    "        self.original_name = self.name\n",
    "        self.exit_status = 'FINISHED'\n",
    "\n",
    "        self._logger = None\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def init(self, rank, root, experiment, name):\n",
    "        assert '/' not in experiment, experiment\n",
    "        assert '/' not in name, name\n",
    "\n",
    "        self.experiments_root = os.path.abspath(root)\n",
    "        self.experiment = experiment\n",
    "        self.name = name\n",
    "        self.path = os.path.join(self.experiments_root, self.experiment, self.script, self.name)\n",
    "\n",
    "        if rank < 1:\n",
    "            if os.path.exists(self.path):\n",
    "                print('\\n\\n')\n",
    "                print_message(\"It seems that \", self.path, \" already exists.\")\n",
    "                print_message(\"Do you want to overwrite it? \\t yes/no \\n\")\n",
    "\n",
    "                # TODO: This should timeout and exit (i.e., fail) given no response for 60 seconds.\n",
    "\n",
    "                response = input()\n",
    "                if response.strip() != 'yes':\n",
    "                    assert not os.path.exists(self.path), self.path\n",
    "            else:\n",
    "                create_directory(self.path)\n",
    "\n",
    "        distributed_barrier(rank)\n",
    "\n",
    "        self._logger = Logger(rank, self)\n",
    "        self._log_args = self._logger._log_args\n",
    "        self.warn = self._logger.warn\n",
    "        self.info = self._logger.info\n",
    "        self.info_all = self._logger.info_all\n",
    "        self.log_metric = self._logger.log_metric\n",
    "        self.log_new_artifact = self._logger.log_new_artifact\n",
    "\n",
    "    def _generate_default_run_name(self):\n",
    "        return timestamp()\n",
    "\n",
    "    def _get_script_name(self):\n",
    "        return os.path.basename('main'.__file__) if '__file__' in dir('main') else 'none'\n",
    "#         return os.path.basename(__main__.__file__) if '__file__' in dir(__main__) else 'none'\n",
    "\n",
    "    @contextmanager\n",
    "    def context(self, consider_failed_if_interrupted=True):\n",
    "        try:\n",
    "            yield\n",
    "\n",
    "        except KeyboardInterrupt as ex:\n",
    "            print('\\n\\nInterrupted\\n\\n')\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            if consider_failed_if_interrupted:\n",
    "                self.exit_status = 'KILLED'  # mlflow.entities.RunStatus.KILLED\n",
    "\n",
    "            sys.exit(128 + 2)\n",
    "\n",
    "        except Exception as ex:\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            self.exit_status = 'FAILED'  # mlflow.entities.RunStatus.FAILED\n",
    "\n",
    "            raise ex\n",
    "\n",
    "        finally:\n",
    "            total_seconds = str(time.time() - self.start_time) + '\\n'\n",
    "            original_name = str(self.original_name)\n",
    "            name = str(self.name)\n",
    "\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'elapsed.txt'), total_seconds)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.original.txt'), original_name)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.txt'), name)\n",
    "\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            mlflow.end_run(status=self.exit_status)\n",
    "\n",
    "\n",
    "Run = _RunManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef32719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/parser.py\n",
    "class Arguments():\n",
    "    def __init__(self, description):\n",
    "        self.parser = ArgumentParser(description=description)\n",
    "        self.checks = []\n",
    "\n",
    "        self.add_argument('--root', dest='root', default='/')\n",
    "        self.add_argument('--experiment', dest='experiment', default='MSMARCO-psg')\n",
    "        self.add_argument('--run', dest='run', default=Run.name)\n",
    "\n",
    "        self.add_argument('--local_rank', dest='rank', default=-1, type=int)\n",
    "\n",
    "    def add_model_parameters(self):\n",
    "        # Core Arguments\n",
    "        self.add_argument('--similarity', dest='similarity', default='cosine', choices=['cosine', 'l2'])\n",
    "        self.add_argument('--dim', dest='dim', default=128, type=int)\n",
    "        self.add_argument('--query_maxlen', dest='query_maxlen', default=32, type=int)\n",
    "        self.add_argument('--doc_maxlen', dest='doc_maxlen', default=180, type=int)\n",
    "\n",
    "        # Filtering-related Arguments\n",
    "        self.add_argument('--mask-punctuation', dest='mask_punctuation', default=True, action='store_true')\n",
    "\n",
    "    def add_index_use_input(self):\n",
    "        self.add_argument('--index_root', dest='index_root', default='/')\n",
    "        self.add_argument('--index_name', dest='index_name', default='MSMARCO.L2.32x200k')\n",
    "        self.add_argument('--partitions', dest='partitions', default=32768, type=int)\n",
    "        \n",
    "    def add_model_inference_parameters(self):\n",
    "        self.add_argument('--checkpoint', dest='checkpoint', default='/', type=str)\n",
    "        self.add_argument('--bsize', dest='bsize', default=256, type=int)\n",
    "        self.add_argument('--amp', dest='amp', default=180, action='store_true')\n",
    "\n",
    "    def add_ranking_input(self):\n",
    "        self.add_argument('--queries', dest='queries', default='/')\n",
    "        self.add_argument('--collection', dest='collection', default=None)\n",
    "        self.add_argument('--qrels', dest='qrels', default=None)\n",
    "\n",
    "    def add_retrieval_input(self):\n",
    "        self.add_index_use_input()\n",
    "        self.add_argument('--nprobe', dest='nprobe', default=32, type=int)\n",
    "        self.add_argument('--retrieve_only', dest='retrieve_only', default=False, action='store_true')\n",
    "\n",
    "    def add_argument(self, *args, **kw_args):\n",
    "        return self.parser.add_argument(*args, **kw_args)\n",
    "\n",
    "    def check_arguments(self, args):\n",
    "        for check in self.checks:\n",
    "            check(args)\n",
    "\n",
    "    def parse(self):\n",
    "        args = self.parser.parse_args()\n",
    "        self.check_arguments(args)\n",
    "\n",
    "        args.input_arguments = copy.deepcopy(args)\n",
    "\n",
    "        args.nranks, args.distributed = distributed_init(args.rank)\n",
    "\n",
    "        args.nthreads = int(max(os.cpu_count(), faiss.omp_get_max_threads()) * 0.8)\n",
    "        args.nthreads = max(1, args.nthreads // args.nranks)\n",
    "\n",
    "        if args.nranks > 1:\n",
    "            print_message(f\"#> Restricting number of threads for FAISS to {args.nthreads} per process\",\n",
    "                          condition=(args.rank == 0))\n",
    "            faiss.omp_set_num_threads(args.nthreads)\n",
    "\n",
    "        Run.init(args.rank, args.root, args.experiment, args.run)\n",
    "        Run._log_args(args)\n",
    "        Run.info(args.input_arguments.__dict__, '\\n')\n",
    "\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bef948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve.py\n",
    "'''from colbert.utils.parser import Arguments\n",
    "from colbert.utils.runs import Run\n",
    "\n",
    "from colbert.evaluation.loaders import load_colbert, load_qrels, load_queries\n",
    "from colbert.indexing.faiss import get_faiss_index_name\n",
    "from colbert.ranking.retrieval import retrieve\n",
    "from colbert.ranking.batch_retrieval import batch_retrieve'''\n",
    "def main():\n",
    "    random.seed(12345)\n",
    "\n",
    "    parser = Arguments(description='End-to-end retrieval and ranking with ColBERT.')\n",
    "\n",
    "    parser.add_model_parameters()\n",
    "    parser.add_model_inference_parameters()\n",
    "    parser.add_ranking_input()\n",
    "    parser.add_retrieval_input()\n",
    "\n",
    "    parser.add_argument('--faiss_name', dest='faiss_name', default=None, type=str)\n",
    "    parser.add_argument('--faiss_depth', dest='faiss_depth', default=1024, type=int)\n",
    "    parser.add_argument('--part-range', dest='part_range', default=None, type=str)\n",
    "    parser.add_argument('--batch', dest='batch', default=False, action='store_true')\n",
    "    parser.add_argument('--depth', dest='depth', default=1000, type=int)\n",
    "    \n",
    "    parser.add_argument('-f')\n",
    "\n",
    "    args = parser.parse()\n",
    "\n",
    "    args.depth = args.depth if args.depth > 0 else None\n",
    "\n",
    "    if args.part_range:\n",
    "        part_offset, part_endpos = map(int, args.part_range.split('..'))\n",
    "        args.part_range = range(part_offset, part_endpos)\n",
    "\n",
    "    with Run.context():\n",
    "        args.colbert, args.checkpoint = load_colbert(args)\n",
    "        args.qrels = load_qrels(args.qrels)\n",
    "        args.queries = load_queries(args.queries)\n",
    "\n",
    "        args.index_path = os.path.join(args.index_root, args.index_name)\n",
    "\n",
    "        if args.faiss_name is not None:\n",
    "            args.faiss_index_path = os.path.join(args.index_path, args.faiss_name)\n",
    "        else:\n",
    "            args.faiss_index_path = os.path.join(args.index_path, get_faiss_index_name(args))\n",
    "\n",
    "        if args.batch:\n",
    "            batch_retrieve(args)\n",
    "        else:\n",
    "            retrieve(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
