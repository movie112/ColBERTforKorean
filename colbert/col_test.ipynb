{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bf133bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import ujson\n",
    "from itertools import accumulate\n",
    "from math import ceil\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "import copy\n",
    "import faiss\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f99dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefcf6f",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef169634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/utils/utils.py\n",
    "\n",
    "def print_message(*s, condition=True):\n",
    "    s = ' '.join([str(x) for x in s])\n",
    "    msg = \"[{}] {}\".format(datetime.datetime.now().strftime(\"%b %d, %H:%M:%S\"), s)\n",
    "\n",
    "    if condition:\n",
    "        print(msg, flush=True)\n",
    "\n",
    "    return msg\n",
    "\n",
    "def timestamp():\n",
    "    format_str = \"%Y-%m-%d_%H.%M.%S\"\n",
    "    result = datetime.datetime.now().strftime(format_str)\n",
    "    return result\n",
    "\n",
    "def create_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        print('\\n')\n",
    "        print_message(\"#> Note: Output directory\", path, 'already exists\\n\\n')\n",
    "    else:\n",
    "        print('\\n')\n",
    "        print_message(\"#> Creating directory\", path, '\\n\\n')\n",
    "        os.makedirs(path)\n",
    "class NullContextManager(object):\n",
    "    def __init__(self, dummy_resource=None):\n",
    "        self.dummy_resource = dummy_resource\n",
    "    def __enter__(self):\n",
    "        return self.dummy_resource\n",
    "    def __exit__(self, *args):\n",
    "        pass\n",
    "    \n",
    "def load_checkpoint(path, model, optimizer=None, do_print=True):\n",
    "    if do_print:\n",
    "        print_message(\"#> Loading checkpoint\", path, \"..\")\n",
    "\n",
    "    if path.startswith(\"http:\") or path.startswith(\"https:\"):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(path, map_location='cpu')\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k\n",
    "        if k[:7] == 'module.':\n",
    "            name = k[7:]\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    checkpoint['model_state_dict'] = new_state_dict\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        print_message(\"[WARNING] Loading checkpoint with strict=False\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if do_print:\n",
    "        print_message(\"#> checkpoint['epoch'] =\", checkpoint['epoch'])\n",
    "        print_message(\"#> checkpoint['batch'] =\", checkpoint['batch'])\n",
    "\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6896df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/utils/logging.py\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from colbert.utils.utils import print_message, create_directory--\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Logger():\n",
    "    def __init__(self, rank, run):\n",
    "        self.rank = rank\n",
    "        self.is_main = self.rank in [-1, 0]\n",
    "        self.run = run\n",
    "        self.logs_path = os.path.join(self.run.path, \"logs/\")\n",
    "\n",
    "        if self.is_main:\n",
    "            self._init_mlflow()\n",
    "            self.initialized_tensorboard = False\n",
    "            create_directory(self.logs_path)\n",
    "\n",
    "    def _init_mlflow(self):\n",
    "        mlflow.set_tracking_uri('file://' + os.path.join(self.run.experiments_root, \"logs/mlruns/\"))\n",
    "        mlflow.set_experiment('/'.join([self.run.experiment, self.run.script]))\n",
    "        \n",
    "        mlflow.set_tag('experiment', self.run.experiment)\n",
    "        mlflow.set_tag('name', self.run.name)\n",
    "        mlflow.set_tag('path', self.run.path)\n",
    "\n",
    "    def _init_tensorboard(self):\n",
    "        root = os.path.join(self.run.experiments_root, \"logs/tensorboard/\")\n",
    "        logdir = '__'.join([self.run.experiment, self.run.script, self.run.name])\n",
    "        logdir = os.path.join(root, logdir)\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=logdir)\n",
    "        self.initialized_tensorboard = True\n",
    "\n",
    "    def _log_exception(self, etype, value, tb):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        output_path = os.path.join(self.logs_path, 'exception.txt')\n",
    "        trace = ''.join(traceback.format_exception(etype, value, tb)) + '\\n'\n",
    "        print_message(trace, '\\n\\n')\n",
    "\n",
    "        self.log_new_artifact(output_path, trace)\n",
    "\n",
    "    def _log_all_artifacts(self):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        mlflow.log_artifacts(self.logs_path)\n",
    "\n",
    "    def _log_args(self, args):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        for key in vars(args):\n",
    "            value = getattr(args, key)\n",
    "            if type(value) in [int, float, str, bool]:\n",
    "                mlflow.log_param(key, value)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.json'), 'w') as output_metadata:\n",
    "            ujson.dump(args.input_arguments.__dict__, output_metadata, indent=4)\n",
    "            output_metadata.write('\\n')\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'args.txt'), 'w') as output_metadata:\n",
    "            output_metadata.write(' '.join(sys.argv) + '\\n')\n",
    "\n",
    "    def log_metric(self, name, value, step, log_to_mlflow=True):\n",
    "        if not self.is_main:\n",
    "            return\n",
    "\n",
    "        if not self.initialized_tensorboard:\n",
    "            self._init_tensorboard()\n",
    "\n",
    "        if log_to_mlflow:\n",
    "            mlflow.log_metric(name, value, step=step)\n",
    "        self.writer.add_scalar(name, value, step)\n",
    "\n",
    "    def log_new_artifact(self, path, content):\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        mlflow.log_artifact(path)\n",
    "\n",
    "    def warn(self, *args):\n",
    "        msg = print_message('[WARNING]', '\\t', *args)\n",
    "\n",
    "        with open(os.path.join(self.logs_path, 'warnings.txt'), 'a') as output_metadata:\n",
    "            output_metadata.write(msg + '\\n\\n\\n')\n",
    "\n",
    "    def info_all(self, *args):\n",
    "        print_message('[' + str(self.rank) + ']', '\\t', *args)\n",
    "\n",
    "    def info(self, *args):\n",
    "        if self.is_main:\n",
    "            print_message(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9df3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/utils/distributed.py\n",
    "def distributed_init(rank):\n",
    "    nranks = 'WORLD_SIZE' in os.environ and int(os.environ['WORLD_SIZE'])\n",
    "    nranks = max(1, nranks)\n",
    "    is_distributed = nranks > 1\n",
    "\n",
    "    if rank == 0:\n",
    "        print('nranks =', nranks, '\\t num_gpus =', torch.cuda.device_count())\n",
    "\n",
    "    if is_distributed:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        torch.cuda.set_device(rank % num_gpus)\n",
    "        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "    return nranks, is_distributed\n",
    "def distributed_barrier(rank):\n",
    "    if rank >= 0:\n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3798b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/utils/runs.py\n",
    "'''\n",
    "import colbert.utils.distributed as distributed--\n",
    "'''\n",
    "'''\n",
    "from colbert.utils.logging import Logger--\n",
    "from colbert.utils.utils import timestamp, create_directory, print_message--\n",
    "'''\n",
    "\n",
    "class _RunManager():\n",
    "    def __init__(self):\n",
    "        self.experiments_root = None\n",
    "        self.experiment = None\n",
    "        self.path = None\n",
    "        self.script = self._get_script_name()\n",
    "        self.name = self._generate_default_run_name()\n",
    "        self.original_name = self.name\n",
    "        self.exit_status = 'FINISHED'\n",
    "\n",
    "        self._logger = None\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def init(self, rank, root, experiment, name):\n",
    "        assert '/' not in experiment, experiment\n",
    "        assert '/' not in name, name\n",
    "\n",
    "        self.experiments_root = os.path.abspath(root)\n",
    "        self.experiment = experiment\n",
    "        self.name = name\n",
    "        self.path = os.path.join(self.experiments_root, self.experiment, self.script, self.name)\n",
    "\n",
    "        if rank < 1:\n",
    "            if os.path.exists(self.path):\n",
    "                print('\\n\\n')\n",
    "                print_message(\"It seems that \", self.path, \" already exists.\")\n",
    "                print_message(\"Do you want to overwrite it? \\t yes/no \\n\")\n",
    "\n",
    "                # TODO: This should timeout and exit (i.e., fail) given no response for 60 seconds.\n",
    "\n",
    "                response = input()\n",
    "                if response.strip() != 'yes':\n",
    "                    assert not os.path.exists(self.path), self.path\n",
    "            else:\n",
    "                create_directory(self.path)\n",
    "\n",
    "\n",
    "        if rank >= 0:\n",
    "            torch.distributed_barrier()\n",
    "\n",
    "        self._logger = Logger(rank, self)\n",
    "        self._log_args = self._logger._log_args\n",
    "        self.warn = self._logger.warn\n",
    "        self.info = self._logger.info\n",
    "        self.info_all = self._logger.info_all\n",
    "        self.log_metric = self._logger.log_metric\n",
    "        self.log_new_artifact = self._logger.log_new_artifact\n",
    "\n",
    "    def _generate_default_run_name(self):\n",
    "        return timestamp()\n",
    "\n",
    "    def _get_script_name(self):\n",
    "        #return os.path.basename(__main__.__file__) if '__file__' in dir(__main__) else 'none'\n",
    "        return os.path.basename('main'.__file__) if '__file__' in dir('main') else 'none'\n",
    "\n",
    "    @contextmanager\n",
    "    def context(self, consider_failed_if_interrupted=True):\n",
    "        try:\n",
    "            yield\n",
    "\n",
    "        except KeyboardInterrupt as ex:\n",
    "            print('\\n\\nInterrupted\\n\\n')\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            if consider_failed_if_interrupted:\n",
    "                self.exit_status = 'KILLED'  # mlflow.entities.RunStatus.KILLED\n",
    "\n",
    "            sys.exit(128 + 2)\n",
    "\n",
    "        except Exception as ex:\n",
    "            self._logger._log_exception(ex.__class__, ex, ex.__traceback__)\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            self.exit_status = 'FAILED'  # mlflow.entities.RunStatus.FAILED\n",
    "\n",
    "            raise ex\n",
    "\n",
    "        finally:\n",
    "            total_seconds = str(time.time() - self.start_time) + '\\n'\n",
    "            original_name = str(self.original_name)\n",
    "            name = str(self.name)\n",
    "\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'elapsed.txt'), total_seconds)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.original.txt'), original_name)\n",
    "            self.log_new_artifact(os.path.join(self._logger.logs_path, 'name.txt'), name)\n",
    "\n",
    "            self._logger._log_all_artifacts()\n",
    "\n",
    "            mlflow.end_run(status=self.exit_status)\n",
    "\n",
    "\n",
    "Run = _RunManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed8958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/utils/amp.py\n",
    "'''\n",
    "from contextlib import contextmanager\n",
    "from colbert.utils.utils import NullContextManager--\n",
    "from packaging import version\n",
    "'''\n",
    "v = version.parse\n",
    "PyTorch_over_1_6  = v(torch.__version__) >= v('1.6')\n",
    "\n",
    "class MixedPrecisionManager():\n",
    "    def __init__(self, activated):\n",
    "        assert (not activated) or PyTorch_over_1_6, \"Cannot use AMP for PyTorch version < 1.6\"\n",
    "\n",
    "        self.activated = activated\n",
    "\n",
    "        if self.activated:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def context(self):\n",
    "        return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
    "\n",
    "    def backward(self, loss):\n",
    "        if self.activated:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "    def step(self, colbert, optimizer):\n",
    "        if self.activated:\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0)\n",
    "\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(colbert.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffc9c7",
   "metadata": {},
   "source": [
    "## modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ce420",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "810ab412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/utils.py \n",
    "def _sort_by_length(ids, mask, bsize):\n",
    "    if ids.size(0) <= bsize:\n",
    "        return ids, mask, torch.arange(ids.size(0))\n",
    "\n",
    "    indices = mask.sum(-1).sort().indices\n",
    "    reverse_indices = indices.sort().indices\n",
    "\n",
    "    return ids[indices], mask[indices], reverse_indices\n",
    "\n",
    "\n",
    "def _split_into_batches(ids, mask, bsize):\n",
    "    batches = []\n",
    "    for offset in range(0, ids.size(0), bsize):\n",
    "        batches.append((ids[offset:offset+bsize], mask[offset:offset+bsize]))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1c7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/utils.py\n",
    "\n",
    "def tensorize_triples(query_tokenizer, doc_tokenizer, queries, positives, negatives, bsize):\n",
    "    assert len(queries) == len(positives) == len(negatives)\n",
    "    assert bsize is None or len(queries) % bsize == 0\n",
    "\n",
    "    N = len(queries)\n",
    "    Q_ids, Q_mask = query_tokenizer.tensorize(queries)\n",
    "    D_ids, D_mask = doc_tokenizer.tensorize(positives + negatives)\n",
    "    D_ids, D_mask = D_ids.view(2, N, -1), D_mask.view(2, N, -1)\n",
    "\n",
    "    # Compute max among {length of i^th positive, length of i^th negative} for i \\in N\n",
    "    maxlens = D_mask.sum(-1).max(0).values\n",
    "\n",
    "    # Sort by maxlens\n",
    "    indices = maxlens.sort().indices\n",
    "    Q_ids, Q_mask = Q_ids[indices], Q_mask[indices]\n",
    "    D_ids, D_mask = D_ids[:, indices], D_mask[:, indices]\n",
    "\n",
    "    (positive_ids, negative_ids), (positive_mask, negative_mask) = D_ids, D_mask\n",
    "\n",
    "    query_batches = _split_into_batches(Q_ids, Q_mask, bsize)\n",
    "    positive_batches = _split_into_batches(positive_ids, positive_mask, bsize)\n",
    "    negative_batches = _split_into_batches(negative_ids, negative_mask, bsize)\n",
    "\n",
    "    batches = []\n",
    "    for (q_ids, q_mask), (p_ids, p_mask), (n_ids, n_mask) in zip(query_batches, positive_batches, negative_batches):\n",
    "        Q = (torch.cat((q_ids, q_ids)), torch.cat((q_mask, q_mask)))\n",
    "        D = (torch.cat((p_ids, n_ids)), torch.cat((p_mask, n_mask)))\n",
    "        batches.append((Q, D))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def _sort_by_length(ids, mask, bsize):\n",
    "    if ids.size(0) <= bsize:\n",
    "        return ids, mask, torch.arange(ids.size(0))\n",
    "\n",
    "    indices = mask.sum(-1).sort().indices\n",
    "    reverse_indices = indices.sort().indices\n",
    "\n",
    "    return ids[indices], mask[indices], reverse_indices\n",
    "\n",
    "\n",
    "def _split_into_batches(ids, mask, bsize):\n",
    "    batches = []\n",
    "    for offset in range(0, ids.size(0), bsize):\n",
    "        batches.append((ids[offset:offset+bsize], mask[offset:offset+bsize]))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bb5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/query_tokenization.py\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "from colbert.modeling.tokenization.utils import _split_into_batches\n",
    "'''\n",
    "\n",
    "class QueryTokenizer():\n",
    "    def __init__(self, query_maxlen):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.query_maxlen = query_maxlen\n",
    "\n",
    "        self.Q_marker_token, self.Q_marker_token_id = '[Q]', self.tok.convert_tokens_to_ids('[unused0]')\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "        self.mask_token, self.mask_token_id = self.tok.mask_token, self.tok.mask_token_id\n",
    "\n",
    "        assert self.Q_marker_token_id == 1 and self.mask_token_id == 103\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.Q_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix + [self.mask_token] * (self.query_maxlen - (len(lst)+3)) for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.Q_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix + [self.mask_token_id] * (self.query_maxlen - (len(lst)+3)) for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        # add placehold for the [Q] marker\n",
    "        batch_text = ['. ' + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(batch_text, padding='max_length', truncation=True,\n",
    "                       return_tensors='pt', max_length=self.query_maxlen)\n",
    "\n",
    "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "\n",
    "        # postprocess for the [Q] marker and the [MASK] augmentation\n",
    "        ids[:, 1] = self.Q_marker_token_id\n",
    "        ids[ids == 0] = self.mask_token_id\n",
    "\n",
    "        if bsize:\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches\n",
    "\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60de94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/tokenization/doc_tokenization.py \n",
    "\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "from colbert.modeling.tokenization.utils import _split_into_batches, _sort_by_length\n",
    "'''\n",
    "\n",
    "class DocTokenizer():\n",
    "    def __init__(self, doc_maxlen):\n",
    "        self.tok = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "\n",
    "        self.D_marker_token, self.D_marker_token_id = '[D]', self.tok.convert_tokens_to_ids('[unused1]')\n",
    "        self.cls_token, self.cls_token_id = self.tok.cls_token, self.tok.cls_token_id\n",
    "        self.sep_token, self.sep_token_id = self.tok.sep_token, self.tok.sep_token_id\n",
    "\n",
    "        assert self.D_marker_token_id == 2\n",
    "\n",
    "    def tokenize(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        tokens = [self.tok.tokenize(x, add_special_tokens=False) for x in batch_text]\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return tokens\n",
    "\n",
    "        prefix, suffix = [self.cls_token, self.D_marker_token], [self.sep_token]\n",
    "        tokens = [prefix + lst + suffix for lst in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, batch_text, add_special_tokens=False):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        ids = self.tok(batch_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "        if not add_special_tokens:\n",
    "            return ids\n",
    "\n",
    "        prefix, suffix = [self.cls_token_id, self.D_marker_token_id], [self.sep_token_id]\n",
    "        ids = [prefix + lst + suffix for lst in ids]\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def tensorize(self, batch_text, bsize=None):\n",
    "        assert type(batch_text) in [list, tuple], (type(batch_text))\n",
    "\n",
    "        # add placehold for the [D] marker\n",
    "        batch_text = ['. ' + x for x in batch_text]\n",
    "\n",
    "        obj = self.tok(batch_text, padding='longest', truncation='longest_first',\n",
    "                       return_tensors='pt', max_length=self.doc_maxlen)\n",
    "\n",
    "        ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "\n",
    "        # postprocess for the [D] marker\n",
    "        ids[:, 1] = self.D_marker_token_id\n",
    "\n",
    "        if bsize:\n",
    "            ids, mask, reverse_indices = _sort_by_length(ids, mask, bsize)\n",
    "            batches = _split_into_batches(ids, mask, bsize)\n",
    "            return batches, reverse_indices\n",
    "\n",
    "        return ids, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add4a90",
   "metadata": {},
   "source": [
    "### colbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d395e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/colbert.py\n",
    "'''\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast\n",
    "from colbert.parameters import DEVICE--\n",
    "'''\n",
    "class ColBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, query_maxlen, doc_maxlen, mask_punctuation, dim=128, similarity_metric='cosine'):\n",
    "\n",
    "        super(ColBERT, self).__init__(config)\n",
    "\n",
    "        self.query_maxlen = query_maxlen\n",
    "        self.doc_maxlen = doc_maxlen\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.dim = dim\n",
    "\n",
    "        self.mask_punctuation = mask_punctuation\n",
    "        self.skiplist = {}\n",
    "\n",
    "        if self.mask_punctuation:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "            self.skiplist = {w: True\n",
    "                             for symbol in string.punctuation\n",
    "                             for w in [symbol, self.tokenizer.encode(symbol, add_special_tokens=False)[0]]}\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.linear = nn.Linear(config.hidden_size, dim, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, Q, D):\n",
    "        return self.score(self.query(*Q), self.doc(*D))\n",
    "\n",
    "    def query(self, input_ids, attention_mask):\n",
    "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "        Q = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        Q = self.linear(Q)\n",
    "\n",
    "        return torch.nn.functional.normalize(Q, p=2, dim=2)\n",
    "\n",
    "    def doc(self, input_ids, attention_mask, keep_dims=True):\n",
    "        input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "        D = self.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "        D = self.linear(D)\n",
    "\n",
    "        mask = torch.tensor(self.mask(input_ids), device=DEVICE).unsqueeze(2).float()\n",
    "        D = D * mask\n",
    "\n",
    "        D = torch.nn.functional.normalize(D, p=2, dim=2)\n",
    "\n",
    "        if not keep_dims:\n",
    "            D, mask = D.cpu().to(dtype=torch.float16), mask.cpu().bool().squeeze(-1)\n",
    "            D = [d[mask[idx]] for idx, d in enumerate(D)]\n",
    "\n",
    "        return D\n",
    "\n",
    "    def score(self, Q, D):\n",
    "        if self.similarity_metric == 'cosine':\n",
    "            return (Q @ D.permute(0, 2, 1)).max(2).values.sum(1)\n",
    "\n",
    "        assert self.similarity_metric == 'l2'\n",
    "        return (-1.0 * ((Q.unsqueeze(2) - D.unsqueeze(1))**2).sum(-1)).max(-1).values.sum(-1)\n",
    "\n",
    "    def mask(self, input_ids):\n",
    "        mask = [[(x not in self.skiplist) and (x != 0) for x in d] for d in input_ids.cpu().tolist()]\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593c50b",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eea28c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/parameters.py \n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "\n",
    "SAVED_CHECKPOINTS = [32*1000, 100*1000, 150*1000, 200*1000, 300*1000, 400*1000]\n",
    "SAVED_CHECKPOINTS += [10*1000, 20*1000, 30*1000, 40*1000, 50*1000, 60*1000, 70*1000, 80*1000, 90*1000]\n",
    "SAVED_CHECKPOINTS += [25*1000, 50*1000, 75*1000]\n",
    "\n",
    "SAVED_CHECKPOINTS = set(SAVED_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134d4b7",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75d03325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/modeling/inference.py\n",
    "'''\n",
    "from colbert.modeling.colbert import ColBERT--\n",
    "from colbert.modeling.tokenization import QueryTokenizer, DocTokenizer--\n",
    "from colbert.utils.amp import MixedPrecisionManager--\n",
    "from colbert.parameters import DEVICE--\n",
    "'''\n",
    "class ModelInference():\n",
    "    def __init__(self, colbert: ColBERT, amp=False):\n",
    "        assert colbert.training is False\n",
    "\n",
    "        self.colbert = colbert\n",
    "        self.query_tokenizer = QueryTokenizer(colbert.query_maxlen)\n",
    "        self.doc_tokenizer = DocTokenizer(colbert.doc_maxlen)\n",
    "\n",
    "        self.amp_manager = MixedPrecisionManager(amp)\n",
    "\n",
    "    def query(self, *args, to_cpu=False, **kw_args):\n",
    "        with torch.no_grad():\n",
    "            with self.amp_manager.context():\n",
    "                Q = self.colbert.query(*args, **kw_args)\n",
    "                return Q.cpu() if to_cpu else Q\n",
    "\n",
    "    def doc(self, *args, to_cpu=False, **kw_args):\n",
    "        with torch.no_grad():\n",
    "            with self.amp_manager.context():\n",
    "                D = self.colbert.doc(*args, **kw_args)\n",
    "                return D.cpu() if to_cpu else D\n",
    "\n",
    "    def queryFromText(self, queries, bsize=None, to_cpu=False):\n",
    "        if bsize:\n",
    "            batches = self.query_tokenizer.tensorize(queries, bsize=bsize)\n",
    "            batches = [self.query(input_ids, attention_mask, to_cpu=to_cpu) for input_ids, attention_mask in batches]\n",
    "            return torch.cat(batches)\n",
    "\n",
    "        input_ids, attention_mask = self.query_tokenizer.tensorize(queries)\n",
    "        return self.query(input_ids, attention_mask)\n",
    "\n",
    "    def docFromText(self, docs, bsize=None, keep_dims=True, to_cpu=False):\n",
    "        if bsize:\n",
    "            batches, reverse_indices = self.doc_tokenizer.tensorize(docs, bsize=bsize)\n",
    "\n",
    "            batches = [self.doc(input_ids, attention_mask, keep_dims=keep_dims, to_cpu=to_cpu)\n",
    "                       for input_ids, attention_mask in batches]\n",
    "\n",
    "            if keep_dims:\n",
    "                D = _stack_3D_tensors(batches)\n",
    "                return D[reverse_indices]\n",
    "\n",
    "            D = [d for batch in batches for d in batch]\n",
    "            return [D[idx] for idx in reverse_indices.tolist()]\n",
    "\n",
    "        input_ids, attention_mask = self.doc_tokenizer.tensorize(docs)\n",
    "        return self.doc(input_ids, attention_mask, keep_dims=keep_dims)\n",
    "\n",
    "    def score(self, Q, D, mask=None, lengths=None, explain=False):\n",
    "        if lengths is not None:\n",
    "            assert mask is None, \"don't supply both mask and lengths\"\n",
    "\n",
    "            mask = torch.arange(D.size(1), device=DEVICE) + 1\n",
    "            mask = mask.unsqueeze(0) <= lengths.to(DEVICE).unsqueeze(-1)\n",
    "\n",
    "        scores = (D @ Q)\n",
    "        scores = scores if mask is None else scores * mask.unsqueeze(-1)\n",
    "        scores = scores.max(1)\n",
    "\n",
    "        if explain:\n",
    "            assert False, \"TODO\"\n",
    "\n",
    "        return scores.values.sum(-1).cpu()\n",
    "\n",
    "\n",
    "def _stack_3D_tensors(groups):\n",
    "    bsize = sum([x.size(0) for x in groups])\n",
    "    maxlen = max([x.size(1) for x in groups])\n",
    "    hdim = groups[0].size(2)\n",
    "\n",
    "    output = torch.zeros(bsize, maxlen, hdim, device=groups[0].device, dtype=groups[0].dtype)\n",
    "\n",
    "    offset = 0\n",
    "    for x in groups:\n",
    "        endpos = offset + x.size(0)\n",
    "        output[offset:endpos, :x.size(1)] = x\n",
    "        offset = endpos\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a9d1e",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75d83aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args, do_print=True):\n",
    "    colbert = ColBERT.from_pretrained('bert-base-uncased',\n",
    "                                      query_maxlen=args.query_maxlen,\n",
    "                                      doc_maxlen=args.doc_maxlen,\n",
    "                                      dim=args.dim,\n",
    "                                      similarity_metric=args.similarity,\n",
    "                                      mask_punctuation=args.mask_punctuation)\n",
    "    colbert = colbert.to(DEVICE)\n",
    "\n",
    "    print_message(\"#> Loading model checkpoint.\", condition=do_print)\n",
    "\n",
    "    checkpoint = load_checkpoint(args.checkpoint, colbert, do_print=do_print)\n",
    "\n",
    "    colbert.eval()\n",
    "\n",
    "    return colbert, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "74bb7779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/evaluation/loaders.py\n",
    "def load_queries(queries_path):\n",
    "    queries = OrderedDict()\n",
    "\n",
    "    print_message(\"#> Loading the queries from\", queries_path, \"...\")\n",
    "\n",
    "    with open(queries_path) as f:\n",
    "        for line in f:\n",
    "            qid, query, *_ = line.strip().split('\\t')\n",
    "            qid = int(qid)\n",
    "\n",
    "            assert (qid not in queries), (\"Query QID\", qid, \"is repeated!\")\n",
    "            queries[qid] = query\n",
    "\n",
    "    print_message(\"#> Got\", len(queries), \"queries. All QIDs are unique.\\n\")\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def load_qrels(qrels_path):\n",
    "    if qrels_path is None:\n",
    "        return None\n",
    "\n",
    "    print_message(\"#> Loading qrels from\", qrels_path, \"...\")\n",
    "\n",
    "    qrels = OrderedDict()\n",
    "    with open(qrels_path, mode='r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            qid, x, pid, y = map(int, line.strip().split('\\t'))\n",
    "            assert x == 0 and y == 1\n",
    "            qrels[qid] = qrels.get(qid, [])\n",
    "            qrels[qid].append(pid)\n",
    "\n",
    "    assert all(len(qrels[qid]) == len(set(qrels[qid])) for qid in qrels)\n",
    "\n",
    "    avg_positive = round(sum(len(qrels[qid]) for qid in qrels) / len(qrels), 2)\n",
    "\n",
    "    print_message(\"#> Loaded qrels for\", len(qrels), \"unique queries with\",\n",
    "                  avg_positive, \"positives per query on average.\\n\")\n",
    "\n",
    "    return qrels\n",
    "\n",
    "\n",
    "def load_topK(topK_path):\n",
    "    queries = OrderedDict()\n",
    "    topK_docs = OrderedDict()\n",
    "    topK_pids = OrderedDict()\n",
    "\n",
    "    print_message(\"#> Loading the top-k per query from\", topK_path, \"...\")\n",
    "\n",
    "    with open(topK_path) as f:\n",
    "        for line_idx, line in enumerate(f):\n",
    "            if line_idx and line_idx % (10*1000*1000) == 0:\n",
    "                print(line_idx, end=' ', flush=True)\n",
    "\n",
    "            qid, pid, query, passage = line.split('\\t')\n",
    "            qid, pid = int(qid), int(pid)\n",
    "\n",
    "            assert (qid not in queries) or (queries[qid] == query)\n",
    "            queries[qid] = query\n",
    "            topK_docs[qid] = topK_docs.get(qid, [])\n",
    "            topK_docs[qid].append(passage)\n",
    "            topK_pids[qid] = topK_pids.get(qid, [])\n",
    "            topK_pids[qid].append(pid)\n",
    "\n",
    "        print()\n",
    "\n",
    "    assert all(len(topK_pids[qid]) == len(set(topK_pids[qid])) for qid in topK_pids)\n",
    "\n",
    "    Ks = [len(topK_pids[qid]) for qid in topK_pids]\n",
    "\n",
    "    print_message(\"#> max(Ks) =\", max(Ks), \", avg(Ks) =\", round(sum(Ks) / len(Ks), 2))\n",
    "    print_message(\"#> Loaded the top-k per query for\", len(queries), \"unique queries.\\n\")\n",
    "\n",
    "    return queries, topK_docs, topK_pids\n",
    "\n",
    "\n",
    "def load_topK_pids(topK_path, qrels):\n",
    "    topK_pids = defaultdict(list)\n",
    "    topK_positives = defaultdict(list)\n",
    "\n",
    "    print_message(\"#> Loading the top-k PIDs per query from\", topK_path, \"...\")\n",
    "\n",
    "    with open(topK_path) as f:\n",
    "        for line_idx, line in enumerate(f):\n",
    "            if line_idx and line_idx % (10*1000*1000) == 0:\n",
    "                print(line_idx, end=' ', flush=True)\n",
    "\n",
    "            qid, pid, *rest = line.strip().split('\\t')\n",
    "            qid, pid = int(qid), int(pid)\n",
    "\n",
    "            topK_pids[qid].append(pid)\n",
    "\n",
    "\n",
    "\n",
    "    assert all(len(topK_pids[qid]) == len(set(topK_pids[qid])) for qid in topK_pids)\n",
    "    assert all(len(topK_positives[qid]) == len(set(topK_positives[qid])) for qid in topK_positives)\n",
    "\n",
    "    # Make them sets for fast lookups later\n",
    "    topK_positives = {qid: set(topK_positives[qid]) for qid in topK_positives}\n",
    "\n",
    "    Ks = [len(topK_pids[qid]) for qid in topK_pids]\n",
    "\n",
    "    print_message(\"#> max(Ks) =\", max(Ks), \", avg(Ks) =\", round(sum(Ks) / len(Ks), 2))\n",
    "    print_message(\"#> Loaded the top-k per query for\", len(topK_pids), \"unique queries.\\n\")\n",
    "\n",
    "    if len(topK_positives) == 0:\n",
    "        topK_positives = None\n",
    "    else:\n",
    "        assert len(topK_pids) >= len(topK_positives)\n",
    "\n",
    "        for qid in set.difference(set(topK_pids.keys()), set(topK_positives.keys())):\n",
    "            topK_positives[qid] = []\n",
    "\n",
    "        assert len(topK_pids) == len(topK_positives)\n",
    "\n",
    "        avg_positive = round(sum(len(topK_positives[qid]) for qid in topK_positives) / len(topK_pids), 2)\n",
    "\n",
    "        print_message(\"#> Concurrently got annotations for\", len(topK_positives), \"unique queries with\",\n",
    "                      avg_positive, \"positives per query on average.\\n\")\n",
    "\n",
    "    assert qrels is None or topK_positives is None, \"Cannot have both qrels and an annotated top-K file!\"\n",
    "\n",
    "    if topK_positives is None:\n",
    "        topK_positives = qrels\n",
    "\n",
    "    return topK_pids, topK_positives\n",
    "\n",
    "\n",
    "def load_collection(collection_path):\n",
    "    print_message(\"#> Loading collection...\")\n",
    "\n",
    "    collection = []\n",
    "\n",
    "    with open(collection_path) as f:\n",
    "        for line_idx, line in enumerate(f):\n",
    "            if line_idx % (1000*1000) == 0:\n",
    "                print(f'{line_idx // 1000 // 1000}M', end=' ', flush=True)\n",
    "\n",
    "            pid, passage, *rest = line.strip().split('\\t')\n",
    "            assert pid == 'id' or int(pid) == line_idx\n",
    "\n",
    "            if len(rest) >= 1:\n",
    "                title = rest[0]\n",
    "                passage = title + ' | ' + passage\n",
    "\n",
    "            collection.append(passage)\n",
    "\n",
    "    print()\n",
    "\n",
    "    return collection\n",
    "\n",
    "\n",
    "def load_colbert(args, do_print=True):\n",
    "    colbert, checkpoint = load_model(args, do_print)\n",
    "\n",
    "    # TODO: If the parameters below were not specified on the command line, their *checkpoint* values should be used.\n",
    "    # I.e., not their purely (i.e., training) default values.\n",
    "\n",
    "    for k in ['query_maxlen', 'doc_maxlen', 'dim', 'similarity', 'amp']:\n",
    "        if 'arguments' in checkpoint and hasattr(args, k):\n",
    "            if k in checkpoint['arguments'] and checkpoint['arguments'][k] != getattr(args, k):\n",
    "                a, b = checkpoint['arguments'][k], getattr(args, k)\n",
    "                Run.warn(f\"Got checkpoint['arguments']['{k}'] != args.{k} (i.e., {a} != {b})\")\n",
    "\n",
    "    if 'arguments' in checkpoint:\n",
    "        if args.rank < 1:\n",
    "            print(ujson.dumps(checkpoint['arguments'], indent=4))\n",
    "\n",
    "    if do_print:\n",
    "        print('\\n')\n",
    "\n",
    "    return colbert, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a235d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/evaluation/metrics.py \n",
    "'''\n",
    "from colbert.utils.runs import Run--\n",
    "'''\n",
    "class Metrics:\n",
    "    def __init__(self, mrr_depths: set, recall_depths: set, success_depths: set, total_queries=None):\n",
    "        self.results = {}\n",
    "        self.mrr_sums = {depth: 0.0 for depth in mrr_depths}\n",
    "        self.recall_sums = {depth: 0.0 for depth in recall_depths}\n",
    "        self.success_sums = {depth: 0.0 for depth in success_depths}\n",
    "        self.total_queries = total_queries\n",
    "\n",
    "        self.max_query_idx = -1\n",
    "        self.num_queries_added = 0\n",
    "\n",
    "    def add(self, query_idx, query_key, ranking, gold_positives):\n",
    "        self.num_queries_added += 1\n",
    "\n",
    "        assert query_key not in self.results\n",
    "        assert len(self.results) <= query_idx\n",
    "        assert len(set(gold_positives)) == len(gold_positives)\n",
    "        assert len(set([pid for _, pid, _ in ranking])) == len(ranking)\n",
    "\n",
    "        self.results[query_key] = ranking\n",
    "\n",
    "        positives = [i for i, (_, pid, _) in enumerate(ranking) if pid in gold_positives]\n",
    "\n",
    "        if len(positives) == 0:\n",
    "            return\n",
    "\n",
    "        for depth in self.mrr_sums:\n",
    "            first_positive = positives[0]\n",
    "            self.mrr_sums[depth] += (1.0 / (first_positive+1.0)) if first_positive < depth else 0.0\n",
    "\n",
    "        for depth in self.success_sums:\n",
    "            first_positive = positives[0]\n",
    "            self.success_sums[depth] += 1.0 if first_positive < depth else 0.0\n",
    "\n",
    "        for depth in self.recall_sums:\n",
    "            num_positives_up_to_depth = len([pos for pos in positives if pos < depth])\n",
    "            self.recall_sums[depth] += num_positives_up_to_depth / len(gold_positives)\n",
    "\n",
    "    def print_metrics(self, query_idx):\n",
    "        for depth in sorted(self.mrr_sums):\n",
    "            print(\"MRR@\" + str(depth), \"=\", self.mrr_sums[depth] / (query_idx+1.0))\n",
    "\n",
    "        for depth in sorted(self.success_sums):\n",
    "            print(\"Success@\" + str(depth), \"=\", self.success_sums[depth] / (query_idx+1.0))\n",
    "\n",
    "        for depth in sorted(self.recall_sums):\n",
    "            print(\"Recall@\" + str(depth), \"=\", self.recall_sums[depth] / (query_idx+1.0))\n",
    "\n",
    "    def log(self, query_idx):\n",
    "        assert query_idx >= self.max_query_idx\n",
    "        self.max_query_idx = query_idx\n",
    "\n",
    "        Run.log_metric(\"ranking/max_query_idx\", query_idx, query_idx)\n",
    "        Run.log_metric(\"ranking/num_queries_added\", self.num_queries_added, query_idx)\n",
    "\n",
    "        for depth in sorted(self.mrr_sums):\n",
    "            score = self.mrr_sums[depth] / (query_idx+1.0)\n",
    "            Run.log_metric(\"ranking/MRR.\" + str(depth), score, query_idx)\n",
    "\n",
    "        for depth in sorted(self.success_sums):\n",
    "            score = self.success_sums[depth] / (query_idx+1.0)\n",
    "            Run.log_metric(\"ranking/Success.\" + str(depth), score, query_idx)\n",
    "\n",
    "        for depth in sorted(self.recall_sums):\n",
    "            score = self.recall_sums[depth] / (query_idx+1.0)\n",
    "            Run.log_metric(\"ranking/Recall.\" + str(depth), score, query_idx)\n",
    "\n",
    "    def output_final_metrics(self, path, query_idx, num_queries):\n",
    "        assert query_idx + 1 == num_queries\n",
    "        assert num_queries == self.total_queries\n",
    "\n",
    "        if self.max_query_idx < query_idx:\n",
    "            self.log(query_idx)\n",
    "\n",
    "        self.print_metrics(query_idx)\n",
    "\n",
    "        output = defaultdict(dict)\n",
    "\n",
    "        for depth in sorted(self.mrr_sums):\n",
    "            score = self.mrr_sums[depth] / (query_idx+1.0)\n",
    "            output['mrr'][depth] = score\n",
    "\n",
    "        for depth in sorted(self.success_sums):\n",
    "            score = self.success_sums[depth] / (query_idx+1.0)\n",
    "            output['success'][depth] = score\n",
    "\n",
    "        for depth in sorted(self.recall_sums):\n",
    "            score = self.recall_sums[depth] / (query_idx+1.0)\n",
    "            output['recall'][depth] = score\n",
    "\n",
    "        with open(path, 'w') as f:\n",
    "            ujson.dump(output, f, indent=4)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def evaluate_recall(qrels, queries, topK_pids):\n",
    "    if qrels is None:\n",
    "        return\n",
    "\n",
    "    assert set(qrels.keys()) == set(queries.keys())\n",
    "    recall_at_k = [len(set.intersection(set(qrels[qid]), set(topK_pids[qid]))) / max(1.0, len(qrels[qid]))\n",
    "                   for qid in qrels]\n",
    "    recall_at_k = sum(recall_at_k) / len(qrels)\n",
    "    recall_at_k = round(recall_at_k, 3)\n",
    "    print(\"Recall @ maximum depth =\", recall_at_k)\n",
    "\n",
    "\n",
    "# TODO: If implicit qrels are used (for re-ranking), warn if a recall metric is requested + add an asterisk to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa908fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/evaluation/slow.py\n",
    "def slow_rerank(args, query, pids, passages):\n",
    "    colbert = args.colbert\n",
    "    inference = args.inference\n",
    "\n",
    "    Q = inference.queryFromText([query])\n",
    "\n",
    "    D_ = inference.docFromText(passages, bsize=args.bsize)\n",
    "    scores = colbert.score(Q, D_).cpu()\n",
    "\n",
    "    scores = scores.sort(descending=True)\n",
    "    ranked = scores.indices.tolist()\n",
    "\n",
    "    ranked_scores = scores.values.tolist()\n",
    "    ranked_pids = [pids[position] for position in ranked]\n",
    "    ranked_passages = [passages[position] for position in ranked]\n",
    "\n",
    "    assert len(ranked_pids) == len(set(ranked_pids))\n",
    "\n",
    "    return list(zip(ranked_scores, ranked_pids, ranked_passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "681fee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/evaluation/ranking_logger.py\n",
    "'''\n",
    "from colbert.utils.utils import print_message, NullContextManager\n",
    "from colbert.utils.runs import Run\n",
    "'''\n",
    "class RankingLogger():\n",
    "    def __init__(self, directory, qrels=None, log_scores=False):\n",
    "        self.directory = directory\n",
    "        self.qrels = qrels\n",
    "        self.filename, self.also_save_annotations = None, None\n",
    "        self.log_scores = log_scores\n",
    "\n",
    "    @contextmanager\n",
    "    def context(self, filename, also_save_annotations=False):\n",
    "        assert self.filename is None\n",
    "        assert self.also_save_annotations is None\n",
    "\n",
    "        filename = os.path.join(self.directory, filename)\n",
    "        self.filename, self.also_save_annotations = filename, also_save_annotations\n",
    "\n",
    "        print_message(\"#> Logging ranked lists to {}\".format(self.filename))\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            self.f = f\n",
    "            with (open(filename + '.annotated', 'w') if also_save_annotations else NullContextManager()) as g:\n",
    "                self.g = g\n",
    "                try:\n",
    "                    yield self\n",
    "                finally:\n",
    "                    pass\n",
    "\n",
    "    def log(self, qid, ranking, is_ranked=True, print_positions=[]):\n",
    "        print_positions = set(print_positions)\n",
    "\n",
    "        f_buffer = []\n",
    "        g_buffer = []\n",
    "\n",
    "        for rank, (score, pid, passage) in enumerate(ranking):\n",
    "            is_relevant = self.qrels and int(pid in self.qrels[qid])\n",
    "            rank = rank+1 if is_ranked else -1\n",
    "\n",
    "            possibly_score = [score] if self.log_scores else []\n",
    "\n",
    "            f_buffer.append('\\t'.join([str(x) for x in [qid, pid, rank] + possibly_score]) + \"\\n\")\n",
    "            if self.g:\n",
    "                g_buffer.append('\\t'.join([str(x) for x in [qid, pid, rank, is_relevant]]) + \"\\n\")\n",
    "\n",
    "            if rank in print_positions:\n",
    "                prefix = \"** \" if is_relevant else \"\"\n",
    "                prefix += str(rank)\n",
    "                print(\"#> ( QID {} ) \".format(qid) + prefix + \") \", pid, \":\", score, '    ', passage)\n",
    "\n",
    "        self.f.write(''.join(f_buffer))\n",
    "        if self.g:\n",
    "            self.g.write(''.join(g_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "489696a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/evaluation/ranking.py\n",
    "'''\n",
    "from colbert.utils.runs import Run--\n",
    "from colbert.utils.utils import print_message--\n",
    "\n",
    "from colbert.evaluation.metrics import Metrics--\n",
    "from colbert.evaluation.ranking_logger import RankingLogger--\n",
    "from colbert.modeling.inference import ModelInference--\n",
    "\n",
    "from colbert.evaluation.slow import slow_rerank--\n",
    "'''\n",
    "\n",
    "def evaluate(args):\n",
    "    args.inference = ModelInference(args.colbert, amp=args.amp)\n",
    "    qrels, queries, topK_pids = args.qrels, args.queries, args.topK_pids\n",
    "\n",
    "    depth = args.depth\n",
    "    collection = args.collection\n",
    "    if collection is None:\n",
    "        topK_docs = args.topK_docs\n",
    "\n",
    "    def qid2passages(qid):\n",
    "        if collection is not None:\n",
    "            return [collection[pid] for pid in topK_pids[qid][:depth]]\n",
    "        else:\n",
    "            return topK_docs[qid][:depth]\n",
    "\n",
    "    metrics = Metrics(mrr_depths={10, 100}, recall_depths={50, 200, 1000},\n",
    "                      success_depths={5, 10, 20, 50, 100, 1000},\n",
    "                      total_queries=len(queries))\n",
    "\n",
    "    ranking_logger = RankingLogger(Run.path, qrels=qrels)\n",
    "\n",
    "    args.milliseconds = []\n",
    "\n",
    "    with ranking_logger.context('ranking.tsv', also_save_annotations=(qrels is not None)) as rlogger:\n",
    "        with torch.no_grad():\n",
    "            keys = sorted(list(queries.keys()))\n",
    "            random.shuffle(keys)\n",
    "\n",
    "            for query_idx, qid in enumerate(keys):\n",
    "                query = queries[qid]\n",
    "\n",
    "                print_message(query_idx, qid, query, '\\n')\n",
    "\n",
    "                if qrels and args.shortcircuit and len(set.intersection(set(qrels[qid]), set(topK_pids[qid]))) == 0:\n",
    "                    continue\n",
    "\n",
    "                ranking = slow_rerank(args, query, topK_pids[qid], qid2passages(qid))\n",
    "\n",
    "                rlogger.log(qid, ranking, [0, 1])\n",
    "\n",
    "                if qrels:\n",
    "                    metrics.add(query_idx, qid, ranking, qrels[qid])\n",
    "\n",
    "                    for i, (score, pid, passage) in enumerate(ranking):\n",
    "                        if pid in qrels[qid]:\n",
    "                            print(\"\\n#> Found\", pid, \"at position\", i+1, \"with score\", score)\n",
    "                            print(passage)\n",
    "                            break\n",
    "\n",
    "                    metrics.print_metrics(query_idx)\n",
    "                    metrics.log(query_idx)\n",
    "\n",
    "                print_message(\"#> checkpoint['batch'] =\", args.checkpoint['batch'], '\\n')\n",
    "                print(\"rlogger.filename =\", rlogger.filename)\n",
    "\n",
    "                if len(args.milliseconds) > 1:\n",
    "                    print('Slow-Ranking Avg Latency =', sum(args.milliseconds[1:]) / len(args.milliseconds[1:]))\n",
    "\n",
    "                print(\"\\n\\n\")\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "        # print('Avg Latency =', sum(args.milliseconds[1:]) / len(args.milliseconds[1:]))\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    print('\\n\\n')\n",
    "    if qrels:\n",
    "        assert query_idx + 1 == len(keys) == len(set(keys))\n",
    "        metrics.output_final_metrics(os.path.join(Run.path, 'ranking.metrics'), query_idx, len(queries))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91745dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d7f3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/utils/parser.py \n",
    "'''\n",
    "import colbert.utils.distributed as distributed--\n",
    "from colbert.utils.runs import Run--\n",
    "from colbert.utils.utils import print_message, timestamp, create_directory--\n",
    "'''\n",
    "\n",
    "class Arguments():\n",
    "    def __init__(self, description):\n",
    "        self.parser = ArgumentParser(description=description)\n",
    "        self.checks = []\n",
    "\n",
    "        self.add_argument('--root', dest='root', default='/', type=str)\n",
    "        self.add_argument('--experiment', dest='experiment', default='MSMARCO-psg', type=str)\n",
    "        self.add_argument('--run', dest='run', default='msmarco.psg.l2', type=str)\n",
    "\n",
    "        self.add_argument('--local_rank', dest='rank', default=-1, type=int)\n",
    "\n",
    "    def add_model_parameters(self):\n",
    "        # Core Arguments\n",
    "        self.add_argument('--similarity', dest='similarity', default='cosine', choices=['cosine', 'l2'])\n",
    "        self.add_argument('--dim', dest='dim', default=128, type=int)\n",
    "        self.add_argument('--query_maxlen', dest='query_maxlen', default=32, type=int)\n",
    "        self.add_argument('--doc_maxlen', dest='doc_maxlen', default=180, type=int)\n",
    "\n",
    "        # Filtering-related Arguments'../movie/data/toptop.tsv'\n",
    "        self.add_argument('--mask-punctuation', dest='mask_punctuation', default=True, action='store_true')\n",
    "\n",
    "    def add_model_inference_parameters(self):\n",
    "        self.add_argument('--checkpoint', dest='checkpoint', default=\"/\", type=str)\n",
    "        self.add_argument('--bsize', dest='bsize', default=128, type=int)\n",
    "        self.add_argument('--amp', dest='amp', default=True, action='store_true')\n",
    "        \n",
    "    def add_ranking_input(self):\n",
    "        self.add_argument('--queries', dest='queries', default='/queries.dev.small.tsv', type=str)\n",
    "        self.add_argument('--collection', dest='collection', default='collection.tsv', type=str)\n",
    "        self.add_argument('--qrels', dest='qrels', default='qrels.dev.small.tsv', type=str)\n",
    "# '/home/dilab/movie/data/'\n",
    "    def add_reranking_input(self):\n",
    "        self.add_ranking_input()\n",
    "        self.add_argument('--topk', dest='topK', default='/top1000.dev', type=str)\n",
    "        self.add_argument('--shortcircuit', dest='shortcircuit', default=False, action='store_true')\n",
    "\n",
    "    def add_argument(self, *args, **kw_args):\n",
    "        return self.parser.add_argument(*args, **kw_args)\n",
    "\n",
    "    def check_arguments(self, args):\n",
    "        for check in self.checks:\n",
    "            check(args)\n",
    "\n",
    "    def parse(self):\n",
    "        args = self.parser.parse_args()\n",
    "        self.check_arguments(args)\n",
    "\n",
    "        args.input_arguments = copy.deepcopy(args)\n",
    "\n",
    "        args.nranks, args.distributed = distributed_init(args.rank)\n",
    "\n",
    "        args.nthreads = int(max(os.cpu_count(), faiss.omp_get_max_threads()) * 0.8)\n",
    "        args.nthreads = max(1, args.nthreads // args.nranks)\n",
    "\n",
    "        if args.nranks > 1:\n",
    "            print_message(f\"#> Restricting number of threads for FAISS to {args.nthreads} per process\",\n",
    "                          condition=(args.rank == 0))\n",
    "            faiss.omp_set_num_threads(args.nthreads)\n",
    "\n",
    "        Run.init(args.rank, args.root, args.experiment, args.run)\n",
    "        Run._log_args(args)\n",
    "        Run.info(args.input_arguments.__dict__, '\\n')\n",
    "\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2048d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colbert/test.py\n",
    "'''\n",
    "from colbert.utils.parser import Arguments\n",
    "from colbert.utils.runs import Run\n",
    "\n",
    "from colbert.evaluation.loaders import load_colbert, load_topK, load_qrels\n",
    "from colbert.evaluation.loaders import load_queries, load_topK_pids, load_collection\n",
    "from colbert.evaluation.ranking import evaluate\n",
    "from colbert.evaluation.metrics import evaluate_recall\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    random.seed(12345)\n",
    "\n",
    "    parser = Arguments(description='Exhaustive (slow, not index-based) evaluation of re-ranking with ColBERT.')\n",
    "\n",
    "    parser.add_model_parameters()\n",
    "    parser.add_model_inference_parameters()\n",
    "    parser.add_reranking_input()\n",
    "    parser.add_argument('-f')\n",
    "\n",
    "    parser.add_argument('--depth', dest='depth', required=False, default=None, type=int)\n",
    "\n",
    "    args = parser.parse()\n",
    "\n",
    "    with Run.context():\n",
    "        args.colbert, args.checkpoint = load_colbert(args)\n",
    "        args.qrels = load_qrels(args.qrels)\n",
    "\n",
    "        if args.collection or args.queries:\n",
    "            assert args.collection and args.queries\n",
    "\n",
    "            args.queries = load_queries(args.queries)\n",
    "            args.collection = load_collection(args.collection)\n",
    "            args.topK_pids, args.qrels = load_topK_pids(args.topK, args.qrels)\n",
    "\n",
    "        else:\n",
    "            args.queries, args.topK_docs, args.topK_pids = load_topK(args.topK)\n",
    "\n",
    "        assert (not args.shortcircuit) or args.qrels, \\\n",
    "            \"Short-circuiting (i.e., applying minimal computation to queries with no positives in the re-ranked set) \" \\\n",
    "            \"can only be applied if qrels is provided.\"\n",
    "\n",
    "        evaluate_recall(args.qrels, args.queries, args.topK_pids)\n",
    "        evaluate(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
